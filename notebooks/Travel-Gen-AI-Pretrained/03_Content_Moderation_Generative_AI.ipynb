{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a15f81e3-0976-4ff7-8ae1-ddb5fbdf1d94",
   "metadata": {},
   "source": [
    "# Lab 3: Content moderation for generative AI applications\n",
    "\n",
    "As we've seen in these workshops, leveraging generative AI to generate creative content such as marketing copy and images can improve the quality of your messaging and provide a boost to productivity. However, turning over this important task to AI is not without risks. Most foundation models are trained and implemented with safeguards to prevent them from generating unsafe or inappropriate content. For example, you can read more about [\"Claude's Constitution\"](https://www.anthropic.com/index/claudes-constitution) which outlines the \"values\" that Anthropic built into the FM we used in the first notebook. In addition, Amazon Bedrock has its own [abuse detection](https://docs.aws.amazon.com/bedrock/latest/userguide/abuse-detection.html) layer on top of the FMs that it provides access to. Nevertheless, adding a moderation step to ensure that generated content remains consistent with the voice of your brand and does not inadvertently include any inappropriate themes or messages can add a layer of confidence to scaling generative AI across your organization. Content moderation can be implemented many different ways. For example, you could use a large language model to evaluate itself by building a prompt that expresses your standards and asks the model to evaluate text that it's generated against those standards. Or you can use separate AI models or services designed and tuned for classification tasks to check your work with generative AI.\n",
    "\n",
    "In this notebook, we will use another FM available in Amazon Bedrock, Titan Text Embeddings, to generate embeddings that will be used to train a classifier model. Embeddings are numerical representations of values or objects like text, images, and audio that can be fed to machine learning models. In this case, the model will be trained on examples of toxic and non-toxic text so that it is able to predict whether our generated text is toxic. As alternatives, we will also evaluate the generated text using the [Trust and Safety](https://docs.aws.amazon.com/comprehend/latest/dg/trust-safety.html) API available with [Amazon Comprehend](https://aws.amazon.com/comprehend/) as well as Anthropic's Claude Instant FM to predict if our generated text is compliant with a stated policy.\n",
    "\n",
    "For generated banner image, will take a different approach of using [Amazon Rekognition](https://aws.amazon.com/rekognition/) to identify unsafe/inappropriate in images.\n",
    "\n",
    "The following diagram highlights the focus of this lab.\n",
    "\n",
    "![Lab 3 Diagram](images/architecture-lab3.png)\n",
    "\n",
    "\n",
    "## In this notebook\n",
    "\n",
    "We will complete the following steps in this notebook.\n",
    "\n",
    "1. Text moderation:\n",
    "    - Custom classifier model:\n",
    "        - Load and examine the labeled dataset of toxic and non-toxic text.\n",
    "        - Generate embeddings using the Amazon Titan Text Embedding model for all text values in the dataset.\n",
    "        - Split the dataset 80/20 into training and testing portions.\n",
    "        - Train a classifier model using the embeddings of the training data using the scikit-learn [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
    "        - Calculate and review metrics that measure the accuracy of the model's ability to propertly classify the text in the held-out test data.\n",
    "        - Finally, use the model to classify the email subject and email body that we generated in the previous notebook.\n",
    "    - Compare the moderation results from the Amazon Comprehend [Trust and Safety](https://docs.aws.amazon.com/comprehend/latest/dg/trust-safety.html) API.\n",
    "    - Compare the moderation results using a prompt with Anthropic's Claude Instant FM.\n",
    "1. Image moderation:\n",
    "    - Use the Amazon Rekognition [DetectModerationLabels](https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DetectModerationLabels.html) API for a test image that we expect to return some moderation labels.\n",
    "    - Use the Amazon Rekognition [DetectModerationLabels](https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DetectModerationLabels.html) API for the images we created in the previous notebook to ensure that no moderation labels are present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc708a77-1dc2-4032-8a8c-5a85b6111dee",
   "metadata": {},
   "source": [
    "## Upgrade and install dependencies <a id=\"installdeps\"></a>\n",
    "\n",
    "Run the below cell to install/update Python dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fdf603-a8a9-4888-92b8-b1eca9cc2aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, let's get the latest installations of our dependencies\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install --upgrade --no-deps --force-reinstall boto3\n",
    "!{sys.executable} -m pip install --upgrade --no-deps --force-reinstall botocore\n",
    "!{sys.executable} -m pip install --quiet \"langchain==0.0.339\"\n",
    "!{sys.executable} -m pip install --quiet \"pillow>=9.5,<10\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fd18cf",
   "metadata": {},
   "source": [
    "### Load variables\n",
    "\n",
    "Let's load the variables passed from the prior notebooks to we can access them in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81fac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdaa599",
   "metadata": {},
   "source": [
    "### Import dependencies\n",
    "\n",
    "Let's load some of the dependencies that we'll need for this notebook as well as print their versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a228be2a-b4b4-4862-b31f-f3294d14fbdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "# Get the Boto3 version\n",
    "boto3_version = boto3.__version__\n",
    "\n",
    "# Get the Botocore version\n",
    "botocore_version = botocore.__version__\n",
    "\n",
    "# Print the Boto3 version\n",
    "print(\"Current Boto3 Version:\", boto3_version)\n",
    "\n",
    "# Print the Botocore version\n",
    "print(\"Current Botocore Version:\", botocore_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5530b40-b6df-4f51-a862-025ded60d513",
   "metadata": {},
   "source": [
    "### Initialize AWS service clients\n",
    "\n",
    "Let's initialize the boto3 client to use for S3 and Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d82998-f5d1-4624-a54f-f9d04422b8d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "bedrock = boto3.client(\"bedrock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8e292d-32b2-49ce-b1fb-066b70cc5cde",
   "metadata": {},
   "source": [
    "# Text moderation three ways\n",
    "\n",
    "In this section we will demonstrate how to perform text moderation using the following three techniques.\n",
    "\n",
    "- Train a custom classification model using embeddings generated from an Amazon Bedrock embeddings FM. This is the most complicated but also allows you to train a classifier on your own labeled data.\n",
    "- Amazon Comprehend's [Trust and Safety](https://docs.aws.amazon.com/comprehend/latest/dg/trust-safety.html) API. This technique is simple to implement and provides detailed toxicity labels and scores allowing you to moderate based on the labels that are important to your business.\n",
    "- Build a prompt to ask the Anthropic Claude Instant model to classify text. This is also fairly easy to implement but does not give you scores and can be harder to generate toxicity labels.\n",
    "\n",
    "Let's start with the text embeddings approach.\n",
    "\n",
    "## Classification model trained on text embeddings generated from a Bedrock FM\n",
    "\n",
    "Let's first ask Bedrock to list the foundation models that it currently supports with an output modality of `EMBEDDING`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10722be-2f83-4f8c-89a4-7d9b181029f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = bedrock.list_foundation_models(\n",
    "    byOutputModality = \"EMBEDDING\"\n",
    ")\n",
    "print(json.dumps(response[\"modelSummaries\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6903113",
   "metadata": {},
   "source": [
    "For this technique, we will be using the \"Titan Embeddings G1 - Text\" model which has a modelId of `amazon.titan-embed-text-v1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1616d517",
   "metadata": {},
   "source": [
    "### Prepare custom classification training dataset \n",
    "Download and unzip the sample data toxicity.zip to the local volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4af4f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.download_file(\n",
    "    \"personalize-solution-staging-us-east-1\",\n",
    "    \"personalize-immersionday-travel/toxicity.zip\",\n",
    "    data_dir + \"/toxicity.zip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c18329",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o $data_dir/toxicity.zip -d $data_dir/toxicity_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f1d198",
   "metadata": {},
   "source": [
    "This CSV file contains 500 toxic and 500 non-toxic comments from a variety of popular social media platforms. Click on toxicity_en.csv to see a spreadsheet of 1000 English examples.\n",
    "\n",
    "Columns:\n",
    "- text: the text of the comment\n",
    "- is_toxic: whether or not the comment is toxic\n",
    "\n",
    "(The dataset contained in **$data_dir/toxicity.zip** is an unaltered redistribution of [the toxicity dataset](https://github.com/surge-ai/toxicity) made available by Surge AI under MIT License.)\n",
    "\n",
    "Let's load the dataset into a dataframe. We'll display some of the non-toxic data to avoid displaying anything offensive from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4caae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_df = pd.read_csv(data_dir + \"/toxicity_dataset/toxicity_en.csv\")\n",
    "toxicity_df.loc[toxicity_df[\"is_toxic\"] == \"Not Toxic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bc4fa3-4196-4f4f-b85b-6b492beb8161",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count the number of toxic and not toxic labels\n",
    "toxicity_df[\"is_toxic\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebec00d8-8af4-49c1-8566-0fd865f6810d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f5f862-9a08-4c85-a789-e34ff85cb69c",
   "metadata": {},
   "source": [
    "Embeddings are a key concept in generative AI and machine learning in general. An embedding is a representation of an object (like a word, image, video, etc.) in a vector space. Typically, semantically similar objects will have embeddings that are close together in the vector space. These are very powerful for use-cases like semantic search, recommendations, and classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a67abc-6541-4317-943a-95f7e353eeec",
   "metadata": {},
   "source": [
    "### Define embedding utility function\n",
    "\n",
    "The following function will generate and return an embedding for a piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6629f8e-3500-40e2-8154-c4844a57fd16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")\n",
    "\n",
    "def get_embedding(body, model_id, accept = \"application/json\", content_type = \"application/json\"):\n",
    "    response = bedrock_runtime.invoke_model(body=body, modelId=model_id, accept=accept, contentType=content_type)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    embedding = response_body.get('embedding')\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f467b0-2f52-46d5-8e57-0b13c0c76769",
   "metadata": {},
   "source": [
    "### Generate embedding vectors for labeled text dataset\n",
    "\n",
    "Next we'll generate embeddings for the labeled text in our dataset. This should take about 2 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d219c2c9-27c7-4a80-875d-d4c218119efd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize a list to store the results\n",
    "embeddings = []\n",
    "\n",
    "model_id = \"amazon.titan-embed-text-v1\"\n",
    "\n",
    "print(f\"Generating embeddings for {len(toxicity_df)} labeled text values...this will take a couple minutes...\")\n",
    "\n",
    "for _,row in toxicity_df.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    label = row[\"is_toxic\"]\n",
    "\n",
    "    # Calculate the embedding for the text\n",
    "    body = json.dumps({\"inputText\": text})\n",
    "    embedding = get_embedding(body, model_id)\n",
    "\n",
    "    embeddings.append({\n",
    "        'label': label,\n",
    "        'embedding': embedding\n",
    "    })\n",
    "\n",
    "# The results can be saved to a file so they can be re-used later if necessary.\n",
    "with open('moderation_vectors.json', 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(embeddings, output_file, indent=2)\n",
    "\n",
    "print('Embedding vectors have been saved to moderation_vectors.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a5059e-f8de-43e0-94cd-d19d87bf8a72",
   "metadata": {},
   "source": [
    "### Train classifier model\n",
    "\n",
    "With the embeddings generated, we can now train our classifier.\n",
    "\n",
    "#### Split embeddings for training and evaluation\n",
    "\n",
    "So that we can evaluate the accuracy of the classifier we will split the dataset into training and test. We'll use an 80/20 split where we train on 80% of the data and test on the 20% that was held-out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6bba6e-eccf-4a07-8902-8bdafce1f0d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the first 100 and last 100 records\n",
    "first_100_records = embeddings[:100]\n",
    "last_100_records = embeddings[-100:]\n",
    "\n",
    "# Create 'test.json' with the combined 200 records\n",
    "test_data = first_100_records + last_100_records\n",
    "with open('test.json', 'w') as test_file:\n",
    "    json.dump(test_data, test_file)\n",
    "\n",
    "# Create 'train.json' with the remaining 800 records\n",
    "train_data = embeddings[100:-100]\n",
    "with open('train.json', 'w') as train_file:\n",
    "    json.dump(train_data, train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febb83f9-c3e9-4aeb-8b2c-62c1a1cdd8f3",
   "metadata": {},
   "source": [
    "### Training a model - RandomForestClassifier by embedding vectors\n",
    "\n",
    "The next cell will prepare the training and testing data, train the classifier model with the training data, and then evaluate the model's accuracy against the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759db9e6-c1d1-437f-b80b-ec7d3c0b6815",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "\n",
    "# Extract features (embedding vectors) and labels from the datasets\n",
    "X_train = [data_point[\"embedding\"] for data_point in train_data]\n",
    "y_train = [data_point[\"label\"] for data_point in train_data]\n",
    "\n",
    "X_test = [data_point[\"embedding\"] for data_point in test_data]\n",
    "y_test = [data_point[\"label\"] for data_point in test_data]\n",
    "\n",
    "# Convert lists to numpy arrays for scikit-learn\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Build the classification model (Random Forest in this example)\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate and print precision\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate and print recall\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate and print F1-score\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Calculate and print ROC-AUC score (Note: ROC-AUC is typically used for binary classification)\n",
    "if len(np.unique(y_test)) == 2:  # Check if it's a binary classification problem\n",
    "    roc_auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "    print(\"ROC-AUC:\", roc_auc)\n",
    "\n",
    "# Print the detailed classification report\n",
    "classification_report_str = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", classification_report_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76308f17-55cf-4caf-a60c-87488cdfec0f",
   "metadata": {},
   "source": [
    "### Save the trained classifier model\n",
    "\n",
    "The model can then be saved to a file so that it can be loaded later to perform inference. You can use the [Joblib](https://pypi.org/project/joblib/) library to save and load scikit-learn models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08aea5a-a698-40f8-afee-5e653fa53f42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# saving the model after training:\n",
    "from joblib import dump\n",
    "\n",
    "# Save the trained model to a file\n",
    "dump(clf, 'trained_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72de89b2",
   "metadata": {},
   "source": [
    "Then if later you want to load the trained model, it can be loaded from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727022d6-51e2-4d99-9d8c-2c0bea2426ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the trained model from a file\n",
    "from joblib import load\n",
    "\n",
    "clf = load('trained_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d935ee84",
   "metadata": {},
   "source": [
    "### Classify text using custom classifier model\n",
    "\n",
    "We'll create a utility function to perform inference against the model. This function will generate an embedding for our input text and then use the model to predict the label for the text as well as predict probabilities for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cb4484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text: str, model_id: str) -> (str, list):\n",
    "    embedding = get_embedding(json.dumps({\"inputText\": text}), model_id)\n",
    "    predicted_label = clf.predict([embedding])\n",
    "    probability_estimates = clf.predict_proba([embedding])\n",
    "    return predicted_label[0], probability_estimates[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c9b88c",
   "metadata": {},
   "source": [
    "### Classify sample text\n",
    "\n",
    "To test our classifier, let's test with a string that should be classified as `Toxic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d8a6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = classify_text(\"This destination is horrible, dangerous, and dirty. I hate this place!\", model_id)\n",
    "print(f\"Test string classification: {prediction[0]} with non-toxic/toxic probabilities of {prediction[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec139be",
   "metadata": {},
   "source": [
    "### Classify generated email subject and body\n",
    "\n",
    "Now let's try our our classifier on the generated email subject and body from the last notebook. First, we need to isolate the email subject and title from the generated response from the Claude Instant model. If you recall from the prompt in the last notebook, we asked Claude to place the email title/subject and body within XML tags. This instruction in the prompt allows us to more easily parse the response to separate these two pieces of content. To do so, we'll wrap the output in an outer `<email></email>` tag and then parse it as an XML document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06203e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "root = ET.fromstring(\"<email>\" + email2 + \"</email>\")\n",
    "subject = root.find(\"email_title\").text.strip()\n",
    "body = root.find(\"email_body\").text.strip()\n",
    "\n",
    "print(f\"Email subject: {subject}\")\n",
    "print(f\"Email body: {body}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4c5163",
   "metadata": {},
   "source": [
    "### Clasify email subject\n",
    "\n",
    "Let's start with the email subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a3705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = classify_text(subject, model_id)\n",
    "print(f\"Email subject classification: {prediction[0]} with non-toxic/toxic probabilities of {prediction[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d200a77",
   "metadata": {},
   "source": [
    "### Clasify email body\n",
    "\n",
    "Now let's run the email body through the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fafac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = classify_text(body, model_id)\n",
    "print(f\"Email body classification: {prediction[0]} with non-toxic/toxic probabilities of {prediction[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42280019",
   "metadata": {},
   "source": [
    "## Moderate text using Amazon Comprehend\n",
    "\n",
    "Next let's evaluate the [Trust and Safety](https://docs.aws.amazon.com/comprehend/latest/dg/trust-safety.html) API in Amazon Comprehend against our sample toxic text and email subject and body.\n",
    "\n",
    "First, let's evaluate the sample toxic text we used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeb312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend = boto3.client(\"comprehend\")\n",
    "\n",
    "response = comprehend.detect_toxic_content(\n",
    "    LanguageCode=\"en\",\n",
    "    TextSegments=[{\"Text\": \"This destination is horrible, dangerous, and dirty. I hate this place!\"}]\n",
    ")\n",
    "print(json.dumps(response[\"ResultList\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be347368",
   "metadata": {},
   "source": [
    "How did Comprehend do compared to the custom model? How close is the toxicity score between the two models? Notice that Comprehend provides more granular details and scores in its response. This allows you to moderate text based on different categories such as insults or harrassment.\n",
    "\n",
    "Next let's see how our generated email subject and body do with Comprehend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92801180",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = comprehend.detect_toxic_content(\n",
    "    LanguageCode=\"en\",\n",
    "    TextSegments=[{\"Text\": subject}]\n",
    ")\n",
    "print(json.dumps(response[\"ResultList\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cf9472",
   "metadata": {},
   "source": [
    "Since the [DetectToxicContent](https://docs.aws.amazon.com/comprehend/latest/APIReference/API_DetectToxicContent.html) API has a limit of 1000 characters per text segment (and 10 segments) per call, we will implement rudimentary tokenizer logic to split the email body into 1000 character text segments by whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2878f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "segments = []\n",
    "if len(body) > 1000 and len(segments) < 10:\n",
    "    tokens = WhitespaceTokenizer().tokenize(body)\n",
    "    text = \"\"\n",
    "    for token in tokens:\n",
    "        if len(text) + 1 + len(token) > 1000:\n",
    "            segments.append({\"Text\": text})\n",
    "            text = token\n",
    "        else:\n",
    "            text += \" \" + token\n",
    "\n",
    "    if len(text) > 0:\n",
    "        segments.append({\"Text\": text})\n",
    "else:\n",
    "    segments.append({\"Text\": body})\n",
    "\n",
    "print(f\"Split the body text into {len(segments)} segments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759b55e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = comprehend.detect_toxic_content(\n",
    "    LanguageCode=\"en\",\n",
    "    TextSegments=segments\n",
    ")\n",
    "print(json.dumps(response[\"ResultList\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f64cee",
   "metadata": {},
   "source": [
    "The output should have listed toxity scores across all segments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e58b7c",
   "metadata": {},
   "source": [
    "## Moderate text using a prompt and Anthropic Claude Instant\n",
    "\n",
    "First, let's define a moderation policy that can be used in our content moderation prompt. We'll use the [toxicity label definitions](https://docs.aws.amazon.com/comprehend/latest/dg/trust-safety.html#toxicity-detection) used by Amazon Comprehend to build a policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de3420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "moderation_policy = \"\"\"\n",
    "1. GRAPHIC: Graphic speech uses visually descriptive, detailed, and unpleasantly vivid imagery. Such language is often made verbose to amplify an insult, discomfort or harm to the recipient.\n",
    "2. HARASSMENT_OR_ABUSE: Speech that imposes disruptive power dynamics between the speaker and hearer, regardless of intent, seeks to affect the psychological well-being of the recipient, or objectifies a person.\n",
    "3. HATE_SPEECH: Speech that criticizes, insults, denounces or dehumanizes a person or a group on the basis of an identity, be it race, ethnicity, gender identity, religion, sexual orientation, ability, national origin, or another identity-group.\n",
    "4. INSULT: Speech that includes demeaning, humiliating, mocking, insulting, or belittling language.\n",
    "5. PROFANITY: Speech that contains words, phrases or acronyms that are impolite, vulgar, or offensive is considered as profane.\n",
    "6. SEXUAL: Speech that indicates sexual interest, activity or arousal by using direct or indirect references to body parts or physical traits or sex .\n",
    "7. VIOLENCE_OR_THREAT: Speech that includes threats which seek to inflict pain, injury or hostility towards a person or group.\n",
    "8. TOXICITY: Speech that contains words, phrases or acronyms that might be considered toxic in nature across any of the above categories.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f5a52c",
   "metadata": {},
   "source": [
    "Now let's create a utility function to create a prompt based on specific content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ab586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_moderation_prompt(content: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "Human: The following is our company's content moderation policy.\n",
    "\n",
    "{moderation_policy}\n",
    "\n",
    "Based on the content moderation policy, tell me if the text within the <content> XML tag containes unsafe content, also give its category and reason if it's unsafe.\n",
    "\n",
    "<content>\n",
    "{content}\n",
    "</content>\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12032663",
   "metadata": {},
   "source": [
    "Next we'll setup LangChain to send our prompt to Anthropic's Claude Instant FM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89961c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "inference_modifier = {'max_tokens_to_sample':4096,\n",
    "                      \"temperature\":0.7,\n",
    "                      \"top_k\":250\n",
    "                     }\n",
    "\n",
    "textgen_llm = Bedrock(model_id = \"anthropic.claude-instant-v1\", model_kwargs = inference_modifier, client = bedrock_runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62339dc0",
   "metadata": {},
   "source": [
    "Now we're ready to test our prompt. We'll first test with the same mildly offensive text we used with the classifier model and Amazon Comprehend. Let's create the prompt and display what we will pass to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb72687",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = create_moderation_prompt(\"This destination is horrible, dangerous, and dirty. I hate this place!\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662db42d",
   "metadata": {},
   "source": [
    "Finally, we can send the prompt to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614ffe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = textgen_llm(prompt).strip()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86ee499",
   "metadata": {},
   "source": [
    "How is the response? Can you think of any ideas to improve the prompt's accuracy and output?\n",
    "\n",
    "Let's try it with the email subject and body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bad545",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = create_moderation_prompt(subject)\n",
    "response = textgen_llm(prompt).strip()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b8d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = create_moderation_prompt(body)\n",
    "response = textgen_llm(prompt).strip()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f13a6f",
   "metadata": {},
   "source": [
    "That does it for text moderation. Hopefully you've seen that there are many techniques to implementing content moderation on text. We've only touched on a few of them in this notebook.\n",
    "\n",
    "Now let's move on to moderating images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d54ae7",
   "metadata": {},
   "source": [
    "# Moderate images\n",
    "\n",
    "Finally let's explore how we can add moderation for the images generated with foundation models like Stable Diffusion. We'll use the AWS AI service, [Amazon Rekognition](https://aws.amazon.com/rekognition/), for this task.\n",
    "\n",
    "## Moderate sample image\n",
    "We'll start with a sample image that should flag some suggestive content in an image. The image contains a man smoking a cigarette and Rekognition Image Moderation will label it with \"Tobacco\" and \"Smoking\".\n",
    "\n",
    "Here is the sample image:\n",
    "\n",
    "![Moderate Image](images/man-smoking-cigarette.jpg \"Test image to moderate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db694f73",
   "metadata": {},
   "source": [
    "### Upload and moderate sample image\n",
    "\n",
    "Let's upload the image to our S3 bucket to stage it for moderation by Amazon Rekognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8efdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_key = 'content-moderation-im/image-moderation/man-smoking-cigarette.jpg'\n",
    "s3.upload_file(\"images/man-smoking-cigarette.jpg\", bucket_name, s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ace965",
   "metadata": {},
   "source": [
    "Next we'll create an SDK client for Rekognition and call the [DetectModerationLabels](https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DetectModerationLabels.html) API for our test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d3684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rekognition = boto3.client('rekognition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bf5e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rekognition.detect_moderation_labels(\n",
    "    Image={\n",
    "       'S3Object': {\n",
    "           'Bucket': bucket_name,\n",
    "           'Name': s3_key,\n",
    "       }\n",
    "    }\n",
    ")\n",
    "print(json.dumps(response[\"ModerationLabels\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fb0eac",
   "metadata": {},
   "source": [
    "Note the response from a call to DetectModerationLabels:\n",
    "\n",
    "- ModerationLabels – The example shows a list of labels for inappropriate or offensive content found in the image. The list includes the top-level label and each second-level label that are detected in the image. Please see Amazon Rekognition doucmentation for the complete list of supported top/second level labels.\n",
    "\n",
    "- Name/ParentName – Each label has a name, an estimation of the confidence that Amazon Rekognition has that the label is accurate, and the name of its parent label. The parent name for a top-level label is \"\".\n",
    "\n",
    "- Confidence – Each label has a confidence value between 0 and 100 that indicates the percentage confidence that Amazon Rekognition has that the label is correct. You specify the minimal confidence level for a label to be returned in the response in the API operation request.\n",
    "\n",
    "As we can see in the Rekognition response, the Image Moderation API labeled the image in 2 categories with confidence scores:\n",
    "\n",
    "- Top level category: Tobacco with a confidence score > 98%\n",
    "- Second level category: Smoking with a confidence score > 98%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997dc63f",
   "metadata": {},
   "source": [
    "## Moderate email campaign banner images\n",
    "\n",
    "Now let's run the generated images from the last notebook through Rekognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee29a1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(image_1_path)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c20cb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_key = 'content-moderation-im/image-moderation/image_1.png'\n",
    "s3.upload_file(image_1_path, bucket_name, s3_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df5bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rekognition.detect_moderation_labels(\n",
    "    Image={\n",
    "       'S3Object': {\n",
    "           'Bucket': bucket_name,\n",
    "           'Name': s3_key,\n",
    "       }\n",
    "    }\n",
    ")\n",
    "print(json.dumps(response[\"ModerationLabels\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302f40b3",
   "metadata": {},
   "source": [
    "If there were no moderation labels returned (i.e., an empty list `[]`), then there were no findings from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fa3cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(image_2_path)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508ff68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_key = 'content-moderation-im/image-moderation/image_2.png'\n",
    "s3.upload_file(image_2_path, bucket_name, s3_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce63590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rekognition.detect_moderation_labels(\n",
    "    Image={\n",
    "       'S3Object': {\n",
    "           'Bucket': bucket_name,\n",
    "           'Name': s3_key,\n",
    "       }\n",
    "    }\n",
    ")\n",
    "print(json.dumps(response[\"ModerationLabels\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e10960",
   "metadata": {},
   "source": [
    "If there were no moderation labels returned (i.e., an empty list `[]`), then there were no findings from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d7551b",
   "metadata": {},
   "source": [
    "# Where to from here\n",
    "\n",
    "In these notebooks we have focused on optimizing the \"who\" and \"what\" in the email marketing campaigns for our ficticious travel company. Optimizing the \"who\" involved using Amazon Personalize's user segmentation recipe to train a ML model that identified users with an affinity for the trip we wanted to promote. Then we turned to optimizing the \"what\" by using Amazon Bedrock to generate an appealing email subject, body, and banner image. What we didn't cover is the \"how\" to deliver these messages. AWS provides services such as [Amazon Pinpoint](https://aws.amazon.com/pinpoint/) and [Amazon Simple Email Service (SES)](https://aws.amazon.com/ses/) that make it easy to deliver, manage, and measure the email delivery process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38305af2",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook we illustrated how to train a random forest classifier model using embeddings generated using the Amazon Titan Text Embeddings FM from Amazon Bedrock. This model was then used to test the email subject and email body generated in the previous notebook for unsafe and inappropriate content. Then we used the AWS AI service [Amazon Rekognition](https://aws.amazon.com/rekognition/) to perform a similar analysis of the images we generated in the previous notebook. Adding a content moderation step to the use of generative AI can help safeguard against unsafe and inappropriate content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7844166",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "\n",
    "To clean up the Amazon Personalize resources created in the first notebook, you can execute the [`04_Clean_Up.ipynb`](04_Clean_Up.ipynb) notebook. If you're running these notebooks as part of an AWS-led workshop where temporary AWS accounts are provided for you, this cleanup will be done automatically for you. Otherwise, if you're running this notebook in a personal or work account, be sure to run the [`04_Clean_Up.ipynb`](04_Clean_Up.ipynb) notebook to shutdown resources that can create ongoing AWS charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf59dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
