{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Travel Marketing Campaign with Generative AI workshop<a class=\"anchor\" id=\"top\"></a>\n",
    "\n",
    "Congratulations! You have just been hired as the marketing manager for an online travel company. Your task is to more effectively promote the flights for an airline through email campaigns using AI. To improve end user engagement (increase click-through-rate) and reduce churn (decrease email unsubscriptions), you will use AI to identify customers with an affinity for specific flights and then use generative AI to to generate marketing content to those users. \n",
    "\n",
    "You’ve been asked by the founders to:\n",
    "\n",
    "- Increase subscriber engagement by tailoring every travel promotions to individual users\n",
    "- Help improve the enjoyment of trips taken by subscribers through personalized recommendations\n",
    "- Increase the breadth of content offered to them from the travel catalog\n",
    "- Reduce the unsubscribe rate of the current non-trageted and non-personalized emails\n",
    "\n",
    "Throughout the course of this workshop you will be exploring your datasets, building/training a user segmentation model, generating user segments with an affinity for specific trips, generating engaging email messages with text and images using generative AI.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> importing and training the datasets will take longer than we have in this workshop. In order to complete this workshop within the time set, we have pre-created several resources on your behalf.  However, the notebooks are designed in such a way that all of the steps are included. If the resources have already been created the notebook cell will return information about the resources, if the resources have not been created, it will create them for you. \n",
    "</div>\n",
    "\n",
    "\n",
    "## In this notebook\n",
    "In this notebook, you will explore the travel company's dataset and prepare it for use with Amazon Personalize to train a user segmentation model. This model will be used to identify the users that have an affinity for the trip that we will be promoting.  \n",
    "\n",
    "1. [How to Use the Notebook](#usenotebook)\n",
    "1. [Introduction to Amazon Personalize Datasets](#datasets)\n",
    "1. [Choose a Dataset or Data Source](#source)\n",
    "1. [Configure an S3 bucket and an IAM role](#bucket_role)\n",
    "1. [Create dataset group](#group_dataset)\n",
    "1. [Create the Interactions Schema](#interact_schema)\n",
    "1. [Create the Items Schema](#items_schema)\n",
    "1. [Create the Users Schema](#users_schema)\n",
    "1. [Import the Interactions Data](#import_interactions)\n",
    "1. [Import the Items Metadata](#import_items)\n",
    "1. [Import the User Metadata](#import_users)\n",
    "1. [Train user segmentation ML Model](#solutions)\n",
    "1. [Evaluate solution version metrics](#eval)\n",
    "1. [Create batch segment job](#batchsegmentjob)\n",
    "1. [Storing Useful Variables](#vars)\n",
    "\n",
    "In the next notebook we'll generate the promotional email, including text and an image, using generative AI and in the final notebook we'll perform some moderation checks on the generated text to ensure it conforms to the travel company's standards.\n",
    "\n",
    "## How to use the notebook <a class=\"anchor\" id=\"usenotebook\"></a>\n",
    "\n",
    "### Executing cells\n",
    "\n",
    "The code is broken up into cells like the one below. There's a triangular Run button at the top of this page that you can click to execute each cell and move onto the next, or you can press `Shift` + `Enter` while in the cell to execute it and move onto the next one.\n",
    "\n",
    "As a cell is executing you'll notice a line to the side showcase an `*` while the cell is running or it will update to a number to indicate the last cell that completed executing after it has finished exectuting all the code within a cell.\n",
    "\n",
    "Simply follow the instructions below and execute the cells to get started with Amazon Personalize.\n",
    "\n",
    "### Understanding the code\n",
    "\n",
    "This notebook can be used in two modalities:\n",
    "\n",
    "1. Train as you go by executing each cell. Some cells may take a long time to finish executing as they wait for resources to be created.\n",
    "2. Use this notebook with previously created resources. All or the majority of the resources will already be created and cells will just retrieve the information of these existing resources to use them in following steps.\n",
    "\n",
    "Because of this, you will find that some cells have `try` and `except` blocks. In particular most of them are handling a `ResourceAlreadyExistsException` exception. \n",
    "\n",
    "You can look at the code in the `try` block to get a good idea of how you can create a resource and understand how to use the Amazon Personalize SDK. The `except` block will let you know that the resource has been created and record the corresponding ARN, which is the Amazon unique identifier.\n",
    "\n",
    "This is an example of the `try` block for creating a dataset group, this code will execute without exceptions if the dataset group does not exist and raise an exception if the dataset group does already exist:\n",
    "\n",
    "```python\n",
    "try:     \n",
    "    # Try to create the dataset group, this block with exectute fully if the dataset group does not exist yet\n",
    "    \n",
    "    create_dataset_group_response = personalize.create_dataset_group(\n",
    "        name = workshop_dataset_group_name\n",
    "    )\n",
    "    workshop_dataset_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "    print(json.dumps(create_dataset_group_response, indent=2))\n",
    "    print ('\\nCreating the Dataset Group with dataset_group_arn = {}'.format(workshop_dataset_group_arn))\n",
    "```\n",
    "and this is the corresponding `except` block that will be exectuted if an exeption is raised because the dataset group already exists. This block saves the ARN for the existig dataset group to use in subsequent cells and lets you know the resource already exists.\n",
    "\n",
    "```python\n",
    "except personalize.exceptions.ResourceAlreadyExistsException as e:\n",
    "    workshop_dataset_group_arn = 'arn:aws:personalize:'+region+':'+account_id+':dataset group/' + \n",
    "        workshop_dataset_group_name \n",
    "    print ('\\nThe the Dataset Group with dataset_group_arn = {} already exists'.format(\n",
    "        workshop_dataset_group_arn))\n",
    "    print ('\\nWe will be using the existing Dataset Group dataset_group_arn = {}'.format(\n",
    "        workshop_dataset_group_arn))\n",
    "```\n",
    "\n",
    "Depending on the resource, you may also find that sometimes the code will check from a list of resourses to find if a resource exists and then use `if` and `else` blocks to either use the existing resource or create it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build!\n",
    "\n",
    "Python ships with a broad collection of libraries and we need to import those as well as the ones installed to help us like [boto3](https://aws.amazon.com/sdk-for-python/) (AWS SDK for python) and [Pandas](https://pandas.pydata.org/)/[Numpy](https://numpy.org/) which are core data science tools. Most of these libraries are already installed on the notebook instance but others we need to install or upgrade to the latest version. Let's get that out of the way first.\n",
    "\n",
    "### Upgrade and install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install --upgrade --no-deps --force-reinstall boto3\n",
    "!{sys.executable} -m pip install --upgrade --no-deps --force-reinstall botocore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import depedencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from time import sleep\n",
    "import csv\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"poc_data\"\n",
    "!mkdir $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize clients and inspect environment\n",
    "\n",
    "Next we'll create the SDK client needed to interact with Amazon Personalize and identify some environment values such as the current AWS account ID and region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the SDK to Personalize:\n",
    "personalize = boto3.client('personalize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the account id and region to use later\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "print(\"Account ID:\", account_id)\n",
    "\n",
    "with open('/opt/ml/metadata/resource-metadata.json') as notebook_info:\n",
    "    data = json.load(notebook_info)\n",
    "    resource_arn = data['ResourceArn']\n",
    "    region = resource_arn.split(':')[3]\n",
    "print(\"Region:\", region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is a workshop and the resources were created for you, we will retrieve the variables of the resources created. Otherwise, default values will be used and resources will be created as you step through the rest of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the Personalize recipe ARN for this workshop (more details later)\n",
    "workshop_item_affinity_recipe_arn = \"arn:aws:personalize:::recipe/aws-item-affinity\"\n",
    "\n",
    "# Verify that parameter configuation file exists.\n",
    "parameter_config_file = \"/home/ec2-user/SageMaker/amazon-personalize-immersion-day/automation/ml_ops/domain/Travel-Gen-AI-Pretrained/params.json\"\n",
    "if os.path.isfile(parameter_config_file):\n",
    "    # Load parameters\n",
    "    with open(parameter_config_file, \"r\") as file:\n",
    "        parameters = json.load(file)\n",
    "\n",
    "        workshop_dataset_group_name = parameters['datasetGroup']['serviceConfig']['name']\n",
    "\n",
    "        interactions_schema_name = parameters['datasets']['interactions']['schema']['serviceConfig']['name']\n",
    "        interactions_dataset_name = parameters['datasets']['interactions']['dataset']['serviceConfig']['name']\n",
    "\n",
    "        items_schema_name = parameters['datasets']['items']['schema']['serviceConfig']['name']\n",
    "        items_dataset_name = parameters['datasets']['items']['dataset']['serviceConfig']['name']\n",
    "\n",
    "        users_schema_name = parameters['datasets']['users']['schema']['serviceConfig']['name']\n",
    "        users_dataset_name = parameters['datasets']['users']['dataset']['serviceConfig']['name']\n",
    "\n",
    "        # The following job names are the prefixes of the job names that can be created\n",
    "        interactions_import_job_name = 'dataset_import_interaction'\n",
    "        items_import_job_name = 'dataset_import_item'\n",
    "        users_import_job_name = 'dataset_import_user'\n",
    "\n",
    "        for solution in parameters['solutions']:\n",
    "            # This is currently configured assuming only one solution of this type, if there are multiple\n",
    "            # solutions of the same type further configuration is needed.\n",
    "            if solution['serviceConfig']['recipeArn'] == workshop_item_affinity_recipe_arn:\n",
    "                workshop_item_affinity_solution_name = solution['serviceConfig']['name']\n",
    "\n",
    "        interactions_file_path = \"/home/ec2-user/SageMaker/amazon-personalize-immersion-day/automation/ml_ops/domain/Travel-Gen-AI-Pretrained/data/Interactions/travel_interactions.csv\"\n",
    "        interactions_file_name = \"interactions.csv\"\n",
    "\n",
    "        users_file_path = \"/home/ec2-user/SageMaker/amazon-personalize-immersion-day/automation/ml_ops/domain/Travel-Gen-AI-Pretrained/data/Users/travel_users.csv\"\n",
    "        users_file_name = 'users.csv'\n",
    "\n",
    "        items_file_path = \"/home/ec2-user/SageMaker/amazon-personalize-immersion-day/automation/ml_ops/domain/Travel-Gen-AI-Pretrained/data/Items/travel_items.csv\"\n",
    "        items_file_name = 'items.csv'\n",
    "\n",
    "        print(f\"Parameters loaded from {parameter_config_file}\")\n",
    "else:\n",
    "    print(\"Parameters file not found on local volume; using default values for Personalize resources\")\n",
    "    workshop_dataset_group_name = \"personalize-poc-travel\"\n",
    "\n",
    "    interactions_schema_name = \"workshop_travel_interactions_schema\"\n",
    "    interactions_dataset_name = \"workshop_travel_interactions\"\n",
    "\n",
    "    items_schema_name = \"workshop_travel_items_schema\"\n",
    "    items_dataset_name = \"workshop_travel_items\"\n",
    "\n",
    "    users_schema_name = \"workshop_travel_users_schema\"\n",
    "    users_dataset_name = \"workshop_travel_users\"\n",
    "\n",
    "    # The following job names are the prefixes of the job names that can be created\n",
    "    interactions_import_job_name = \"dataset_import_interaction\"\n",
    "    items_import_job_name = \"dataset_import_item\"\n",
    "    users_import_job_name = \"dataset_import_user\"\n",
    "\n",
    "    workshop_item_affinity_solution_name = \"workshop_item_affinity_travel\"\n",
    "\n",
    "    interactions_file_path = \"s3://personalize-solution-staging-us-east-1/personalize-immersionday-travel/travel_interactions.csv\"\n",
    "    interactions_file_name = \"interactions.csv\"\n",
    "\n",
    "    users_file_path = \"s3://personalize-solution-staging-us-east-1/personalize-immersionday-travel/travel_users.csv\"\n",
    "    users_file_name = 'users.csv'\n",
    "\n",
    "    items_file_path = \"s3://personalize-solution-staging-us-east-1/personalize-immersionday-travel/travel_items.csv\"\n",
    "    items_file_name = 'items.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Amazon Personalize datasets <a class=\"anchor\" id=\"datasets\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "[Amazon Personalize](https://aws.amazon.com/personalize/) is a fully managed machine learning (ML) service that uses your data to train ML models that can be used to generate item recommendations for your users. It can also generate user segments based on users’ affinity for certain items or item metadata.\n",
    "\n",
    "Regardless of the use case, the algorithms all share a base of learning on user-item-interaction data which is defined by 3 core attributes:\n",
    "\n",
    "1. **UserID** - The user who interacted\n",
    "1. **ItemID** - The item the user interacted with\n",
    "1. **Timestamp** - The time at which the interaction occurred\n",
    "\n",
    "Generally speaking your data will not arrive in a perfect form for Personalize, and will take some modification to be structured correctly. Since the data provided with this workshop already has the datasets prepared, this step is done for you.\n",
    "\n",
    "### Interactions data\n",
    "\n",
    "The interaction dataset consists of information about the interactions the users of the fictional app will have with the content. For this workshop, the interactions represent past trips that users have taken or trips they have shown interest in in the travel application. This data usually comes from order processing systems, analytics tools, or a customer data platform (CDP). The best interaction data for use for Amazon Personalize would include the sequential order of user behavior, what content was clicked on/purchased and the order it was interacted with. To simulate our interaction data, we will be using a synthetic interactions dataset. \n",
    "\n",
    "### Items data\n",
    "\n",
    "The item data consists of information about the content that is being interacted with, this typically comes from a content management system (CMS). For the purpose of this workshop our items are trips that are available to purchase in the travel application. To simulate our items data we will be using a synthetic item dataset.\n",
    "\n",
    "### User data\n",
    "\n",
    "The user data is what information you have about your users, it usually comes from a customer relationship management (CRM) or subscriber management system. We will be generating a small synthetic dataset to simulate this component of the workshop. This dataset is not manatory, but providing good user metadata will ensure the best results in your trained models.\n",
    "\n",
    "In this notebook we will be inspecting the datasets to become familiar with their contents. Then we will import the datasets into Amazon Personalize so that it can be used to train recommendation models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open and explore the simulated travel interactions dataset\n",
    "\n",
    "For this example we are using a synthetic dataset generated when you deployed your working environment via CloudFormation. Let's load the dataset into a dataframe and display some rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_data = pd.read_csv(interactions_file_path)\n",
    "interaction_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note of a few aspects of the CSV data:\n",
    "\n",
    "- An `ITEM_ID` column of the item interacted with\n",
    "- A `USER_ID` column of the user who interacted\n",
    "- A `TIMESTAMP` of when the interaction happened (represented as a Unix timestamp)\n",
    "- A custom field called `CABIN_TYPE` that represents the type of flight ticket purchased by the user\n",
    "- An `EVENT_VALUE` representing the user's rating of the flight on a 10 point scale\n",
    "- An `EVENT_TYPE` column which can be used to train different Personalize campaigns and also to filter on recommendations; in this case we're only working with flight ratings by users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Amazon Personalize, you need to private timestamp values in Unix Epoch format.\n",
    "\n",
    "Lets validate that the timestamp is actually in a Unix Epoch format by converting it into a more easily understood time/date format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arb_time_stamp = interaction_data.iloc[50]['TIMESTAMP']\n",
    "print('timestamp')\n",
    "print(arb_time_stamp)\n",
    "print()\n",
    "print('Date & Time')\n",
    "print(datetime.utcfromtimestamp(arb_time_stamp).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formatted timestamp looks reasonable given our data so we're good to move on.\n",
    "\n",
    "We will do some general summarization and inspection of the data to ensure that it will be helpful for Amazon Personalize. The following cell will tell us if any columns have null value(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to local storage\n",
    "interaction_data.to_csv(data_dir+\"/\"+interactions_file_name, index=False, float_format='%.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Users dataset\n",
    "[Back to top](#top)\n",
    "\n",
    "Next let's load and inspect the users dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = pd.read_csv(users_file_path)\n",
    "user_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the required `USER_ID` column, we have a single custom field named `MEMBER_CLASS`, which identifies the membership tier of each user.\n",
    "\n",
    "Let's take a closer look at the distribution of users across the tiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_attributes = user_data.select_dtypes(include = ['object'])\n",
    "\n",
    "plt.figure(figsize=(16,3))\n",
    "chart = sns.countplot(data = class_attributes, x = 'MEMBER_CLASS')\n",
    "plt.xticks(rotation=90, horizontalalignment='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to local storage\n",
    "user_data.to_csv(data_dir+\"/\"+users_file_name, index=False, float_format='%.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Items dataset\n",
    "\n",
    "Finally, let us look at the items dataset which includes metadata about the flights rated by our users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_data = pd.read_csv(items_file_path)\n",
    "item_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note of a few aspects of the CSV data:\n",
    "\n",
    "- An `ITEM_ID` column that represents the item; a flight or trip in our case\n",
    "- A custom field called `DST_CITY` that represents the destination city of the trip\n",
    "- A custom field called `SRC_CITY` that represents the source city of the trip\n",
    "- A custom field called `AIRLINE` that represents the ficticious airline for of the trip\n",
    "- A custom field called `DISCOUNT_FOR_MEMBER` that represents the discount percentage that members of our travel company receive for the flight\n",
    "- A custom field called `EXPIRED` that indicates whether the flight/trip is still available or not\n",
    "\n",
    "Besides training on these fields, we will also use them to lookup flights to promote when we build our marketing campaign in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to local storage\n",
    "item_data.to_csv(data_dir+\"/\"+items_file_name, index=False, float_format='%.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Amazon Personalize Resources and importing data <a class=\"anchor\" id=\"import\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure an S3 bucket and IAM  role <a class=\"anchor\" id=\"bucket_role\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "So far, we have downloaded, inspected, and saved the data onto the Amazon EBS instance attached to instance running this Jupyter notebook.  \n",
    "\n",
    "By default, the Personalize service does not have permission to acccess the data we uploaded into the S3 bucket in our account. In order to grant access to the  Personalize service to read our CSVs, we need to set a Bucket Policy and create an IAM role that the Amazon Personalize service will assume. Let's set all of that up.\n",
    "\n",
    "Use the metadata stored on the instance underlying this Amazon SageMaker notebook, to determine the region it is operating in. If you are using a Jupyter notebook outside of Amazon SageMaker, simply define the region as a string below. The Amazon S3 bucket needs to be in the same region as the Amazon Personalize resources we have been creating so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon S3 bucket names are globally unique. To create a unique bucket name, the code below will append the string `personalize-poc-travel` to your AWS account number. Then it creates a bucket with this name in the current region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "bucket_name = account_id + \"-\" + region + \"-\" + \"personalize-poc-travel\"\n",
    "\n",
    "#getting existing buckets in the account\n",
    "response = s3.list_buckets()\n",
    "\n",
    "if bucket_name in [x['Name'] for x in response['Buckets']]:\n",
    "    print(\"The bucket already exists.\")\n",
    "else:\n",
    "    if region == \"us-east-1\":\n",
    "        bucket_responese = s3.create_bucket(Bucket=bucket_name)\n",
    "    else:\n",
    "        bucket_responese = s3.create_bucket(\n",
    "            Bucket=bucket_name,\n",
    "            CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "print('bucket_name:', bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Personalize needs to be able to read the contents of your S3 bucket. So add a bucket policy which allows that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Id\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"personalize.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\",\n",
    "                \"s3:PutObject\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::{}\".format(bucket_name),\n",
    "                \"arn:aws:s3:::{}/*\".format(bucket_name)\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "bucket_current_policy = None\n",
    "\n",
    "try:\n",
    "    bucket_current_policy = s3.get_bucket_policy(Bucket=bucket_name)['Policy']\n",
    "\n",
    "except s3.exceptions.from_code('NoSuchBucketPolicy') as e:\n",
    "    print(\"There is no current Bucket Policy for bucket \" + bucket_name)\n",
    "\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "\n",
    "if (bucket_current_policy and policy == json.loads(bucket_current_policy)):\n",
    "    print (\"The policy is already associated with the S3 Bucket.\")\n",
    "else:\n",
    "    print (\"Adding the policy to the bucket.\")\n",
    "    print(s3.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(policy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an IAM role\n",
    "\n",
    "Amazon Personalize needs the ability to assume roles in AWS in order to have the permissions to execute certain tasks. Let's create an IAM role and attach the required policies to it. The code below attaches very permissive policies; please use more restrictive policies for any production application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client(\"iam\")\n",
    "\n",
    "role_name = account_id+\"-PersonalizeS3-Immersion-Day\"\n",
    "assume_role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "          \"Effect\": \"Allow\",\n",
    "          \"Principal\": {\n",
    "            \"Service\": \"personalize.amazonaws.com\"\n",
    "          },\n",
    "          \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    create_role_response = iam.create_role(\n",
    "        RoleName = role_name,\n",
    "        AssumeRolePolicyDocument = json.dumps(assume_role_policy_document)\n",
    "    );\n",
    "    role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "\n",
    "except iam.exceptions.EntityAlreadyExistsException as e:\n",
    "    print('Warning: role already exists:', e)\n",
    "    role_arn = iam.get_role(\n",
    "        RoleName = role_name\n",
    "    )[\"Role\"][\"Arn\"];\n",
    "\n",
    "print('IAM Role: {}\\n'.format(role_arn))\n",
    "\n",
    "# Attach the policy if it is not previously attached:\n",
    "policy_arn = \"arn:aws:iam::aws:policy/AmazonS3FullAccess\"\n",
    "\n",
    "if (policy_arn in [ x['PolicyArn'] for x in iam.list_attached_role_policies( RoleName = role_name)['AttachedPolicies']]):\n",
    "    print ('The policy {} is already attached to this role.'.format(policy_arn))\n",
    "else:\n",
    "    print (\"Attaching the role_policy\")\n",
    "    attach_response = iam.attach_role_policy(\n",
    "        RoleName = role_name,\n",
    "        PolicyArn = \"arn:aws:iam::aws:policy/AmazonS3FullAccess\"\n",
    "    );\n",
    "    print (\"30s pause to allow role to be fully consistent.\")\n",
    "    time.sleep(30)\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3\n",
    "\n",
    "Now that your Amazon S3 bucket has been created, upload the CSV files of our 3 datasets (Item, Interaction and User)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_file_path = data_dir + \"/\" + interactions_file_name\n",
    "\n",
    "try:\n",
    "    s3.get_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=interactions_file_path,\n",
    "    )\n",
    "    print(\"{} already exists in the bucket {}\".format(interactions_file_path, bucket_name))\n",
    "except s3.exceptions.NoSuchKey:\n",
    "    # Uploading the file if it does not already exist\n",
    "    boto3.Session().resource('s3').Bucket(bucket_name).Object(interactions_file_name).upload_file(interactions_file_path)\n",
    "    print(\"File {} uploaded to bucket {}\".format(interactions_file_name, bucket_name))\n",
    "\n",
    "items_file_path = data_dir + \"/\" + items_file_name\n",
    "\n",
    "try:\n",
    "    s3.get_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=items_file_name,\n",
    "    )\n",
    "    print(\"{} already exists in the bucket {}\".format(items_file_path, bucket_name))\n",
    "except s3.exceptions.NoSuchKey:\n",
    "    # Uploading the file if it does not already exist\n",
    "    boto3.Session().resource('s3').Bucket(bucket_name).Object(items_file_name).upload_file(items_file_path)\n",
    "    print(\"File {} uploaded to bucket {}\".format(items_file_name, bucket_name))\n",
    "\n",
    "users_file_path = data_dir + \"/\" + users_file_name\n",
    "\n",
    "try:\n",
    "    s3.get_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=users_file_name,\n",
    "    )\n",
    "    print(\"{} already exists in the bucket {}\".format(users_file_path, bucket_name))\n",
    "except s3.exceptions.NoSuchKey:\n",
    "    # Uploading the file if it does not already exist\n",
    "    boto3.Session().resource('s3').Bucket(bucket_name).Object(users_file_name).upload_file(users_file_path)\n",
    "    print(\"File {} uploaded to bucket {}\".format(users_file_name, bucket_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Amazon Personalize dataset group <a class=\"anchor\" id=\"group_dataset\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "The highest level of isolation and abstraction with Amazon Personalize is a *dataset group*. Information stored within one of these dataset groups has no impact on any other dataset group or models created from one - they are completely isolated. This allows you to run many experiments and is part of how we keep your models private and fully trained only on your data. \n",
    "\n",
    "Before importing the data prepared earlier, there needs to be a dataset group and a dataset added to it that handles the interactions.\n",
    "\n",
    "Dataset groups can contain the following types of information:\n",
    "\n",
    "* User-item-interactions\n",
    "* Event streams (real-time interactions)\n",
    "* User metadata\n",
    "* Item metadata\n",
    "\n",
    "We need to create the dataset group that will contain our three datasets.\n",
    "\n",
    "Your dataset group can be one of the following types:\n",
    "\n",
    "* A Domain dataset group, where you create preconfigured resources for different business domains and use cases, such as getting recommendations for similar videos (VIDEO_ON_DEMAND domain) or best selling items (ECOMMERCE domain). You choose your business domain, import your data, and create recommenders. You use recommenders in your application to get recommendations. Use a [Domain dataset group](https://docs.aws.amazon.com/personalize/latest/dg/domain-dataset-groups.html) if you have a video on demand or e-commerce application and want Amazon Personalize to find the best configurations for your use cases. If you start with a Domain dataset group, you can also add custom resources such as solutions with solution versions trained with recipes for custom use cases.\n",
    "\n",
    "\n",
    "* A [Custom dataset group](https://docs.aws.amazon.com/personalize/latest/dg/custom-dataset-groups.html), where you create configurable resources for custom use cases and batch recommendation workflows. You choose a recipe, train a solution version (model), and deploy the solution version with a campaign. You use a campaign in your application to get recommendations. Use a Custom dataset group if you don't have a video on demand or e-commerce application or want to configure and manage only custom resources, or want to get recommendations in a batch workflow. If you start with a Custom dataset group, you can't associate it with a domain later. Instead, create a new Domain dataset group.\n",
    "\n",
    "You can create and manage Domain dataset groups and Custom dataset groups with the AWS console, the AWS Command Line Interface (AWS CLI), or programmatically with the AWS SDKs.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> If you are running this as part of an AWS workshop, the resources have been created ahead of time, this is to eliminate the time spent waiting for the data to import, models to train, etc. In these notebooks we will check to see if the resources exist and use them. Therefore, you may see “Resource X Already exists” messages. If you run these notebooks in your own account, it will create these resources as needed, which will add approximatedly 90 minutes to this workshop.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset Group\n",
    "\n",
    "The following cell will create a new dataset group with the name from the workshop parameter file loaded earlier. Since we're working on a use-case for a travel company, we will create a custom dataset group (i.e., without specifying a domain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Try to create the dataset group, this block with exectute fully if the dataset group does not exist yet\n",
    "    create_dataset_group_response = personalize.create_dataset_group(\n",
    "        name = workshop_dataset_group_name\n",
    "    )\n",
    "    workshop_dataset_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "    print(json.dumps(create_dataset_group_response, indent=2))\n",
    "    print ('\\nCreating the Dataset Group with dataset_group_arn = {}'.format(workshop_dataset_group_arn))\n",
    "\n",
    "except personalize.exceptions.ResourceAlreadyExistsException as e:\n",
    "    # if the dataset group already exists, get the unique identifier workshop_dataset_group_arn\n",
    "    # from the existing resource\n",
    "\n",
    "    workshop_dataset_group_arn = 'arn:aws:personalize:'+region+':'+account_id+':dataset-group/'+workshop_dataset_group_name\n",
    "    print ('\\nThe the Dataset Group with dataset_group_arn = {} already exists'.format(workshop_dataset_group_arn))\n",
    "    print ('\\nWe will be using the existing Dataset Group dataset_group_arn = {}'.format(workshop_dataset_group_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Dataset Group to have ACTIVE status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use the dataset group, it must be active. This can take a minute or two. Execute the cell below and wait for it to show the ACTIVE status. It checks the status of the dataset group every 15 seconds, up to a maximum of 3 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_group_response = personalize.describe_dataset_group(\n",
    "        datasetGroupArn = workshop_dataset_group_arn\n",
    "    )\n",
    "    status = describe_dataset_group_response[\"datasetGroup\"][\"status\"]\n",
    "    print(\"DatasetGroup: {}\".format(status))\n",
    "\n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a dataset group, you can create a dataset for the interaction data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Interactions schema <a class=\"anchor\" id=\"interact_schema\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Now that we've loaded and prepared our three dataset files and uploaded them to an S3 bucket, we'll need to configure the Amazon Personalize service to understand our data so that it can be used to train models for generating recommendations. Amazon Personalize requires a schema for each dataset so it can map the columns in our CSVs to fields for model training and filtering. Each schema is declared in JSON using the [Apache Avro](https://avro.apache.org/) format. \n",
    "\n",
    "First, define a schema to tell Amazon Personalize what type of dataset you are uploading. There are several reserved and mandatory keywords required in the schema, based on the type of dataset. More detailed information can be found in the [documentation](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html).\n",
    "\n",
    "Here, you will create a schema for interactions data, which requires the `USER_ID`, `ITEM_ID`, and `TIMESTAMP` fields. We will also be specifying the reserved columns `EVENT_TYPE` and `EVENT_VALUE` as described earlier and finally a custom field for the `CABIN_TYPE` for the trip. These must be defined in the same order in the schema as they appear in the dataset.\n",
    "\n",
    "Custom fields can be categorical, numeric, or boolean. Categorical fields are those where one or more string-based values can be specified and are indicated in the schema with `\"categorical\": true` as shown below for `CABIN_TYPE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Interactions\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TIMESTAMP\",\n",
    "            \"type\": \"long\"\n",
    "        },\n",
    "        {\n",
    "            \"name\":\"CABIN_TYPE\",\n",
    "            \"type\": \"string\",\n",
    "            \"categorical\": True\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"EVENT_TYPE\",\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"EVENT_VALUE\",\n",
    "          \"type\": \"float\"\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Try to create the interactions dataset schema, this block with exectute fully\n",
    "    # if the interactions dataset schema does not exist yet\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = interactions_schema_name,\n",
    "        schema = json.dumps(interactions_schema)\n",
    "    )\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "    workshop_interactions_schema_arn = create_schema_response['schemaArn']\n",
    "    print ('\\nCreating the Interactions Schema with workshop_interactions_schema_arn = {}'.format(workshop_interactions_schema_arn))\n",
    "\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    # if the interactions dataset schema already exists, get the unique identifier workshop_interactions_schema_arn\n",
    "    # from the existing resource\n",
    "\n",
    "    workshop_interactions_schema_arn = 'arn:aws:personalize:'+region+':'+account_id+':schema/'+interactions_schema_name\n",
    "    print('The schema {} already exists.'.format(workshop_interactions_schema_arn))\n",
    "    print ('\\nWe will be using the existing Interactions Schema with workshop_interactions_schema_arn = {}'.format(workshop_interactions_schema_arn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the interactions dataset\n",
    "\n",
    "With a schema created, you can create a dataset within the dataset group. Note that this does not load the data yet, but creates a schema of what the data looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Try to create the interactions dataset, this block with exectute fully\n",
    "    # if the interactions dataset does not exist yet\n",
    "\n",
    "    dataset_type = 'INTERACTIONS'\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = interactions_dataset_name,\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = workshop_dataset_group_arn,\n",
    "        schemaArn = workshop_interactions_schema_arn\n",
    "    )\n",
    "\n",
    "    workshop_interactions_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "    print ('\\nCreating the Interactions Dataset with workshop_interactions_dataset_arn = {}'.format(workshop_interactions_dataset_arn))\n",
    "\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    # if the interactions dataset already exists, get the unique identifier workshop_interactions_dataset_arn\n",
    "    # from the existing resource\n",
    "    workshop_interactions_dataset_arn =  'arn:aws:personalize:'+region+':'+account_id+':dataset/'+workshop_dataset_group_name+'/INTERACTIONS'\n",
    "    print('The Interactions Dataset {} already exists.'.format(workshop_interactions_dataset_arn))\n",
    "    print ('\\nWe will be using the existing Interactions Dataset with workshop_interactions_dataset_arn = {}'.format(workshop_interactions_dataset_arn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Items schema<a class=\"anchor\" id=\"items_schema\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Next, let's repeat the process for the items dataset by creating the schema and dataset resources.\n",
    "\n",
    "The items dataset schema requires an `ITEM_ID` column and at least one metadata column, up to a maximum of 100 metadata columns.\n",
    "\n",
    "For this dataset we have several metadata columns that identify the attributes of a flight and trip including the destination city (`DST_CITY`), source city (`SRC_CITY`), airline name (`AIRLINE`), duration in days (`DURATION_DAYS`), price (`DYNAMIC_PRICE`), discount percentage (`DISCOUNT_FOR_MEMBER`), and whether the trip is expired (`EXPIRED`). For more information, please refer to [the documentation](https://docs.aws.amazon.com/personalize/latest/dg/items-datasets.html).\n",
    "\n",
    "Custom fields can be categorical, textual, numeric, or boolean. Categorical fields are those where one or more string-based values can be specified and are indicated in the schema with `\"categorical\": true` as shown below. Although we don't have a textual field in this dataset, it can be used to specify unstructured textual information such as long descriptions that Personalize will analyze to extract additional features for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Items\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"DST_CITY\",\n",
    "            \"type\": [\"null\", \"string\"],\n",
    "            \"categorical\": True\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"SRC_CITY\",\n",
    "            \"type\": [\"null\", \"string\"],\n",
    "            \"categorical\": True\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"AIRLINE\",\n",
    "            \"type\": [\"null\", \"string\"],\n",
    "            \"categorical\": True\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"DURATION_DAYS\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"SEASON\",\n",
    "            \"type\": [\"null\", \"string\"],\n",
    "            \"categorical\": True\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"NUMBER_OF_SEARCH_BY_USER\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"PROMOTION\",\n",
    "            \"type\": [\"null\", \"string\"],\n",
    "            \"categorical\": True\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"DYNAMIC_PRICE\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"DISCOUNT_FOR_MEMBER\",\n",
    "            \"type\": \"float\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"EXPIRED\",\n",
    "            \"type\": [\"null\", \"string\"],\n",
    "            \"categorical\": True\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Try to create the items dataset schema, this block with exectute fully\n",
    "    # if the items dataset schema does not exist yet\n",
    "\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = items_schema_name,\n",
    "        schema = json.dumps(items_schema)\n",
    "    )\n",
    "    workshop_items_schema_arn = create_schema_response['schemaArn']\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "\n",
    "    print ('\\nCreating the Items Schema with workshop_items_schema_arn = {}'.format(workshop_items_schema_arn))\n",
    "\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    # if the items dataset schema already exists, get the unique identifier workshop_items_schema_arn\n",
    "    # from the existing resource\n",
    "\n",
    "    workshop_items_schema_arn = 'arn:aws:personalize:'+region+':'+account_id+':schema/'+items_schema_name\n",
    "    print('The schema {} already exists.'.format(workshop_items_schema_arn))\n",
    "    print ('\\nWe will be using the existing Items Schema with workshop_items_schema_arn = {}'.format(workshop_items_schema_arn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Items dataset\n",
    "With a schema created, you can create a dataset within the dataset group. Note that this does not load the data yet, but creates a schema of what the data looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Try to create the items dataset, this block with exectute fully if the items dataset does not exist yet\n",
    "\n",
    "    dataset_type = \"ITEMS\"\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = items_dataset_name,\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = workshop_dataset_group_arn,\n",
    "        schemaArn = workshop_items_schema_arn\n",
    "    )\n",
    "\n",
    "    workshop_items_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "\n",
    "    print ('\\nCreating the Items Dataset with workshop_items_dataset_arn = {}'.format(workshop_items_dataset_arn))\n",
    "\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    # if the items dataset already exists, get the unique identifier workshop_items_dataset_arn\n",
    "    # from the existing resource\n",
    "\n",
    "    workshop_items_dataset_arn =  'arn:aws:personalize:'+region+':'+account_id+':dataset/'+workshop_dataset_group_name+'/ITEMS'\n",
    "    print('The Items Dataset {} already exists.'.format(workshop_items_dataset_arn))\n",
    "    print ('\\nWe will be using the existing Items Dataset with workshop_items_dataset_arn = {}'.format(workshop_items_dataset_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Users schema<a class=\"anchor\" id=\"users_schema\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Let's repeat the process one more time for the users dataset, creating a schema and dataset resource. The users dataset requires the `USER_ID` field/column and at least one metadata field, up to a maximum of 25 metadata fields. For this workshop, we have one metadata field called `MEMBER_CLASS` that represents the membership tier of the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Users\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"MEMBER_CLASS\",\n",
    "            \"type\": \"string\",\n",
    "            \"categorical\": True\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Try to create the users dataset schema, this block with exectute fully\n",
    "    # if the users dataset schema does not exist yet\n",
    "\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = users_schema_name,\n",
    "        schema = json.dumps(users_schema)\n",
    "    )\n",
    "\n",
    "    workshop_users_schema_arn = create_schema_response['schemaArn']\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "\n",
    "    print ('\\nCreating the Users Schema with workshop_users_schema_arn = {}'.format(workshop_users_schema_arn))\n",
    "\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    # if the users dataset schema already exists, get the unique identifier workshop_users_schema_arn\n",
    "    # from the existing resource\n",
    "\n",
    "    workshop_users_schema_arn = 'arn:aws:personalize:'+region+':'+account_id+':schema/'+users_schema_name\n",
    "    print('The schema {} already exists.'.format(workshop_users_schema_arn))\n",
    "    print ('\\nWe will be using the existing Users Schema with workshop_users_schema_arn = {}'.format(workshop_users_schema_arn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Users dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Try to create the users dataset, this block with exectute fully if the users dataset does not exist yet\n",
    "\n",
    "    dataset_type = \"USERS\"\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = users_dataset_name,\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = workshop_dataset_group_arn,\n",
    "        schemaArn = workshop_users_schema_arn\n",
    "    )\n",
    "\n",
    "    workshop_users_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "\n",
    "    print ('\\nCreating the Users Dataset with workshop_users_dataset_arn = {}'.format(workshop_users_dataset_arn))\n",
    "\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    # if the users dataset already exists, get the unique identifier workshop_users_dataset_arn\n",
    "    # from the existing resource\n",
    "\n",
    "    workshop_users_dataset_arn =  'arn:aws:personalize:'+region+':'+account_id+':dataset/'+workshop_dataset_group_name+'/USERS'\n",
    "    print('The Users Dataset {} already exists.'.format(workshop_users_dataset_arn))\n",
    "    print ('\\nWe will be using the existing Users Dataset with workshop_users_dataset_arn = {}'.format(workshop_users_dataset_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wait untill all the datasets have been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_time = time.time() + 6*60*60 # 6 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_response = personalize.describe_dataset(\n",
    "        datasetArn = workshop_interactions_dataset_arn\n",
    "    )\n",
    "    status_interaction_dataset =  describe_dataset_response[\"dataset\"]['status']\n",
    "    print(\"Interactions Dataset: {}\".format(status_interaction_dataset))\n",
    "\n",
    "    if status_interaction_dataset == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(workshop_interactions_dataset_arn))\n",
    "\n",
    "    elif status_interaction_dataset == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(workshop_interactions_dataset_arn))\n",
    "        break\n",
    "\n",
    "    if not status_interaction_dataset == \"ACTIVE\":\n",
    "        print(\"The interaction dataset creation is still in progress\")\n",
    "    else:\n",
    "        print(\"The interaction dataset  is ACTIVE\")\n",
    "\n",
    "\n",
    "    describe_dataset_response = personalize.describe_dataset(\n",
    "        datasetArn = workshop_items_dataset_arn\n",
    "    )\n",
    "    status_item_dataset =  describe_dataset_response[\"dataset\"]['status']\n",
    "    print(\"Items Dataset: {}\".format(status_item_dataset))\n",
    "\n",
    "    if status_item_dataset == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(workshop_items_dataset_arn))\n",
    "\n",
    "    elif status_item_dataset == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(workshop_items_dataset_arn))\n",
    "        break\n",
    "\n",
    "    if not status_item_dataset == \"ACTIVE\":\n",
    "        print(\"The item dataset creation is still in progress\")\n",
    "    else:\n",
    "        print(\"The item dataset  is ACTIVE\")\n",
    "\n",
    "    describe_dataset_response = personalize.describe_dataset(\n",
    "        datasetArn = workshop_users_dataset_arn\n",
    "    )\n",
    "    status_user_dataset =  describe_dataset_response[\"dataset\"]['status']\n",
    "    print(\"Users Dataset: {}\".format(status_user_dataset))\n",
    "\n",
    "    if status_user_dataset == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(workshop_users_dataset_arn))\n",
    "\n",
    "    elif status_user_dataset == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(workshop_users_dataset_arn))\n",
    "        break\n",
    "\n",
    "    if not status_user_dataset == \"ACTIVE\":\n",
    "        print(\"The user dataset creation is still in progress\")\n",
    "    else:\n",
    "        print(\"The user dataset  is ACTIVE\")\n",
    "\n",
    "    if status_interaction_dataset == \"ACTIVE\" and status_item_dataset == \"ACTIVE\" and status_user_dataset == 'ACTIVE':\n",
    "        break\n",
    "\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the import job already exists\n",
    "\n",
    "# List the import jobs\n",
    "interactions_dataset_import_jobs = personalize.list_dataset_import_jobs(\n",
    "    datasetArn=workshop_interactions_dataset_arn,\n",
    "    maxResults=100\n",
    ")['datasetImportJobs']\n",
    "\n",
    "#check if there is an existing job with the prefix\n",
    "job_exists = False\n",
    "job_arn = None\n",
    "\n",
    "for job in interactions_dataset_import_jobs:\n",
    "    if (interactions_import_job_name in job['jobName']):\n",
    "        job_exists = True\n",
    "        job_arn = job['datasetImportJobArn']\n",
    "\n",
    "if (job_exists):\n",
    "    workshop_interactions_dataset_import_job_arn = job_arn\n",
    "    print('The Interactions Import Job {} already exists.'.format(workshop_interactions_dataset_import_job_arn))\n",
    "    print ('\\nWe will be using the existing Interactions Import Job with workshop_interactions_dataset_import_job_arn = {}'.format(workshop_interactions_dataset_import_job_arn))\n",
    "\n",
    "else:\n",
    "    # If there is no import job with the prefix, create it:\n",
    "    create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "        jobName = interactions_import_job_name,\n",
    "        datasetArn = workshop_interactions_dataset_arn,\n",
    "        dataSource = {\n",
    "            \"dataLocation\": \"s3://{}/{}\".format(bucket_name, interactions_file_name)\n",
    "        },\n",
    "        roleArn = role_arn\n",
    "    )\n",
    "    workshop_interactions_dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "    print(json.dumps(create_dataset_import_job_response, indent=2))\n",
    "\n",
    "    print ('\\nImporting the Interactions Data with workshop_interactions_dataset_import_job_arn = {}'.format(workshop_interactions_dataset_import_job_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Item metadata <a class=\"anchor\" id=\"import_items\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Earlier you created the dataset group and dataset to house your information, now you will execute an import job that will load the item data from the S3 bucket into the Amazon Personalize dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the import job already exists\n",
    "\n",
    "# List the import jobs\n",
    "items_dataset_import_jobs = personalize.list_dataset_import_jobs(\n",
    "    datasetArn=workshop_items_dataset_arn,\n",
    "    maxResults=100\n",
    ")['datasetImportJobs']\n",
    "\n",
    "job_exists = False\n",
    "job_arn = None\n",
    "\n",
    "#check if there is an existing job with the prefix\n",
    "for job in items_dataset_import_jobs:\n",
    "    if (items_import_job_name in job['jobName']):\n",
    "        job_exists = True\n",
    "        job_arn = job['datasetImportJobArn']\n",
    "\n",
    "if (job_exists):\n",
    "    workshop_items_dataset_import_job_arn =  job_arn\n",
    "    print('The Items Import Job {} already exists.'.format(workshop_items_dataset_import_job_arn))\n",
    "    print ('\\nWe will be using the existing Items Import Job with workshop_items_dataset_import_job_arn = {}'.format(workshop_items_dataset_import_job_arn))\n",
    "\n",
    "else:\n",
    "    # If there is no import job with the prefix, create it:\n",
    "    create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "        jobName = items_import_job_name,\n",
    "        datasetArn = workshop_items_dataset_arn,\n",
    "        dataSource = {\n",
    "            \"dataLocation\": \"s3://{}/{}\".format(bucket_name, items_file_name)\n",
    "        },\n",
    "        roleArn = role_arn\n",
    "    )\n",
    "\n",
    "    workshop_items_dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "    print(json.dumps(create_dataset_import_job_response, indent=2))\n",
    "    print ('\\nImporting the Items Data with workshop_items_dataset_import_job_arn = {}'.format(workshop_items_dataset_import_job_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the User metadata <a class=\"anchor\" id=\"import_users\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Earlier you created the dataset group and dataset to house your information, now you will execute an import job that will load the user data from the S3 bucket into the Amazon Personalize dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the import job already exists\n",
    "\n",
    "# List the import jobs\n",
    "users_dataset_import_jobs = personalize.list_dataset_import_jobs(\n",
    "    datasetArn=workshop_users_dataset_arn,\n",
    "    maxResults=100\n",
    ")['datasetImportJobs']\n",
    "\n",
    "#check if there is an existing job with the prefix\n",
    "job_exists = False\n",
    "job_arn = None\n",
    "for job in users_dataset_import_jobs:\n",
    "    if (users_import_job_name in job['jobName']):\n",
    "        job_exists = True\n",
    "        job_arn = job['datasetImportJobArn']\n",
    "\n",
    "if (job_exists):\n",
    "    workshop_users_dataset_import_job_arn =  job_arn\n",
    "    print('The Users Import Job {} already exists.'.format(workshop_users_dataset_import_job_arn))\n",
    "    print ('\\nWe will be using the existing Users Import Job with workshop_users_dataset_import_job_arn = {}'.format(workshop_users_dataset_import_job_arn))\n",
    "\n",
    "else:\n",
    "    # If there is no import job with the prefix, create it:\n",
    "    create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "        jobName = users_import_job_name,\n",
    "        datasetArn = workshop_users_dataset_arn,\n",
    "        dataSource = {\n",
    "            \"dataLocation\": \"s3://{}/{}\".format(bucket_name, users_file_name)\n",
    "        },\n",
    "        roleArn = role_arn\n",
    "    )\n",
    "\n",
    "    workshop_users_dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "    print(json.dumps(create_dataset_import_job_response, indent=2))\n",
    "\n",
    "    print ('\\nImporting the Users Data with workshop_users_dataset_import_job_arn = {}'.format(workshop_users_dataset_import_job_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for import jobs to complete\n",
    "\n",
    "Before we can use the dataset, the import job must be active. Execute the cell below and wait for it to show the ACTIVE status. It checks the status of the import job every minute, up to a maximum of 6 hours.\n",
    "\n",
    "It will take 10-15 minutes for the import jobs to complete. While you're waiting you can learn more about Datasets and Schemas in [the documentation](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html).\n",
    "\n",
    "We will wait for all three jobs to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = time.time() + 6*60*60 # 10 hours\n",
    "while time.time() < max_time:\n",
    "\n",
    "    # Interactions dataset import\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = workshop_interactions_dataset_import_job_arn\n",
    "    )\n",
    "    status_interactions_import = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "\n",
    "    if status_interactions_import == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(workshop_interactions_dataset_import_job_arn))\n",
    "\n",
    "    elif status_interactions_import == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(workshop_interactions_dataset_import_job_arn))\n",
    "        break\n",
    "\n",
    "    if not status_interactions_import == \"ACTIVE\":\n",
    "        print(\"The interactions dataset import is still in progress\")\n",
    "    else:\n",
    "        print(\"The interactions dataset import is ACTIVE\")\n",
    "\n",
    "    # Items dataset import\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = workshop_items_dataset_import_job_arn\n",
    "    )\n",
    "    status_items_import = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "\n",
    "    if status_items_import == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(workshop_items_dataset_import_job_arn))\n",
    "\n",
    "    elif status_items_import == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(workshop_items_dataset_import_job_arn))\n",
    "        break\n",
    "\n",
    "    if not status_items_import == \"ACTIVE\":\n",
    "        print(\"The items dataset import is still in progress\")\n",
    "    else:\n",
    "        print(\"The items dataset import is ACTIVE\")\n",
    "\n",
    "    # Users dataset import\n",
    "    describe_users_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = workshop_users_dataset_import_job_arn\n",
    "    )\n",
    "    status_users_import = describe_users_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "\n",
    "    if status_users_import == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(workshop_users_dataset_import_job_arn))\n",
    "\n",
    "    elif status_users_import == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(workshop_users_dataset_import_job_arn))\n",
    "        break\n",
    "\n",
    "    if not status_users_import == \"ACTIVE\":\n",
    "        print(\"The user dataset import is still in progress\")\n",
    "    else:\n",
    "        print(\"The user dataset import is ACTIVE\")\n",
    "\n",
    "\n",
    "    if status_interactions_import == \"ACTIVE\" and status_items_import == 'ACTIVE' and status_users_import  == 'ACTIVE':\n",
    "        break\n",
    "\n",
    "    print()\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train User Segmentation ML model\n",
    "\n",
    "## Create Solution <a class=\"anchor\" id=\"solutions\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "With our dataset resources created and data imported into Amazon Personalize, we can now train a ML model for our user segmentation use case. Training a ML model in Personalize involves creating a solution or use-case optimized recommender. A solution in Personalize is a resource that connects the data in a dataset group with a Personalize recipe and configuration details. [Recipes](https://docs.aws.amazon.com/personalize/latest/dg/working-with-predefined-recipes.html) in Personalize are algorithms that target personalization use cases. Personalize use cases fall into four categories.\n",
    "\n",
    "- [User Personalization](https://docs.aws.amazon.com/personalize/latest/dg/user-personalization-recipes.html) recipes target the canonical use case of recommending items to a specific user. For example, on the homepage of website or mobile application.\n",
    "- [Popular Items](https://docs.aws.amazon.com/personalize/latest/dg/popularity-recipes.html) recipes provide recommendations based on popularity across all users or items that are trending in popularity.\n",
    "- [Personalized Ranking](https://docs.aws.amazon.com/personalize/latest/dg/personalized-ranking-recipes.html) is a recipe that will rank a supplied set of items based on a user's learned preferences and intent. For example, ranking search results from a search engine for each user.\n",
    "- [Related Items](https://docs.aws.amazon.com/personalize/latest/dg/related-items-recipes.html) recipes recommend items that are similar to a specific item.\n",
    "- [User Segmentation](https://docs.aws.amazon.com/personalize/latest/dg/user-segmentation-recipes.html) recipes are designed to identify users that have an affinity for a specific item or item attributes.\n",
    "\n",
    "Let's use the Personalize SDK to list all of the custom recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginator = personalize.get_paginator('list_recipes')\n",
    "for page in paginator.paginate():\n",
    "    for recipe in page[\"recipes\"]:\n",
    "        if not \"domain\" in recipe:\n",
    "            print(recipe[\"recipeArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this workshop, we want to identify users that have an affinity for a specific trip that we want to promote with a marketing campaign. Therefore, we will create a solution with [Item-Affinity](https://docs.aws.amazon.com/personalize/latest/dg/item-affinity-recipe.html) user segmentation recipe. Let's confirm the recipe ARN we initialized earlier in this notebook by printing it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(workshop_item_affinity_recipe_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Item Affinity solution\n",
    "\n",
    "First you create a solution specifying a name, the dataset group, and recipe. This establishes the configuration for the model. We will initiate model training in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = personalize.create_solution(\n",
    "        name = workshop_item_affinity_solution_name,\n",
    "        datasetGroupArn = workshop_dataset_group_arn,\n",
    "        recipeArn = workshop_item_affinity_recipe_arn\n",
    "    )\n",
    "\n",
    "    workshop_item_affinity_solution_arn = response['solutionArn']\n",
    "    print(json.dumps(response, indent=2))\n",
    "\n",
    "    print ('\\nCreating the Personalize Item Affinity Solution with workshop_item_affinity_solution_arn = {}'.format(workshop_item_affinity_solution_arn))\n",
    "\n",
    "except personalize.exceptions.ResourceAlreadyExistsException as e:\n",
    "    workshop_item_affinity_solution_arn =  'arn:aws:personalize:'+region+':'+account_id+':solution/'+workshop_item_affinity_solution_name\n",
    "    print('The Personalize Item Affinity Solution {} already exists.'.format(workshop_item_affinity_solution_arn))\n",
    "    print ('\\nWe will be using the existing Personalize Item Affinity Solution with workshop_item_affinity_solution_arn = {}'.format(workshop_item_affinity_solution_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the solution to be fully created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_time = time.time() + 6*60*60 # 6 hours\n",
    "while time.time() < max_time:\n",
    "    describe_solution_response = personalize.describe_solution(\n",
    "        solutionArn = workshop_item_affinity_solution_arn\n",
    "    )\n",
    "    status_solution =  describe_solution_response[\"solution\"]['status']\n",
    "    print(\"Solution: {}\".format(status_solution))\n",
    "    \n",
    "    if status_solution == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(workshop_interactions_dataset_arn))\n",
    "        break\n",
    "        \n",
    "    elif status_solution == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(workshop_interactions_dataset_arn))\n",
    "        break\n",
    "        \n",
    "    if not status_solution == \"ACTIVE\":\n",
    "        print(\"The Solution creation is still in progress\")\n",
    "    else:\n",
    "        print(\"The Solution dataset is ACTIVE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create solution version <a class=\"anchor\" id=\"solution_version\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Once you have a solution, you need to create a version in order to complete the model training. The training can take a while to complete (depending on the size of the datasets). For this workshop, if a pre-trained solution version is being provided, you won't have to wait for the training process to complete. If a pre-trained solution is not provided, it can take 30-50 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workshop_item_affinity_solution_version_arn = None\n",
    "\n",
    "solution_versions_list = personalize.list_solution_versions(\n",
    "    solutionArn=workshop_item_affinity_solution_arn,\n",
    "    maxResults=10\n",
    ")['solutionVersions']\n",
    "\n",
    "for solution_vers in solution_versions_list:\n",
    "    if solution_vers['status'] in ['CREATE PENDING', 'CREATE IN_PROGRESS', 'ACTIVE']:\n",
    "        workshop_item_affinity_solution_version_arn = solution_vers['solutionVersionArn']\n",
    "    if workshop_item_affinity_solution_version_arn:\n",
    "        break\n",
    "\n",
    "if workshop_item_affinity_solution_version_arn:\n",
    "    print ('\\nWe will be using the existing Personalize Item Affinity Solution Version with workshop_item_affinity_solution_version_arn = {}'.format(workshop_item_affinity_solution_version_arn))\n",
    "else:\n",
    "    response = personalize.create_solution_version(\n",
    "        solutionArn = workshop_item_affinity_solution_arn\n",
    "    )\n",
    "    workshop_item_affinity_solution_version_arn = response['solutionVersionArn']\n",
    "    print(json.dumps(response, indent=2))\n",
    "    \n",
    "    print ('\\nTraining the Personalize Item Affinity Solution Version with workshop_item_affinity_solution_version_arn = {}'.format(workshop_item_affinity_solution_version_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View solution version creation status\n",
    "\n",
    "To view the status updates in the console:\n",
    "\n",
    "* In another browser tab you should already have the AWS Console up from opening this notebook instance. \n",
    "* Switch to that tab and search at the top for the service `Personalize`, then go to that service page. \n",
    "* Click `Dataset groups`.\n",
    "* Click the name of your dataset group, if you did not change it, it is \"personalize-poc-travel\".\n",
    "* Click `Custom resources` and then `Solutions and recipes`.\n",
    "* You will see the solution we just created above. Click on the solution name to see its details.\n",
    "* On the solution details page you will see the solution versions. When the solution version's status becomes `Active`, the model is fully trained.\n",
    "\n",
    "Or simply run the cell below to keep track of the solution version creation status using the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_time = time.time() + 10*60*60 # 10 hours\n",
    "while time.time() < max_time:\n",
    "    # Item Affinity Solution \n",
    "    version_response = personalize.describe_solution_version(\n",
    "        solutionVersionArn = workshop_item_affinity_solution_version_arn\n",
    "    )\n",
    "    status = version_response[\"solutionVersion\"][\"status\"]\n",
    "\n",
    "    if status == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(workshop_item_affinity_solution_version_arn))\n",
    "        \n",
    "    elif status == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(workshop_item_affinity_solution_version_arn))\n",
    "        break\n",
    "\n",
    "    if not status == \"ACTIVE\":\n",
    "        print(\"Item Affinity Solution Version build is still in progress\")\n",
    "    else:\n",
    "        print(\"The Item Affinity solution is ACTIVE\")\n",
    "        break\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate solution version metrics <a class=\"anchor\" id=\"eval\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "To get performance metrics from the trained model, Amazon Personalize splits the input interactions data into a training set and a testing set. The split depends on the type of recipe you choose:\n",
    "\n",
    "- For USER_SEGMENTATION recipes, the training set consists of 80% of each user's interactions data and the testing set consists of 20% of each user's interactions data.\n",
    "- For all other recipe types, the training set consists of 90% of your users and their interactions data. The testing set consists of the remaining 10% of users and their interactions data.\n",
    "\n",
    "We recommend reading the [documentation](https://docs.aws.amazon.com/personalize/latest/dg/working-with-training-metrics.html) to understand the metrics, but we have also copied parts of the documentation below for convenience.\n",
    "\n",
    "You need to understand the following terms regarding evaluation in Personalize:\n",
    "\n",
    "* *Relevant recommendation* refers to a recommendation that matches a value in the testing data for the particular user.\n",
    "* *Rank* refers to the position of a recommended item in the list of recommendations. Position 1 (the top of the list) is presumed to be the most relevant to the user.\n",
    "* *Query* refers to the internal equivalent of a GetRecommendations call.\n",
    "\n",
    "The metrics produced by Personalize for user segmentation recipes are:\n",
    "* **coverage**: The proportion of unique recommended items from all queries out of the total number of unique items in the training data (includes both the Items and Interactions datasets).\n",
    "* **hit (hit at K)**: If you trained the solution version with a USER_SEGMENTATION recipe, the average number of users in the predicted top relevant K results that match the actual users. Actual users are the users who actually interacted with the items in the test set. K is the top 1% of the most relevant users. The higher the value the more accurate the predictions.\n",
    "* **recall (recall at K)**: If you trained the solution version with a USER_SEGMENTATION recipe, the average percentage of predicted users in the predicted top relevant K results that match the actual users. Actual users are the users who actually interacted with the items in the test set. K is the top 1% of the most relevant users. The higher the value, the more accurate the predictions.\n",
    "\n",
    "Models trained with recipes from other categories will have a different set of metrics. See the [documentation](https://docs.aws.amazon.com/personalize/latest/dg/working-with-training-metrics.html) for details.\n",
    "\n",
    "Let's take a look at the evaluation metrics for the solution produced in this notebook. Please note that your results might differ from the results described in the text of this notebook, due to the quality of the synthetic dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = personalize.get_solution_metrics(\n",
    "    solutionVersionArn = workshop_item_affinity_solution_version_arn\n",
    ")\n",
    "\n",
    "for metric in response[\"metrics\"]:\n",
    "    print (\"{}: {}\".format(metric, response[\"metrics\"][metric] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using evaluation metrics <a class=\"anchor\" id=\"usemetrics\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "It is important to use evaluation metrics carefully. There are a number of factors to keep in mind.\n",
    "\n",
    "* If there is an existing recommendation system in place, this will have influenced the user's interaction history which you use to train your new solutions. This means the evaluation metrics are biased to favor the existing system. If you work to push the evaluation metrics to match or exceed the existing system, you may just be pushing the Personalize models to behave like the existing solution and might not end up with something better.\n",
    "\n",
    "Keeping in mind these factors, the evaluation metrics produced by Personalize are generally useful for two cases:\n",
    "1. Comparing the performance of solution versions trained on the same recipe, but with different values for the hyperparameters and features\n",
    "1. Comparing the performance of solution versions trained on different recipes. Here also keep in mind that the recipes answer different use cases and comparing them to each other might not make sense in your solution.\n",
    "\n",
    "Properly evaluating a recommendation system is always best done through A/B testing while measuring actual business outcomes. Since recommendations generated by a system usually influence the user behavior which it is based on, it is better to run small experiments and apply A/B testing for longer periods of time. Over time, the bias from the existing model will fade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create batch segment job <a class=\"anchor\" id=\"batchsegmentjob\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "With our item affinity user segmentation model fully trained, we can now to turn to creating a batch segment job to generate our user segment. If you recall from the outset of this workshop, we have been tasked with building a marketing campaign to promote a flight and trip using generative AI. Well, generating the campaign's creative assets is just part of the work. We also need to identify who to target with the campaign. While we could just blast all of our users with the same promotion, we know from past campaigns using this approach that users have unsubscribed from our emails when they are not relevant. Therefore, we will be using the item affinity model to build our target list of users that have any affinity for the trip we are promoting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input file for batch segment job\n",
    "\n",
    "Before we can create a batch segment job, we need to prepare an input file containing the items for which we want to generate segments.\n",
    "\n",
    "First, let's consider the format of the job input file. Below is a sample of the input file for an item affinity job that builds 3 user segments for users that have an affinity for 3 different items:\n",
    "\n",
    "```javascript\n",
    "{\"itemId\": \"1\"}\n",
    "{\"itemId\": \"2\"}\n",
    "{\"itemId\": \"3\"}\n",
    "```\n",
    "\n",
    "Notice that the input file is in [JSON Lines](https://jsonlines.org/) format where each line is a JSON document specifying the item ID for each to generate a segment. As you will see, the output file format will echo each input line with the user IDs inserted into each JSON document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the items that we want to promote:\n",
    "# 1. Promoted items\n",
    "# 2. Season/month is October\n",
    "# 3. Destination city is Hong Kong\n",
    "promoted_items = item_data[(item_data['PROMOTION'] == 'Yes') & (item_data['SEASON'] == 'October') & (item_data['DST_CITY'] == 'Hong Kong')]\n",
    "\n",
    "promoted_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly select a promoted item to use for our promotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promoted_item = promoted_items.sample(n=1)\n",
    "promoted_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the batch segment job input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + \"/batch_segment_input.json\", \"w\") as output_file:\n",
    "    json.dump({\"itemId\": str(promoted_item.iloc[0][\"ITEM_ID\"])}, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's dump the contents of the file to check its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $data_dir/batch_segment_input.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload batch segment input file to S3\n",
    "\n",
    "Before we can create a batch segment job to process our input file, we need to stage the input file in S3. We'll use the S3 bucket we created earlier in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_segment_input_s3_key = \"batch_segment/input/batch_segment_input.json\"\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket_name).Object(batch_segment_input_s3_key).upload_file(data_dir+\"/batch_segment_input.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create batch segment job\n",
    "\n",
    "Now we have what we need to submit the batch segment job. We'll start by creating a job name that includes the current timestamp (in case you want to run multiple jobs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current timestamp in the desired format\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Combine the prefix and current timestamp to create the job name\n",
    "job_name = f\"travel-item-affinity-job-{current_time}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the job, specifying the job name, the Personalize solution version ARN for our model, a max number of 10 users for our segment, the input and output locations on S3, and the IAM role for Personalize to assume to read and write to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = personalize.create_batch_segment_job(\n",
    "    jobName = job_name,\n",
    "    solutionVersionArn = workshop_item_affinity_solution_version_arn,\n",
    "    numResults = 10,\n",
    "    jobInput =  {\n",
    "        \"s3DataSource\": {\n",
    "            \"path\": f\"s3://{bucket_name}/{batch_segment_input_s3_key}\"\n",
    "        }\n",
    "    },\n",
    "    jobOutput = {\n",
    "        \"s3DataDestination\": {\n",
    "            \"path\": f\"s3://{bucket_name}/batch_segment/output/\"\n",
    "        }\n",
    "    },\n",
    "    roleArn = role_arn \n",
    "    )\n",
    "\n",
    "batch_segment_job_arn = response['batchSegmentJobArn']\n",
    "print(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for batch segment job to complete\n",
    "\n",
    "The user segmentation job can take 5-10 minutes to complete. Even though our input file only specifies a single item, there is a certain amount of fixed overhead required for Personalize to spin up the resources needed to execute the job. This overhead is amortized for larger input files that generate many user segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    response = personalize.describe_batch_segment_job(\n",
    "        batchSegmentJobArn = batch_segment_job_arn\n",
    "    )\n",
    "    status = response[\"batchSegmentJob\"]['status']\n",
    "    print(\"BatchSegmentJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and inspect batch segment output file\n",
    "\n",
    "The batch segment job wrote its output to the `s3Destination` specified when we created the job. The filename is the same as the input file with `.out` appended. Let's download the output file to the local volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_object_key = \"batch_segment/output/batch_segment_input.json.out\"\n",
    "s3.download_file(bucket_name, output_object_key, data_dir + \"/batch_segment_output.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now dump the output of the batch segment job. The `output.usersList` array specifies the IDs of the users that have an affinity for the flight/trip that we want to promote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + \"/batch_segment_output.json\", 'r') as file:\n",
    "    # Load the JSON data\n",
    "    batch_segment_output = json.load(file)\n",
    "    \n",
    "print(json.dumps(batch_segment_output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we established a dataset group in Amazon Personalize with three datasets: interactions representing users rating flights/trips, item metadata about the items, and user metadata about the users. Once our data was imported into datasets in the dataset group, we trained a model in Personalize using the item affinity recipe for user segmentation. This recipe is designed for identifying users with an affinity for a specific item based on their past behavior. Once the model was trained, we created a batch segment job to identify the users to target with our promotional email. This answer the question of \"who\" to target with our campaign. In the next notebook we'll use generative AI to answer the question of \"what\" we'll send to these users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Useful Variables <a class=\"anchor\" id=\"vars\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Before exiting this notebook, run the following cells to save variables for use in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store data_dir\n",
    "%store bucket_name\n",
    "\n",
    "%store workshop_dataset_group_arn\n",
    "%store promoted_item\n",
    "%store batch_segment_output\n",
    "\n",
    "%store region\n",
    "%store account_id\n",
    "%store role_name\n",
    "%store role_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to the next notebook `02_Generative_AI_Marketing_Campaign.ipynb`](02_Generative_AI_Marketing_Campaign.ipynb) to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
