{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f64145f",
   "metadata": {},
   "source": [
    "# Lab 2: Create Marketing Campaign Content with Generative AI <a class=\"anchor\" id=\"top\"></a>\n",
    "\n",
    "In the previous notebook we prepared 3 different datasets that represent our fictional online travel company (user interactions representing flight/trip reviews, item metadata for the flights/trips, and user metadata of our subscribers), created dataset resources in Amazon Personalize, trained an ML model in Amazon Personalize with the item affinity user segmention recipe, and created a batch segment job to generate a user segment of users with an affinity for our target flight/trip.\n",
    "\n",
    "In this notebook we show how to generate the content for a promotional personalized marketing campaign. [LangChain](https://www.langchain.com/) is a framework for developing applications powered by large language models. The key aspects of this framework allow us to augment the Large Language Models by chaining together various components to create advanced use cases. We will use the [Amazon Bedrock](https://aws.amazon.com/bedrock/) [component](https://python.langchain.com/docs/integrations/llms/bedrock) provided in LangChain. The prompt used in this example creates a custom LangChain prompt template for adding context to the text generation request. The following diagram highlights the focus of this lab.\n",
    "\n",
    "![Lab 2 Diagram](images/architecture-lab2.png)\n",
    "\n",
    "Then in the next notebook we will perform content moderation against the generated email text and image to ensure that it meets our Travel company's policies.\n",
    "\n",
    "## In this notebook\n",
    "\n",
    "1. [Introduction](#intro)\n",
    "1. [Generative AI use case 1: email text generation](#usecase1)\n",
    "1. [Generative AI use case 2: email banner image generation](#usecase2)\n",
    "1. [Putting it all together](#alltogether)\n",
    "1. [Storing Useful Variables](#vars)\n",
    "\n",
    "To run this notebook, you need to have run [the previous notebook: `01_Personalize_User_Segmentation.ipynb`](01_Personalize_User_Segmentation.ipynb), where you trained a ML model with the item affinity user segmentation recipe using Amazon Personalize, selected a flight to promote in our marketing campaign, and used the user segmentation model to generate a segment of users to target with the marketing campaign. At the end of that notebook, you saved some of the variable values, which you now need to load into this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa11828a-243d-4808-9c92-e8caf4cebd37",
   "metadata": {},
   "source": [
    "### Pre-requisites\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Important:</b> this notebook and the following notebook will be using three <a href=\"https://aws.amazon.com/bedrock/\">Amazon Bedrock</a> foundation models (FMs). Before these foundation models can be invoked, you must have requested and received access to the three FMs in this AWS account. If you haven't already done so, please request access to the following FMs in the Amazon Bedrock console.\n",
    "<ul>\n",
    "<li>Anthropic Claude Instant</li>\n",
    "<li>Stability AI SDXL 1.0</li>\n",
    "<li>Amazon Titan Embeddings G1 - Text</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "Before we get started with the implementation we have to make sure that the required latest boto3 and botocore packages are installed. These will be used to leverage the Amazon Bedrock API client.\n",
    "\n",
    "Additionally we would need langchain one of  the latest versions, which has Amazon Bedrock class implemented under llms module. Also we are installing the transformers framework from HuggingFace, which we will use to quickly count the number of tokens in the input prompt.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> You may receive some warnings/errors about depedencies when you execute the following cell. You can ignore those errors since they are not needed for this lab.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b7248-f29d-4abe-9aea-57471e676bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install --upgrade --no-deps --force-reinstall boto3\n",
    "!{sys.executable} -m pip install --upgrade --no-deps --force-reinstall botocore\n",
    "!{sys.executable} -m pip install lxml\n",
    "!{sys.executable} -m pip install anthropic\n",
    "!{sys.executable} -m pip install --quiet \"langchain==0.0.339\"\n",
    "!{sys.executable} -m pip install --quiet \"pillow>=9.5,<10\"\n",
    "!{sys.executable} -m pip show langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607949d1-c951-4107-ba0c-5a0075121271",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcee7d7-774f-4ade-b00e-e66052a9a8d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import io\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import boto3\n",
    "import botocore\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from langchain import PromptTemplate\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7a3efd",
   "metadata": {},
   "source": [
    "Display some details on some of the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5793924-55ad-4b6a-86f8-549afa67dcc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the Boto3 version\n",
    "boto3_version = boto3.__version__\n",
    "\n",
    "# Get the Botocore version\n",
    "botocore_version = botocore.__version__\n",
    "\n",
    "# Print the Boto3 version\n",
    "print(\"Current Boto3 Version:\", boto3_version)\n",
    "\n",
    "# Print the Botocore version\n",
    "print(\"Current Botocore Version:\", botocore_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f245f9",
   "metadata": {},
   "source": [
    "Load variables from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d464084",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fbc31c",
   "metadata": {},
   "source": [
    "Lab 1 was skipped; initialize missing variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f71cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"promoted_item\" not in globals():\n",
    "    # Lab 1 was skipped; initialize missing variables.\n",
    "    data_dir = \"poc_data\"\n",
    "    items_file_path = \"s3://personalize-solution-staging-us-east-1/personalize-immersionday-travel/travel_items.csv\"\n",
    "    items_file_name = 'items.csv'\n",
    "    item_data = pd.read_csv(items_file_path)\n",
    "\n",
    "    # filter promoted items criteria\n",
    "    promoted_items = item_data[(item_data['PROMOTION'] == 'Yes') & (item_data['MONTH'] == 'October') & (item_data['DST_CITY'] == 'Hong Kong')]\n",
    "\n",
    "    # random pick an item as promoted_item\n",
    "    promoted_item = promoted_items.sample(1)\n",
    "    promoted_item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7daa1a8-d21a-410c-adbf-b253c2dabf80",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare Amazon Bedrock client\n",
    "\n",
    "Let's prepare an SDK client to access Amazon Bedrock and provide the LangChain in subsequent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7f5c74-30b1-4ae8-b044-282f5232653f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock = boto3.client(service_name='bedrock')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8c4803-7879-43a1-9c80-625f6dc5d01e",
   "metadata": {},
   "source": [
    "### List all supported foundation models (FMs) in Bedrock\n",
    "\n",
    "Amazon Bedrock provides API access to several foundation models that support multiple modalities for generative AI use cases. Let's list the FMs using the Bedrock API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642ab211-dc91-444e-9968-3821d1e32326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = bedrock.list_foundation_models()\n",
    "print(json.dumps(response[\"modelSummaries\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd0ec42-3b11-4db4-aee2-2ce342875204",
   "metadata": {},
   "source": [
    "### Selecting FMs for our use case\n",
    "\n",
    "The modalities for our use case are text-to-text and text-to-image. That is, we're going to provide a text-based prompt as input and receive text as output from a model for the promotional text and provide text or an image as input and receive an image as output from another model for banner image for our email. Take a look at the foundation models listed in the output above and inspect the `inputModalities` and `outputModalities` values to get a sense for the modalities supported by various models.\n",
    "\n",
    "For our text-to-text use case we're going to use the \"Claude Instant\" model from Anthropic to generate the text for our promotional email. Its modelId is `anthropic.claude-instant-v1`. This model will provide the quality of output that we're looking for and is ligther, faster, and less expensive than other Anthropic models. Depending on your use case, you may have multiple FMs to choose from. Selecting the right model will involve evaluating the output from different models and considering their price and responsiveness. For more details, please see the [documentation](https://aws.amazon.com/bedrock/claude/).\n",
    "\n",
    "Since we also want to include a banner image in our promotional email, we will use the \"Stable Diffusion XL\" model from Stability AI. It's modelId is `stability.stable-diffusion-xl-v0`. This model supports text or an image as input and generates an image as output. For more details, please see the [documentation](https://aws.amazon.com/bedrock/stable-diffusion/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fbb8c9",
   "metadata": {},
   "source": [
    "# Generative AI use case 1: email text content generation <a class=\"anchor\" id=\"usecase1\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "We'll start by using LangChain and Amazon Bedrock to create the textual content for our email marketing campaign. That is, the email subject and body.\n",
    "\n",
    "## Preparing LangChain\n",
    "\n",
    "For our text-to-text use case, we will be using the LangChain framework to help us compose our prompt and invoke our selected Bedrock FM. You can also use the AWS Bedrock SDK to invoke models directly.\n",
    "\n",
    "Each model accepts arguments that are specific to the model. For the Claude Instant model, we are specifying the following arguments.\n",
    "\n",
    "- `max_tokens_to_sample`: The maximum number of tokens to generate before stopping. Note that the model may stop before reaching this maximum. This parameter only specifies the absolute maximum number of tokens to generate.\n",
    "- `temperature`: Amount of randomness injected into the response. Defaults to 1. Ranges from 0 to 1. Use temp closer to 0 for analytical / multiple choice, and closer to 1 for creative and generative tasks.\n",
    "- `top_k`: Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. More details [here](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277).\n",
    "\n",
    "You can learn more about these and other model arguments in Anthropic's [documentation](https://docs.anthropic.com/claude/reference/complete_post)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffa1250-56cd-4b6d-b3d8-c62baac143ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")\n",
    "\n",
    "inference_modifier = {'max_tokens_to_sample':4096,\n",
    "                      \"temperature\":0.7,\n",
    "                      \"top_k\":250\n",
    "                     }\n",
    "\n",
    "textgen_llm = Bedrock(model_id = \"anthropic.claude-instant-v1\", model_kwargs = inference_modifier, client = bedrock_runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f310c88-bb7b-4dab-8980-ce29f5ea34fd",
   "metadata": {},
   "source": [
    "## Prepare contextual item medata and question template \n",
    "\n",
    "In the prior notebook we selected a flight to promote as part of our marketing campaign. We will use attributes of this flight in our prompt to the FM so that it can be used as context to direct the model to generate relevant text.\n",
    "\n",
    "As a reminder of the item we selected, let's display it again below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3f6cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "promoted_item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95f8315",
   "metadata": {},
   "source": [
    "Next let's load a template for our prompt and inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abb3572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"ticketing_template.txt\", \"r\") as file:\n",
    "    prompt_template = file.read()\n",
    "\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b793536",
   "metadata": {},
   "source": [
    "Some things to notice in our prompt template:\n",
    "\n",
    "- The value for our question starts with `Human: ` and ends with `Assitant:`. These are specific tokens used by the Claude model to know where our instruction ends and where the model should generate its output. Each FM typically has its own requirements and idiosyncrasies for template formatting.\n",
    "- XML tags are used to demarcate structured contextual data. Anthropic's models are trained specifically to work well with XML tags in prompts.\n",
    "- There are several placeholders in the template such as `{AIRLINE}`, `{SRC_CITY}`, `{DST_CITY}`, and so on. We will use LangChain to substitute the placeholders with values from our promoted item. \n",
    "\n",
    "Since there are more fields in our promoted item than we need in the prompt, let's drop the columns that we don't need. If we don't do this, our the placeholder substitution will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59141a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "promoted_item_for_prompt = promoted_item.drop(columns=[\"PROMOTION\", \"EXPIRED\", \"NUMBER_OF_SEARCH_BY_USER\"])\n",
    "promoted_item_for_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b40033",
   "metadata": {},
   "source": [
    "Next we'll create a helper function to substitute the placeholders with values from our promoted item. We'll use [PromptTemplate](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/) from LangChain to help us with this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f2f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_prompt_template(template: str, context: dict) -> str:\n",
    "    keys = list(context.keys())\n",
    "\n",
    "    multi_var_prompt = PromptTemplate(\n",
    "        input_variables=keys,\n",
    "        template=template\n",
    "    )\n",
    "\n",
    "    prompt = multi_var_prompt.format(**context)\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab862f3",
   "metadata": {},
   "source": [
    "### Create prompt for LLM\n",
    "\n",
    "We'll convert our promoted item from a dataframe to a JSON object so we can use it in our interpolation function. Then we'll call our function to generate our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c36882",
   "metadata": {},
   "outputs": [],
   "source": [
    "promoted_item_json = json.loads(promoted_item_for_prompt.to_json(orient='records', lines=True).splitlines()[0])\n",
    "prompt = interpolate_prompt_template(prompt_template, promoted_item_json)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca85b04e",
   "metadata": {},
   "source": [
    "### Calculate the number of input tokens\n",
    "\n",
    "Since the cost of invoking FMs is based on the number of input and output tokens, let's see how many tokens are in our prompt. See the Amazon Bedrock [pricing page](https://aws.amazon.com/bedrock/pricing/) for more details on cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e3ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = textgen_llm.get_num_tokens(prompt)\n",
    "print(f\"Our prompt has {num_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0ac71a",
   "metadata": {},
   "source": [
    "## Invoke LLM to generate email title and body\n",
    "\n",
    "It's time to see what our prompt can do with Claude. The following cell uses LangChain to execute the Claude model in Amazon Bedrock with our prompt. The `%%time` directive will time how long the cell takes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdcbb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "email = textgen_llm(prompt).strip()\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ab13e",
   "metadata": {},
   "source": [
    "How did the model do with our prompt? Did it account for all the specific directions we provided? Do you spot any [hallucinations](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)) in the email subject or email body? \n",
    "\n",
    "Note: You will notice some timing details at the end of the output (CPU times and Wall time). These details are generated by the `%%time` directive in the code cell above and are not part of the model's output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460879b2-86f9-48f1-87ce-6d5348a394e0",
   "metadata": {},
   "source": [
    "## Prompt Chaining - enhance the content relevance\n",
    "\n",
    "Prompt chaining can allow you to accomplish a complex task by passing Claude multiple smaller and simpler prompts instead of a very long and detailed one. It can sometimes work better than putting all of a task's subtasks in a single prompt.\n",
    "\n",
    "Turning a long and complex prompt into a prompt chain can have a few advantages:\n",
    "\n",
    "- You can write less complicated instructions.\n",
    "- You can isolate parts of a problem that Claude is having trouble with to focus your troubleshooting efforts.\n",
    "- You can check Claude's output in stages, instead of just at the end.\n",
    "\n",
    "https://docs.anthropic.com/claude/docs/prompt-chaining\n",
    "\n",
    "In the cell below we're going to chain our original prompt with a prompt that includes the top destinations in Hong Kong to provide context on the destinations to recommend in the itinerary. Although the top destinations list is static in this example, we could have also used Amazon Personalize recipes such as Popularity-Count and Trending-Now to surface top destinations based on popularity or trendiness. We'd just need behavioral data on the destinations that travelers visit in the destination city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272e57ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "# Load the JSON file\n",
    "with open('top50inHK.json', 'r') as file:\n",
    "    top_50_hk = json.load(file)\n",
    "\n",
    "# Define a prompt and additional statement\n",
    "additional_statement = \"Human: The most fun and delicious places to recommend in Hong Kong are listed in <destination> XML tags and grouped by <category> XML tags below. Please enrich the itinerary with these details and generate again.\"\n",
    "\n",
    "categories = []\n",
    "\n",
    "# Loop through each category and its items\n",
    "for category, items in top_50_hk.items():\n",
    "    cat_xml = ET.Element(\"category\", name=category)\n",
    "    for item in items:\n",
    "        ET.SubElement(cat_xml, \"destination\").text = item\n",
    "\n",
    "    categories.append(ET.tostring(cat_xml, \"UTF-8\").decode())\n",
    "\n",
    "combined_tags = \"\\n\".join(categories)\n",
    "\n",
    "prompt_chaining = f\"{prompt}\\n\\n{additional_statement}\\n\\n{combined_tags}\\n\\nAssistant:\"\n",
    "\n",
    "print(prompt_chaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5833e547-f93a-494b-add2-a9c7e14347f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "email2 = textgen_llm(prompt).strip()\n",
    "\n",
    "print(email2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467711f8",
   "metadata": {},
   "source": [
    "How did this prompt do at enhancing the itinerary? Are there destinations from the top 50 included in the output?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d828166",
   "metadata": {},
   "source": [
    "# Generative AI use case 2: email banner image generation <a class=\"anchor\" id=\"usecase2\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "To add to the visual appeal of our promotional email, we will generate an image that complements the text. We will use the Stable Diffusion XL model in Bedrock from Stability AI for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab780d8-19aa-450c-ae0d-c83a33a3c544",
   "metadata": {},
   "source": [
    "## Prepare text-to-image prompt\n",
    "\n",
    "Just like when we created a prompt to generate the email subject and body, we will use a prompt to create our image too. To create our prompt to generate the banner image, we will select a destination from the \"Outdoor and Scenic Spots\" category in the top 50 things to do in Hong Kong list that we used above. This destination will serve as the subject of our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb355219-b6c0-48b6-b226-707ae4ad414e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_subject = random.choice(top_50_hk[\"Outdoor and Scenic Spots\"])\n",
    "print(f\"The subject of our prompt will be \\\"{prompt_subject}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58d80dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_image = f\"Photograph of {prompt_subject} in Hong Kong that is hyperrealistic, highly detailed, sharp focus\"\n",
    "print(f\"Image prompt: {prompt_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fe7b70-a0e1-43f1-ac62-ba5aee86b93d",
   "metadata": {},
   "source": [
    "### Prepare negative prompting\n",
    "\n",
    "In addition to instructing the Stable Diffusion model what we want in our image, we can also instruct the model what we _don't_ want in our image. That is called negative prompts.\n",
    "\n",
    "Below are the phrases that we want to build into our negative prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e5238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_prompt_terms = ['people', 'cartoonish',\n",
    "                         'background distorted', 'distorted perspective',\n",
    "                         'faded color', 'flawed', 'flipped', 'folded', 'improper proportion',\n",
    "                         'incomplete', 'incorrect geometry', 'inverted', 'kitsch', 'low quality',\n",
    "                         'low resolution', 'macabre', 'misaligned parts', 'misshapen', 'missing parts',\n",
    "                         'mutated', 'off-center', 'out of focus', 'over-saturated', 'overexposed', 'oversized',\n",
    "                         'poorly rendered', 'surreal', 'tilted', 'underexposed', 'unrealistic', 'upside down']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253cdf8d-c7bb-4808-947b-95473ab5a4eb",
   "metadata": {},
   "source": [
    "### Stable Diffusion parameters\n",
    "\n",
    "In addition to the prompts, the Stable Diffusion model supports several parameters that control how the image is generated. Some of the parameters we will use for our image include:\n",
    "\n",
    "- `cfg_scale`: Prompt strength– Determines how much the final image portrays the prompt random generations. The range is 0—30, and the default value is 10. The `cfg_scale` essentially governs how much the image looks closer to the prompt or input image. The higher the CFG scale, the more the image will match your prompt. Conversely, a lower CFG scale value produces a better-quality image that may differ from the original prompt or image. In Stable Diffusion, CFG stands for Classifier Free Guidance scale. CFG is the setting that controls how closely Stable Diffusion should follow your text prompt. It is applied in text-to-image (txt2img) and image-to-image (img2img) generations. The higher the CFG value, the more strictly it will follow your prompt, in theory. The default value is 10, which gives a good balance between creative freedom and following your direction. A value of 1 will give Stable Diffusion almost complete freedom, whereas values above 15 are quite restrictive.\n",
    "- `step`: Generation step determines how many times the image is sampled. More steps can result in a more accurate result. The range is 0—150, and the default value is 5.\n",
    "- `seed`: The seed determines the initial noise setting. If you use the same seed and the same settings as a previous run, inference creates a similar image. The seed value is a random number.\n",
    "- `style_preset`: the parameter includes enhance, anime, photographic, digital-art, comic-book, fantasy-art, line-art, analog-film, neon-punk, isometric, low-poly, origami, modeling-compound, cinematic, 3d-model, pixel-art, and tile-texture. This list of style presets is subject to change; refer to the latest release and documentation for updates.\n",
    "\n",
    "https://platform.stability.ai/docs/api-reference#tag/v1generation/operation/textToImage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c23e810",
   "metadata": {},
   "source": [
    "## Generate image\n",
    "\n",
    "The following cell will prepare the parameters to pass to the Stable Diffusion model and invoke the model using the Bedrock API client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e68d3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "style_preset = \"photographic\"  # (e.g. photographic, digital-art, cinematic, ...)\n",
    "\n",
    "# Generate a random seed value\n",
    "random_seed = random.randint(1, 9999999)  # Adjust the range as needed\n",
    "\n",
    "request = json.dumps({\n",
    "    \"text_prompts\": (\n",
    "        [{\"text\": prompt_image, \"weight\": 1.0}]\n",
    "        + [{\"text\": (\",\".join(negative_prompt_terms)), \"weight\": -1.0}]\n",
    "    ),\n",
    "    \"cfg_scale\": 20,\n",
    "    \"height\": 640, # integer (DiffuseImageHeight) multiple of 64 >= 128, Default: 512\n",
    "    \"width\": 1536, # integer (DiffuseImageWidth) multiple of 64 >= 128, Default: 512\n",
    "    \"seed\": random_seed,  # Assign the random seed value here\n",
    "    \"steps\": 45,\n",
    "    \"style_preset\": style_preset,\n",
    "})\n",
    "model_id = \"stability.stable-diffusion-xl-v1\"\n",
    "\n",
    "response = bedrock_runtime.invoke_model(body=request, modelId=model_id)\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "print(f\"API result: {response_body['result']}\")\n",
    "base_64_img_str = response_body[\"artifacts\"][0].get(\"base64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10f0a7a-1793-4ab4-96cb-aa830fdc0bd5",
   "metadata": {},
   "source": [
    "### Decode generated image\n",
    "\n",
    "Since the response from Bedrock encodes the image data as a Base64 string, we must decode the string to binary values and feed the binary format to an image processing library so we can render the image. In this case, we will use the Pillow library available in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a41629b-e0ab-4518-a21d-614265a72fd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image_1 = Image.open(io.BytesIO(base64.decodebytes(bytes(base_64_img_str, \"utf-8\"))))\n",
    "image_1_path = data_dir + \"/image_1.png\"\n",
    "image_1.save(image_1_path)\n",
    "image_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fae8b3-a919-42c9-b769-fcd895649671",
   "metadata": {},
   "source": [
    "## Image to Image\n",
    "\n",
    "Generating images from text is powerful, but in some cases that may require many rounds of prompt refinement to get an image \"just right\".\n",
    "\n",
    "Rather than starting from scratch with text each time, image-to-image generation lets us modify an existing image to make the specific changes we'd like.\n",
    "\n",
    "We'll have to pass our initial image in to the API in base64 encoding, so first let's prepare that. You can use either the initial image from the previous section, or a different one if you'd prefer. The following function will help us do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ec7b5f-e248-4fed-8948-74d808f93254",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def image_to_base64(img) -> str:\n",
    "    \"\"\"Convert a PIL Image or local image file path to a base64 string for Amazon Bedrock\"\"\"\n",
    "    if isinstance(img, str):\n",
    "        if os.path.isfile(img):\n",
    "            print(f\"Reading image from file: {img}\")\n",
    "            with open(img, \"rb\") as f:\n",
    "                return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File {img} does not exist\")\n",
    "    elif isinstance(img, Image.Image):\n",
    "        print(\"Converting PIL Image to base64 string\")\n",
    "        buffer = io.BytesIO()\n",
    "        img.save(buffer, format=\"PNG\")\n",
    "        return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    else:\n",
    "        raise ValueError(f\"Expected str (filename) or PIL Image. Got {type(img)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ed41b-f97d-42bf-b2b1-c41c18390913",
   "metadata": {},
   "source": [
    "### Use the sample image as input image\n",
    "\n",
    "We load a sample image to use as the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a8183-8862-4ab9-8f69-606cf68dc45e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define image_to_image1 as a PIL Image object with your actual image file path)\n",
    "image_to_image1 = Image.open('image_to_image1.png')\n",
    "\n",
    "init_image_b64 = image_to_base64(image_to_image1)\n",
    "image_to_image1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b1358d",
   "metadata": {},
   "source": [
    "### Generate image\n",
    "\n",
    "Since the sample image is of Victoria Harbour, we'll design a prompt that changes the lighting and attempts to remove boats from the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2657202b-2dda-473d-8728-5fe8bf50d52c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "prompt_image = f\"Photograph of Victoria Harbour in Hong Kong, night, night lights, hyperrealistic, highly detailed, sharp focus\"\n",
    "\n",
    "if not \"boats\" in negative_prompt_terms:\n",
    "    negative_prompt_terms.insert(0, \"boats\")\n",
    "\n",
    "# Generate a random seed value\n",
    "random_seed = random.randint(1, 9999999)  # Adjust the range as needed\n",
    "\n",
    "request = json.dumps({\n",
    "    \"text_prompts\": (\n",
    "        [{\"text\": prompt_image, \"weight\": 1.0}]\n",
    "        + [{\"text\": (\",\".join(negative_prompt_terms)), \"weight\": -1.0}]\n",
    "    ),\n",
    "    \"cfg_scale\": 20,\n",
    "    \"init_image\": init_image_b64,\n",
    "    \"image_strength\": 0.5, # number <float> (InitImageStrength) [ 0 .. 1 ], Default: 0.35, Values close to 1 will yield images very similar to the init_image\n",
    "    \"height\": 640, # integer (DiffuseImageHeight) multiple of 64 >= 128, Default: 512\n",
    "    \"width\": 1536, # integer (DiffuseImageWidth) multiple of 64 >= 128, Default: 512\n",
    "    \"seed\": random_seed,\n",
    "    \"start_schedule\": 0.6,\n",
    "    \"steps\": 45,\n",
    "    \"style_preset\": style_preset,\n",
    "})\n",
    "model_id = \"stability.stable-diffusion-xl-v1\"\n",
    "\n",
    "response = bedrock_runtime.invoke_model(body=request, modelId=model_id)\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "image_2_b64_str = response_body[\"artifacts\"][0].get(\"base64\")\n",
    "print(f\"API result: {response_body['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10c394b-3f5d-4016-b181-d373a870680c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_2 = Image.open(io.BytesIO(base64.decodebytes(bytes(image_2_b64_str, \"utf-8\"))))\n",
    "image_2_path = data_dir + \"/image_2.png\"\n",
    "image_2.save(image_2_path)\n",
    "image_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e511e91",
   "metadata": {},
   "source": [
    "# Putting it all together <a class=\"anchor\" id=\"alltogether\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "With our email subject, email body, and banner image generated, let's assemble the finished product for each of our target users. To keep it simple, we'll create HTML pages that represent an HTML email that we'd send to each user.\n",
    "\n",
    "First, we need to isolate the email subject and title from the generated response from the Claude Instant model. If you recall from the prompt used to generate the email text, we asked Claude to place the email title/subject and body within XML tags. This instruction in the prompt allows us to more easily parse the response to separate these two pieces of content. To do so, we'll wrap the output in an outer `<email></email>` tag and then parse it as an XML document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2032f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "root = ET.fromstring(\"<email>\" + email2 + \"</email>\")\n",
    "subject = root.find(\"email_title\").text.strip()\n",
    "body = root.find(\"email_body\").text.strip()\n",
    "\n",
    "print(f\"Email subject: {subject}\")\n",
    "print(f\"Email body: {body}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a90aab",
   "metadata": {},
   "source": [
    "Now we can reference these text elements more easily in our HTML email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9675b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "body_html = body.replace(\"\\n\", \"<br/>\")\n",
    "display(HTML(f'<hr/><h1>{subject}</h1><img src=\"{data_dir}/image_2.png\"/><p>{body_html}</p><hr/>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783ee2aa-4904-40c8-82c1-912498befaa3",
   "metadata": {},
   "source": [
    "# Summary <a class=\"anchor\" id=\"summary\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "In this notebook we used two foundation models available in Amazon Bedrock to generate the subject and body of our promotional email as well as a banner image to include in our email. We also learned how to use the LangChain framework to make working with and building prompts easier. In the third and final notebook, we'll learn how to add content moderation to our project to ensure that the content created in those notebook does not contain any unsafe or inappropriate content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e609fcc",
   "metadata": {},
   "source": [
    "## Storing Useful Variables <a class=\"anchor\" id=\"vars\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Before exiting this notebook, run the following cells to save variables for use in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a11477",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store email2\n",
    "%store image_1_path\n",
    "%store image_2_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2fd8fd",
   "metadata": {},
   "source": [
    "[Go to the next notebook `03_Content_Moderation_Generative_AI.ipynb`](03_Content_Moderation_Generative_AI.ipynb) to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4210f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
