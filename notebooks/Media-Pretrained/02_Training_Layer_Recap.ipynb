{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommenders and Solutions Recap and Evaluation<a class=\"anchor\" id=\"top\"></a>\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. [Introduction](#intro)\n",
    "1. [Configure an S3 bucket and an IAM  role](#bucket_role)\n",
    "1. [Group Dataset](#group_dataset)\n",
    "1. [Create the Interactions Schema](#interact_schema)\n",
    "1. [Create the Items(Movies) Schema](#items_schema)\n",
    "1. [Create the Users Schema](#users_schema)\n",
    "1. [Import the interactions data](#import_interactions)\n",
    "1. [Import the Item Metadata](#import_items)\n",
    "1. [Import the User Metadata](#import_users)\n",
    "1. [Create Use Case Optimized Recommenders](#recommenders)\n",
    "1. [Create solutions](#solutions)\n",
    "1. [Evaluate solutions and recommenders](#eval)\n",
    "1. [Using Evaluation Metrics](#use)\n",
    "1. [Deploy a campaign](#deploy)\n",
    "1. [Storing useful variables](#wrapup)\n",
    "\n",
    "To run this notebook, you need to have run [the previous notebook: `01_Data_Layer.ipynb`](01_Data_Layer.ipynb), where you created a dataset and imported interaction, item, and user metadata data into Amazon Personalize. At the end of that notebook, you saved some of the variable values, which you now need to load into this notebook.\n",
    "\n",
    "## Introduction <a class=\"anchor\" id=\"intro\"></a>\n",
    "\n",
    "In the previous notebook we prepared 3 different datasets that represent sample data that would exist in a Media & Entertainment applicatiopn (User interactions, Media catalog data and subscriber/user data). In order to complete this workshoop within the time set, we have already created several resources on your behalf. \n",
    "\n",
    "### In this notebook we will accomplish the following:\n",
    "\n",
    "Walk you through how we created Video on Demand Use Case Optimized Recommenders for the following use cases:\n",
    "\n",
    "1. [More like X](https://docs.aws.amazon.com/personalize/latest/dg/VIDEO_ON_DEMAND-use-cases.html#more-like-y-use-case): recommendations for movies that are similar to a movie that you specify. With this use case, Amazon Personalize automatically filters movies the user watched based on the userId that you specify and Watch events.\n",
    "\n",
    "1. [Top picks for you](https://docs.aws.amazon.com/personalize/latest/dg/VIDEO_ON_DEMAND-use-cases.html#top-picks-use-case): personalized content recommendations for a user that you specify. With this use case, Amazon Personalize automatically filters videos the user watched based on the userId that you specify and Watch events.\n",
    "\n",
    "Walk you through how we created custom solution and solution versions for the following use case:\n",
    "\n",
    "3. [Personalized-Ranking](https://docs.aws.amazon.com/personalize/latest/dg/working-with-predefined-recipes.html): will be used to rerank a list of movies.\n",
    "\n",
    "The following diagram shows the resources that we will create in this section, highlighted in blue.\n",
    "\n",
    "![Workflow](images/image2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous notebook, start by importing the relevant packages, and set up a connection to Amazon Personalize using the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the latest version of botocore to ensure we have the latest features in the SDK\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install --upgrade --no-deps --force-reinstall botocore\n",
    "import time\n",
    "from time import sleep\n",
    "import json\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "import random\n",
    "import boto3\n",
    "import botocore\n",
    "from botocore.exceptions import ClientError\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the SDK to Personalize:\n",
    "personalize = boto3.client('personalize')\n",
    "personalize_runtime = boto3.client('personalize-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('params.json')\n",
    "parameters = json.load(f)\n",
    "print(json.dumps(parameters, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workshop_dataset_group_name = parameters['datasetGroup']['serviceConfig']['name']\n",
    "\n",
    "interactions_schema_name = 'workshop-personalize-poc-movielens-interactions-schema'\n",
    "interactions_dataset_name = parameters['datasets']['interactions']['dataset']['serviceConfig']['name']\n",
    "\n",
    "items_schema_name = 'workshop-personalize-poc-movielens-items-schema'\n",
    "items_dataset_name = parameters['datasets']['items']['dataset']['serviceConfig']['name']\n",
    "\n",
    "users_schema_name = 'workshop-personalize-poc-movielens-users-schema'\n",
    "users_dataset_name = parameters['datasets']['users']['dataset']['serviceConfig']['name']\n",
    "\n",
    "interactions_import_job_name = 'workshop-personalize-poc-interactions-import'\n",
    "items_import_job_name = 'workshop-personalize-poc-items-import'\n",
    "users_import_job_name = 'workshop-personalize-poc-users-import'\n",
    "\n",
    "for recommender in parameters['recommenders']:\n",
    "    # This is currently configured assuming only one recommender of each type, if there are multiple \n",
    "    # recommenders of the same type further configuration is needed.\n",
    "    if (recommender['serviceConfig']['recipeArn'] == 'arn:aws:personalize:::recipe/aws-vod-more-like-x'):\n",
    "        recommender_more_like_x_name =recommender['serviceConfig']['name'] \n",
    "    if (recommender['serviceConfig']['recipeArn'] == 'arn:aws:personalize:::recipe/aws-vod-top-picks'):\n",
    "        recommender_top_picks_for_you_name =recommender['serviceConfig']['name']\n",
    "        \n",
    "for solution in parameters['solutions']:\n",
    "    # This is currently configured assuming only one solution of this type, if there are multiple \n",
    "    # solutions of the same type further configuration is needed.\n",
    "    if (solution['serviceConfig']['recipeArn'] == 'arn:aws:personalize:::recipe/aws-personalized-ranking'):\n",
    "        workshop_rerank_solution_name = solution['serviceConfig']['name'] \n",
    "        # This is currently configured assuming only one campaign, if there are multiple campaigns \n",
    "        # further configuration is needed.\n",
    "        workshop_rerank_campaign_name = solution['campaigns'][0]['serviceConfig']['name'] \n",
    "        print (workshop_rerank_campaign_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train your Use Case Optimized Recommenders and Solution Versions\n",
    "\n",
    "As mentioned at the top of this notebook, a dataset group, schemas, datasets, solutions, and campaigns have already been created for you. You can open another browser tab/window to view these resources in the Personalize AWS Console.\n",
    "\n",
    "Below we will walk you through the steps we used to create these resources. Since they are already created, we will only be retrieving the automated deployment variables, however you can also run this code to train resources if you have not run the automation.\n",
    "\n",
    "Please take into account that creating these resources in your account will incur a cost and that it will take upload and training time to do these steps trough this notebook (this can be several hours)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure an S3 bucket and an IAM  role <a class=\"anchor\" id=\"bucket_role\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "So far, we have downloaded, manipulated, and saved the data onto the Amazon EBS instance attached to instance running this Jupyter notebook. \n",
    "\n",
    "By default, the Amazon Personalize service does not have permission to access the data we uploaded into the S3 bucket in our account. In order to grant access to the Amazon Personalize service to read our CSVs, we need to set a Bucket Policy and create an IAM role that the Amazon Personalize service will assume. Let's set all of that up.\n",
    "\n",
    "Use the metadata stored on the instance underlying this Amazon SageMaker notebook, to determine the region it is operating in. If you are using a Jupyter notebook outside of Amazon SageMaker, simply define the region as a string below. The Amazon S3 bucket needs to be in the same region as the Amazon Personalize resources we have been creating so far.\n",
    "\n",
    "First, let us get the current notebook region. If you are running your Amazon personalize experiments in a different region than your notebooks, you can specify it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/opt/ml/metadata/resource-metadata.json') as notebook_info:\n",
    "    data = json.load(notebook_info)\n",
    "    resource_arn = data['ResourceArn']\n",
    "    region = resource_arn.split(':')[3]\n",
    "print(\"region:\", region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon S3 bucket names are globally unique. To create a unique bucket name, the code below will append the string `personalizepocvod` to your AWS account number. Then it creates a bucket with this name in the region discovered in the previous cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "bucket_name = account_id + \"-\" + region + \"-\" + \"personalizepocvod\"\n",
    "\n",
    "#getting existing buckets in the account\n",
    "response = s3.list_buckets()\n",
    "\n",
    "if bucket_name in [x['Name'] for x in response['Buckets']]:\n",
    "    print(\"The bucket already exists.\")\n",
    "else:\n",
    "    if region == \"us-east-1\":\n",
    "        bucket_responese = s3.create_bucket(Bucket=bucket_name)\n",
    "    else:\n",
    "        bucket_responese = s3.create_bucket(\n",
    "            Bucket=bucket_name,\n",
    "            CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "print('bucket_name:', bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the S3 bucket policy\n",
    "Amazon Personalize needs to be able to read the contents of your S3 bucket. So add a bucket policy which allows that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Id\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"personalize.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": [\n",
    "                \"s3:*Object\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::{}\".format(bucket_name),\n",
    "                \"arn:aws:s3:::{}/*\".format(bucket_name)\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    bucket_current_policy = s3.get_bucket_policy(Bucket=bucket_name)['Policy']\n",
    "    \n",
    "except s3.exceptions.from_code('NoSuchBucketPolicy') as e:    \n",
    "    print(\"There is no current Bucket Policy for bucket \" + bucket_name)\n",
    "    \n",
    "except Exception as e: \n",
    "    raise(e)\n",
    "\n",
    "if (policy == json.loads(bucket_current_policy)):\n",
    "    print (\"The policy is already associated with the S3 Bucket.\")\n",
    "else:\n",
    "    print (\"Adding the policy to the bucket.\")\n",
    "    print(s3.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(policy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an IAM role\n",
    "\n",
    "Amazon Personalize needs the ability to assume roles in AWS in order to have the permissions to execute certain tasks. Let's create an IAM role and attach the required policies to it so it can access data from S3. The code below attaches very permissive policies, please use more restrictive policies for any production application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client(\"iam\")\n",
    "\n",
    "role_name = account_id+\"-PersonalizeS3-Immersion-Day\"\n",
    "assume_role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "          \"Effect\": \"Allow\",\n",
    "          \"Principal\": {\n",
    "            \"Service\": \"personalize.amazonaws.com\"\n",
    "          },\n",
    "          \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create or retrieve the role:\n",
    "try:\n",
    "    create_role_response = iam.create_role(\n",
    "        RoleName = role_name,\n",
    "        AssumeRolePolicyDocument = json.dumps(assume_role_policy_document)\n",
    "    );\n",
    "    role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "    \n",
    "except iam.exceptions.EntityAlreadyExistsException as e:\n",
    "    print('Warning: role already exists: {}\\n'.format(e))\n",
    "    role_arn = iam.get_role(\n",
    "        RoleName = role_name\n",
    "    )[\"Role\"][\"Arn\"];\n",
    "\n",
    "print('IAM Role: {}\\n'.format(role_arn))\n",
    "\n",
    "\n",
    "# Attach the policy if it is not previously attached:\n",
    "policy_arn = \"arn:aws:iam::aws:policy/AmazonS3FullAccess\"\n",
    "\n",
    "if (policy_arn in [ x['PolicyArn'] for x in iam.list_attached_role_policies( RoleName = role_name)['AttachedPolicies']]):\n",
    "    print ('The policy {} is already attached to this role.'.format(policy_arn))\n",
    "else:\n",
    "    print (\"Attaching the role_policy\")\n",
    "    attach_response = iam.attach_role_policy(\n",
    "        RoleName = role_name,\n",
    "        PolicyArn = \"arn:aws:iam::aws:policy/AmazonS3FullAccess\"\n",
    "    );\n",
    "    print (\"30s pause to allow role to be fully consistent.\")\n",
    "    time.sleep(30)\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3\n",
    "\n",
    "Now that your Amazon S3 bucket has been created, upload the CSV file of our user-item-interaction data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_file_path = data_dir + \"/\" + interactions_filename\n",
    "\n",
    "try:\n",
    "    s3.get_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=interactions_filename,\n",
    "    )\n",
    "    print(\"{} already exists in the bucket {}\".format(interactions_file_path, bucket_name))\n",
    "except s3.exceptions.NoSuchKey:\n",
    "    boto3.Session().resource('s3').Bucket(bucket_name).Object(interactions_filename).upload_file(interactions_file_path)\n",
    "    print(\"File {} uploaded to bucket {}\".format(interactions_filename, bucket_name))\n",
    "\n",
    "items_file_path = data_dir + \"/\" + items_filename\n",
    "try:\n",
    "    s3.get_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=items_filename,\n",
    "    )\n",
    "    print(\"{} already exists in the bucket {}\".format(items_file_path, bucket_name))\n",
    "except s3.exceptions.NoSuchKey:\n",
    "    boto3.Session().resource('s3').Bucket(bucket_name).Object(items_filename).upload_file(items_file_path)\n",
    "    print(\"File {} uploaded to bucket {}\".format(items_filename, bucket_name))\n",
    "\n",
    "users_file_path = data_dir + \"/\" + users_filename\n",
    "try:\n",
    "    s3.get_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=users_filename,\n",
    "    )\n",
    "    print(\"{} already exists in the bucket {}\".format(users_file_path, bucket_name))\n",
    "except s3.exceptions.NoSuchKey:\n",
    "    boto3.Session().resource('s3').Bucket(bucket_name).Object(users_filename).upload_file(users_file_path)\n",
    "    print(\"File {} uploaded to bucket {}\".format(users_filename, bucket_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dataset Group <a class=\"anchor\" id=\"group_dataset\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "The highest level of isolation and abstraction with Amazon Personalize is a *dataset group*. Information stored within one of these dataset groups has no impact on any other dataset group or models created from one - they are completely isolated. This allows you to run many experiments and is part of how we keep your models private and fully trained only on your data. \n",
    "\n",
    "Before importing the data prepared earlier, there needs to be a dataset group and a dataset added to it that handles the interactions.\n",
    "\n",
    "Dataset groups can house the following types of information:\n",
    "\n",
    "* User-item-interactions\n",
    "* Event streams (real-time interactions)\n",
    "* User metadata\n",
    "* Item metadata\n",
    "\n",
    "We need to create the dataset group that will contain our three datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Dataset Group\n",
    "\n",
    "The following cell will create a new dataset group with the name `personalize-poc-movielens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    create_dataset_group_response = personalize.create_dataset_group(\n",
    "        name = workshop_dataset_group_name,\n",
    "        domain='VIDEO_ON_DEMAND'\n",
    "    )\n",
    "\n",
    "    workshop_dataset_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "    print(json.dumps(create_dataset_group_response, indent=2))\n",
    "    print ('\\nCreating the Dataset Group with dataset_group_arn = {}'.format(workshop_dataset_group_arn))\n",
    "\n",
    "except personalize.exceptions.ResourceAlreadyExistsException as e:  \n",
    "    workshop_dataset_group_arn = 'arn:aws:personalize:'+region+':'+account_id+':dataset-group/'+workshop_dataset_group_name \n",
    "    print ('\\nThe the Dataset Group eith dataset_group_arn = {} already exists'.format(workshop_dataset_group_arn))\n",
    "    print ('\\nWe will be using the existing Dataset Group dataset_group_arn = {}'.format(workshop_dataset_group_arn))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Dataset Group to have ACTIVE Status \n",
    "\n",
    "Before we can use the Dataset Group to create more resources below, it must be active. This can take a minute or two. Execute the cell below and wait for it to show the ACTIVE status. It checks the status of the dataset group every 60 seconds, up to a maximum of 3 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_group_response = personalize.describe_dataset_group(\n",
    "        datasetGroupArn = workshop_dataset_group_arn\n",
    "    )\n",
    "    status = describe_dataset_group_response[\"datasetGroup\"][\"status\"]\n",
    "    print(\"DatasetGroup: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a dataset group, you can create a dataset for the interaction data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Interactions Schema <a class=\"anchor\" id=\"interact_schema\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Now that we've loaded and prepared our three datasets we'll configure the Amazon Personalize service to understand our data so that it can be used to train models for generating recommendations. Amazon Personalize requires a schema for each dataset, so it can map the columns in our CSVs to fields for model training. Each schema is declared in JSON using the [Apache Avro](https://avro.apache.org/) format. \n",
    "\n",
    "First, define a schema to tell Amazon Personalize what type of dataset you are uploading. There are several mandatory fields that are required in the schema, depending on the type of dataset. More detailed information can be found in the [documentation](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html).\n",
    "\n",
    "The interactions dataset has three required columns: `ITEM_ID`, `USER_ID`, and `TIMESTAMP`. The `TIMESTAMP` represents when the user interated with an item and must be expressed in Unix timestamp format (seconds). For this dataset we also have an `EVENT_TYPE` column. These must be defined in the same order in the schema as they appear in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Interactions\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"EVENT_TYPE\", # \"Watch\", \"Click\", etc.\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TIMESTAMP\",\n",
    "            \"type\": \"long\"\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "\n",
    "try:\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = interactions_schema_name,\n",
    "        schema = json.dumps(interactions_schema),\n",
    "        domain='VIDEO_ON_DEMAND'\n",
    "    )\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "    workshop_interactions_schema_arn = create_schema_response['schemaArn']\n",
    "    print ('\\nCreating the Interactions Schema with workshop_interactions_schema_arn = {}'.format(workshop_interactions_schema_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    workshop_interactions_schema_arn = 'arn:aws:personalize:'+region+':'+account_id+':schema/'+interactions_schema_name \n",
    "    print('The schema {} already exists.'.format(workshop_interactions_schema_arn))\n",
    "    print ('\\nWe will be using the existing Interactions Shema with workshop_interactions_schema_arn = {}'.format(workshop_interactions_schema_arn))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Interactions Dataset\n",
    "\n",
    "With a schema created, you can create a dataset within the dataset group. Note that this does not load the data yet, but creates a schema of what the data looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    dataset_type = 'INTERACTIONS'\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = interactions_dataset_name,\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = workshop_dataset_group_arn,\n",
    "        schemaArn = workshop_interactions_schema_arn\n",
    "    )\n",
    "\n",
    "    workshop_interactions_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "    print ('\\nCreating the Interactions Dataset with workshop_interactions_dataset_arn = {}'.format(workshop_interactions_dataset_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    workshop_interactions_dataset_arn =  'arn:aws:personalize:'+region+':'+account_id+':dataset/'+workshop_dataset_group_name+'/INTERACTIONS'\n",
    "    print('The Interactions Dataset {} already exists.'.format(workshop_interactions_dataset_arn))\n",
    "    print ('\\nWe will be using the existing Interactions Dataset with workshop_interactions_dataset_arn = {}'.format(workshop_interactions_dataset_arn))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Items (Movies) Schema<a class=\"anchor\" id=\"items_schema\"></a>\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a schema to tell Amazon Personalize what type of dataset we are uploading. There are several reserved and mandatory keywords required in the schema, based on the type of dataset. More detailed information can be found in the [documentation](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html).\n",
    "\n",
    "Our item metadata data has the following columns: `ITEM_ID`, `TITLE`, `YEAR`, `IMDB_RATING`,`IMDB_NUMBEROFVOTES`,  `PLOT`, `US_MATURITY_RATING_STRING`, `US_MATURITY_RATING`,`GENRES`, `CREATION_TIMESTAMP`, and `PROMOTION` fields. These must be defined in the same order in the schema as they appear in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Items\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TITLE\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"YEAR\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"IMDB_RATING\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"IMDB_NUMBEROFVOTES\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"PLOT\",\n",
    "            \"type\": \"string\",\n",
    "            \"textual\": True\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"US_MATURITY_RATING_STRING\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"US_MATURITY_RATING\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"GENRES\",\n",
    "            \"type\": \"string\",\n",
    "            \"categorical\": True\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"CREATION_TIMESTAMP\",\n",
    "            \"type\": \"long\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"PROMOTION\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = items_schema_name,\n",
    "        schema = json.dumps(items_schema),\n",
    "        domain='VIDEO_ON_DEMAND'\n",
    "    )\n",
    "    workshop_items_schema_arn = create_schema_response['schemaArn']\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "\n",
    "    print ('\\nCreating the Items Schema with workshop_items_schema_arn = {}'.format(workshop_items_schema_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    workshop_items_schema_arn = 'arn:aws:personalize:'+region+':'+account_id+':schema/'+items_schema_name \n",
    "    print('The schema {} already exists.'.format(workshop_items_schema_arn))\n",
    "    print ('\\nWe will be using the existing Items Schema with workshop_items_schema_arn = {}'.format(workshop_items_schema_arn))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Items Dataset\n",
    "\n",
    "With a schema created, you can create a dataset within the dataset group. Note that this does not load the data yet, but creates a schema of what the data looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    \n",
    "    dataset_type = \"ITEMS\"\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = items_dataset_name,\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = workshop_dataset_group_arn,\n",
    "        schemaArn = workshop_items_schema_arn\n",
    "    )\n",
    "\n",
    "    workshop_items_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "\n",
    "    print ('\\nCreating the Items Dataset with workshop_items_dataset_arn = {}'.format(workshop_items_dataset_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    workshop_items_dataset_arn =  'arn:aws:personalize:'+region+':'+account_id+':dataset/'+workshop_dataset_group_name+'/ITEMS'\n",
    "    print('The Items Dataset {} already exists.'.format(workshop_items_dataset_arn))\n",
    "    print ('\\nWe will be using the existing Items Dataset with workshop_items_dataset_arn = {}'.format(workshop_items_dataset_arn))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Users Schema<a class=\"anchor\" id=\"users_schema\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "First, define a schema to tell Amazon Personalize what type of dataset you are uploading. There are several reserved and mandatory keywords required in the schema, based on the type of dataset. More detailed information can be found in the [documentation](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html).\n",
    "\n",
    "Here, you will create a schema for user data, which requires the `USER_ID`, and an additonal metadata field, in this case `GENDER`. These must be defined in the same order in the schema as they appear in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Users\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"MEMBERLEVEL\",\n",
    "            \"type\": \"string\",\n",
    "            \"categorical\": True\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "    \n",
    "try:\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = users_schema_name,\n",
    "        schema = json.dumps(users_schema),\n",
    "        domain='VIDEO_ON_DEMAND'\n",
    "    )\n",
    "    \n",
    "    workshop_users_schema_arn = create_schema_response['schemaArn']\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "\n",
    "    print ('\\nCreating the Users Schema with workshop_users_schema_arn = {}'.format(workshop_users_schema_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    workshop_users_schema_arn = 'arn:aws:personalize:'+region+':'+account_id+':schema/'+users_schema_name \n",
    "    print('The schema {} already exists.'.format(workshop_users_schema_arn))\n",
    "    print ('\\nWe will be using the existing Users Schema with workshop_users_schema_arn = {}'.format(workshop_users_schema_arn))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Users dataset\n",
    "\n",
    "With a schema created, you can create a dataset within the dataset group. Note that this does not load the data yet, but creates a schema of what the data looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dataset_type = \"USERS\"\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = users_dataset_name,\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = workshop_dataset_group_arn,\n",
    "        schemaArn = workshop_users_schema_arn\n",
    "    )\n",
    "\n",
    "    workshop_users_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "\n",
    "    print ('\\nCreating the Users Dataset with workshop_users_dataset_arn = {}'.format(workshop_users_dataset_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    workshop_users_dataset_arn =  'arn:aws:personalize:'+region+':'+account_id+':dataset/'+workshop_dataset_group_name+'/USERS'\n",
    "    print('The Users Dataset {} already exists.'.format(workshop_users_dataset_arn))\n",
    "    print ('\\nWe will be using the existing Users Dataset with workshop_users_dataset_arn = {}'.format(workshop_users_dataset_arn))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wait until all the datasets have been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_time = time.time() + 6*60*60 # 6 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_response = personalize.describe_dataset(\n",
    "        datasetArn = workshop_interactions_dataset_arn\n",
    "    )\n",
    "    status =  describe_dataset_response[\"dataset\"]['status']\n",
    "    print(\"Interactions Dataset: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)\n",
    "    \n",
    "while time.time() < max_time:\n",
    "    describe_dataset_response = personalize.describe_dataset(\n",
    "        datasetArn = workshop_items_dataset_arn\n",
    "    )\n",
    "    status =  describe_dataset_response[\"dataset\"]['status']\n",
    "    print(\"Items Dataset: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)\n",
    "    \n",
    "while time.time() < max_time:\n",
    "    describe_dataset_response = personalize.describe_dataset(\n",
    "        datasetArn = workshop_users_dataset_arn\n",
    "    )\n",
    "    status =  describe_dataset_response[\"dataset\"]['status']\n",
    "    print(\"Users Dataset: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the interactions data <a class=\"anchor\" id=\"import_interactions\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Earlier you created the dataset group and dataset to house your information, so now you will execute an import job that will load the interactions data from the S3 bucket into the Amazon Personalize dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_dataset_import_jobs = personalize.list_dataset_import_jobs(\n",
    "    datasetArn=workshop_interactions_dataset_arn,\n",
    "    maxResults=100\n",
    ")['datasetImportJobs']\n",
    "\n",
    "\n",
    "if interactions_import_job_name in [x['jobName'] for x in interactions_dataset_import_jobs]:\n",
    "    \n",
    "    workshop_interactions_dataset_import_job_arn =  'arn:aws:personalize:'+region+':'+account_id+':dataset-import-job/'+interactions_import_job_name\n",
    "    print('The Interactions Import Job {} already exists.'.format(workshop_interactions_dataset_import_job_arn))\n",
    "    print ('\\nWe will be using the existing Interactions Import Job with workshop_interactions_dataset_import_job_arn = {}'.format(workshop_interactions_dataset_import_job_arn))\n",
    "        \n",
    "else:\n",
    "    create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "        jobName = interactions_import_job_name,\n",
    "        datasetArn = workshop_interactions_dataset_arn,\n",
    "        dataSource = {\n",
    "            \"dataLocation\": \"s3://{}/{}\".format(bucket_name, interactions_filename)\n",
    "        },\n",
    "        roleArn = role_arn\n",
    "    )\n",
    "    workshop_interactions_dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "    print(json.dumps(create_dataset_import_job_response, indent=2))\n",
    "    \n",
    "    print ('\\nImporting the Interactions Data with workshop_interactions_dataset_import_job_arn = {}'.format(workshop_interactions_dataset_import_job_arn))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Item Metadata <a class=\"anchor\" id=\"import_items\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Earlier you created the dataset group and dataset to house your information, now you will execute an import job that will load the item data from the S3 bucket into the Amazon Personalize dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_dataset_import_jobs = personalize.list_dataset_import_jobs(\n",
    "    datasetArn=workshop_items_dataset_arn,\n",
    "    maxResults=100\n",
    ")['datasetImportJobs']\n",
    "\n",
    "\n",
    "if items_import_job_name in [x['jobName'] for x in items_dataset_import_jobs]:\n",
    "    \n",
    "    workshop_items_dataset_import_job_arn =  'arn:aws:personalize:'+region+':'+account_id+':dataset-import-job/'+items_import_job_name\n",
    "    print('The Items Import Job {} already exists.'.format(workshop_items_dataset_import_job_arn))\n",
    "    print ('\\nWe will be using the existing Items Import Job with workshop_items_dataset_import_job_arn = {}'.format(workshop_items_dataset_import_job_arn))\n",
    "        \n",
    "else:\n",
    "    create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "        jobName = items_import_job_name,\n",
    "        datasetArn = workshop_items_dataset_arn,\n",
    "        dataSource = {\n",
    "            \"dataLocation\": \"s3://{}/{}\".format(bucket_name, items_filename)\n",
    "        },\n",
    "        roleArn = role_arn\n",
    "    )\n",
    "\n",
    "    workshop_items_dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "    print(json.dumps(create_dataset_import_job_response, indent=2))\n",
    "    print ('\\nImporting the Items Data with workshop_items_dataset_import_job_arn = {}'.format(workshop_items_dataset_import_job_arn))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the User Metadata <a class=\"anchor\" id=\"import_users\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Earlier you created the dataset group and dataset to house your information, now you will execute an import job that will load the user data from the S3 bucket into the Amazon Personalize dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_dataset_import_jobs = personalize.list_dataset_import_jobs(\n",
    "    datasetArn=workshop_users_dataset_arn,\n",
    "    maxResults=100\n",
    ")['datasetImportJobs']\n",
    "\n",
    "\n",
    "if users_import_job_name in [x['jobName'] for x in users_dataset_import_jobs]:\n",
    "    \n",
    "    workshop_users_dataset_import_job_arn =  'arn:aws:personalize:'+region+':'+account_id+':dataset-import-job/'+users_import_job_name\n",
    "    print('The Users Import Job {} already exists.'.format(workshop_users_dataset_import_job_arn))\n",
    "    print ('\\nWe will be using the existing Users Import Job with workshop_users_dataset_import_job_arn = {}'.format(workshop_users_dataset_import_job_arn))\n",
    "        \n",
    "else:\n",
    "    create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "        jobName = users_import_job_name,\n",
    "        datasetArn = workshop_users_dataset_arn,\n",
    "        dataSource = {\n",
    "            \"dataLocation\": \"s3://{}/{}\".format(bucket_name, users_filename)\n",
    "        },\n",
    "        roleArn = role_arn\n",
    "    )\n",
    "\n",
    "    workshop_users_dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "    print(json.dumps(create_dataset_import_job_response, indent=2))\n",
    "    \n",
    "    print ('\\nImporting the Users Data with workshop_users_dataset_import_job_arn = {}'.format(workshop_users_dataset_import_job_arn))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use the dataset, the import job must be active. Execute the cell below and wait for it to show the ACTIVE status. It checks the status of the import job every minute, up to a maximum of 6 hours.\n",
    "\n",
    "Importing the data can take some time, depending on the size of the dataset. In this workshop, the data import job should take around 15 minutes. While you're waiting you can learn more about Datasets and Schemas in [the documentation](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html). We need to wait for the data imports to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_time = time.time() + 6*60*60 # 6 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = workshop_interactions_dataset_import_job_arn\n",
    "    )\n",
    "    status = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    print(\"Interactions DatasetImportJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(30)\n",
    "    \n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = workshop_items_dataset_import_job_arn\n",
    "    )\n",
    "    status = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    print(\"Items DatasetImportJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(30)\n",
    "    \n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = workshop_users_dataset_import_job_arn\n",
    "    )\n",
    "    status = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    print(\"Users DatasetImportJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ready... Set... Train! :\n",
    "\n",
    "Now that the data is imported and ready for use, we will create Video on Demand Use Case Optimized Recommenders for the following use cases:\n",
    "\n",
    "1. [More like X](https://docs.aws.amazon.com/personalize/latest/dg/VIDEO_ON_DEMAND-use-cases.html#more-like-y-use-case): recommendations for movies that are similar to a movie that you specify. With this use case, Amazon Personalize automatically filters movies the user watched based on the userId that you specify and Watch events.\n",
    "\n",
    "1. [Top picks for you](https://docs.aws.amazon.com/personalize/latest/dg/VIDEO_ON_DEMAND-use-cases.html#top-picks-use-case): personalized content recommendations for a user that you specify. With this use case, Amazon Personalize automatically filters videos the user watched based on the userId that you specify and Watch events.\n",
    "\n",
    "We will also create a custom solution and solution versions for the following use case:\n",
    "\n",
    "3. [Personalized-Ranking](https://docs.aws.amazon.com/personalize/latest/dg/working-with-predefined-recipes.html): will be used to rerank a list of movies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Use Case Optimized Recommenders <a class=\"anchor\" id=\"recommenders\"></a>\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with pre-configured VIDEO_ON_DEMAND Recommenders that match some of our core use cases. Each domain has different use cases. When you create a recommender you create it for a specific use case, and each use case has different requirements for getting recommendations.\n",
    "\n",
    "Let us look at the recommenders supported for this domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_recipes = personalize.list_recipes(domain='VIDEO_ON_DEMAND')\n",
    "display_available_recipes = available_recipes ['recipes']\n",
    "available_recipes = personalize.list_recipes(domain='VIDEO_ON_DEMAND',nextToken=available_recipes['nextToken'])#paging to get the rest of the recipes \n",
    "display_available_recipes = display_available_recipes + available_recipes['recipes']\n",
    "display(display_available_recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[More use cases per domain](https://docs.aws.amazon.com/personalize/latest/dg/domain-use-cases.html).\n",
    "\n",
    "### Create a \"More like X\" recommender\n",
    "\n",
    "We are going to create a recommender of the type \"More like X\". This type of recommender offers recommendations for videos that are similar to a video a user watched. With this use case, Amazon Personalize automatically filters videos the user watched based on the userId specified in the `get_recommendations` call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    create_recommender_response = personalize.create_recommender(\n",
    "      name = recommender_more_like_x_name,\n",
    "      recipeArn = 'arn:aws:personalize:::recipe/aws-vod-more-like-x',\n",
    "      datasetGroupArn = workshop_dataset_group_arn\n",
    "    )\n",
    "    workshop_recommender_more_like_x_arn = create_recommender_response[\"recommenderArn\"]\n",
    "    \n",
    "    print (json.dumps(create_recommender_response))\n",
    "    print ('\\nCreating the More Like X recommender with workshop_recommender_more_like_x_arn = {}'.format(workshop_recommender_more_like_x_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException as e:\n",
    "    workshop_recommender_more_like_x_arn =  'arn:aws:personalize:'+region+':'+account_id+':recommender/'+recommender_more_like_x_name\n",
    "    print('The More Like X recommender {} already exists.'.format(workshop_recommender_more_like_x_arn))\n",
    "    print ('\\nWe will be using the existing More Like X recommender with workshop_recommender_more_like_x_arn = {}'.format(workshop_recommender_more_like_x_arn))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Create a \"Top picks for you\" Recommender\n",
    "\n",
    "We are going to create a second recommender of the type \"Top picks for you\". This type of recommender offers personalized streaming content recommendations for a user that you specify. With this use case, Amazon Personalize automatically filters videos the user watched based on the userId that you specify and Watch events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    create_recommender_response = personalize.create_recommender(\n",
    "      name = recommender_top_picks_for_you_name,\n",
    "      recipeArn = 'arn:aws:personalize:::recipe/aws-vod-top-picks',\n",
    "      datasetGroupArn = workshop_dataset_group_arn\n",
    "    )\n",
    "    workshop_recommender_top_picks_arn = create_recommender_response[\"recommenderArn\"]\n",
    "    \n",
    "    print (json.dumps(create_recommender_response))\n",
    "    print ('\\nCreating the Top Picks For You recommender with workshop_recommender_top_picks_arn = {}'.format(workshop_recommender_top_picks_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException as e:\n",
    "    workshop_recommender_top_picks_arn =  'arn:aws:personalize:'+region+':'+account_id+':recommender/'+recommender_top_picks_for_you_name\n",
    "    print('The Top Picks For You recommender {} already exists.'.format(workshop_recommender_top_picks_arn))\n",
    "    print ('\\nWe will be using the existing Top Picks For You recommender with workshop_recommender_top_picks_arn = {}'.format(workshop_recommender_top_picks_arn))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arn:aws:personalize:us-east-1:674465894274:recommender/workshop_top_picks_for_you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Solutions <a class=\"anchor\" id=\"solutions\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Some use cases require a custom implementation. \n",
    "\n",
    "In Amazon Personalize, a specific variation of an algorithm is called a recipe. Different recipes are suitable for different situations. A trained model is called a solution, and each solution can have many versions that relate to a given volume of data when the model was trained.\n",
    "\n",
    "Let's look at all available recipes that are not of a specific domain and can be used to create custom solutions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_recipes = personalize.list_recipes()\n",
    "display_available_recipes = available_recipes ['recipes']\n",
    "available_recipes = personalize.list_recipes(nextToken=available_recipes['nextToken'])#paging to get the rest of the recipes \n",
    "display_available_recipes = display_available_recipes + available_recipes['recipes']\n",
    "\n",
    "display ([recipe  for recipe in display_available_recipes if 'domain' not in recipe])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to rank a list of items for a specific user. This is useful if you have a collection of ordered items, such as search results, promotions, or curated lists, and you want to provide a personalized re-ranking for each of your users. To implement this use case, we will create a custom solution using the recipe.\n",
    "\n",
    "The Personalized-Ranking recipe provides recommendations in ranked order based on predicted interest level. This recipe generates personalized rankings of items. A personalized ranking is a list of recommended items that are re-ranked for a specific user. This is useful if you have a collection of ordered items, such as search results, curated lists or anything you cannot easily categorize and you want to provide a personalized re-ranking for each of your users.\n",
    "\n",
    "These custom solution will use the same datasets that we already implemented so all we need to do is create a solution and solution version for this recipe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personalized Ranking\n",
    "\n",
    "Personalized Ranking is an interesting application of HRNN. Instead of just recommending what a user would be most interested on, this algorithm takes in a list of items as well as a user. The items are then returned in the order of most probable relevance for the user. \n",
    "\n",
    "You can use this for ranking unique categories that you do not have item metadata to create a filter for, or when you have a particular collection of items that you would like better ordered for a particular user.\n",
    "\n",
    "For our use case, using the MovieLens data, we could imagine that a Video on Demand application may want to create a shelf of comic book movies, or seasonal movies. We can generate these lists based on metadata we have. We would use personalized ranking to re-order the list of movies for each user.\n",
    "\n",
    "We start by selecting the recipe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workshop_rerank_recipe_arn = \"arn:aws:personalize:::recipe/aws-personalized-ranking\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the solution\n",
    "\n",
    "First you create a solution using the recipe. Although you provide the dataset ARN in this step, the model is not yet trained. See this as an identifier instead of a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    rerank_create_solution_response = personalize.create_solution(\n",
    "        name = workshop_rerank_solution_name,\n",
    "        datasetGroupArn = workshop_dataset_group_arn,\n",
    "        recipeArn = workshop_rerank_recipe_arn\n",
    "    )\n",
    "\n",
    "    workshop_rerank_solution_arn = rerank_create_solution_response['solutionArn']\n",
    "    print(json.dumps(rerank_create_solution_response, indent=2))\n",
    "\n",
    "    print ('\\nCreating the Personalize Ranking Solution with workshop_rerank_solution_arn = {}'.format(workshop_rerank_solution_arn))\n",
    " \n",
    "except personalize.exceptions.ResourceAlreadyExistsException as e:\n",
    "    workshop_rerank_solution_arn =  'arn:aws:personalize:'+region+':'+account_id+':solution/'+workshop_rerank_solution_name\n",
    "    print('The Personalize Ranking Solution {} already exists.'.format(workshop_rerank_solution_arn))\n",
    "    print ('\\nWe will be using the existing Personalize Ranking Solution with workshop_rerank_solution_arn = {}'.format(workshop_rerank_solution_arn))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the solution version\n",
    "\n",
    "Once you have a solution, you need to create a version in order to complete the model training. The training can take a while to complete, upwards of 25 minutes, and an average of 35 minutes for this recipe with our dataset. Normally, we would use a while loop to poll until the task is completed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workshop_rerank_solution_version_arn = None\n",
    "\n",
    "solution_versions_list = personalize.list_solution_versions(\n",
    "    solutionArn=workshop_rerank_solution_arn,\n",
    "    maxResults=10\n",
    ")['solutionVersions']\n",
    "\n",
    "for solution_vers in solution_versions_list:\n",
    "    if solution_vers['status'] in ['CREATE PENDING', 'CREATE IN_PROGRESS', 'ACTIVE']:\n",
    "        workshop_rerank_solution_version_arn = solution_vers['solutionVersionArn']\n",
    "    if workshop_rerank_solution_version_arn:\n",
    "        break\n",
    "\n",
    "if workshop_rerank_solution_version_arn:\n",
    "    print ('\\nWe will be using the existing Personalize Ranking Solution Version with workshop_rerank_solution_version_arn = {}'.format(workshop_rerank_solution_version_arn))\n",
    "else:\n",
    "    rerank_create_solution_version_response = personalize.create_solution_version(\n",
    "        solutionArn = workshop_rerank_solution_arn\n",
    "    )\n",
    "    workshop_rerank_solution_version_arn = rerank_create_solution_version_response['solutionVersionArn']\n",
    "    print(json.dumps(rerank_create_solution_version_response, indent=2))\n",
    "    \n",
    "    print ('\\nTraining the Personalize Ranking Solution Version with workshop_rerank_solution_version_arn = {}'.format(workshop_rerank_solution_version_arn))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View solution and recommender creation status\n",
    "\n",
    "To view the status updates in the console:\n",
    "\n",
    "* In another browser tab you should already have the AWS Console up from opening this notebook instance. \n",
    "* Switch to that tab and search at the top for the service `Personalize`, then go to that service page. \n",
    "* Click `Dataset groups`.\n",
    "* Click the name of your dataset group, if you did not change it, it is \"personalize-poc-movielens\".\n",
    "* Click `Recommenders`.\n",
    "* You will see a list of the two recommenders you created above, including a column with the status of the recommender. Once it is `Active`, your recommender is ready.\n",
    "* Click on `Custom Resources`. This oppens up the list of custom resources that youhave created.\n",
    "* Click on `Solutions and Recipes` to see your re-ranking solutions. If you click on `personalize-poc-rerank` you can see the status of the solution versions. Once it is `Active`, your solution is ready to be reviewed. It is also capable of being deployed.\n",
    "\n",
    "Or simply run the cell below to keep track of the recommenders and solution version creation status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = time.time() + 10*60*60 # 10 hours\n",
    "while time.time() < max_time:\n",
    "\n",
    "    # Recommender more_like_x\n",
    "    version_response = personalize.describe_recommender(\n",
    "        recommenderArn = workshop_recommender_more_like_x_arn\n",
    "    )\n",
    "    status_more_like_x = version_response[\"recommender\"][\"status\"]\n",
    "\n",
    "    if status_more_like_x == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(workshop_recommender_more_like_x_arn))\n",
    "        \n",
    "    elif status_more_like_x == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(workshop_recommender_more_like_x_arn))\n",
    "        break\n",
    "\n",
    "    if not status_more_like_x == \"ACTIVE\":\n",
    "        print(\"The recommender more_like_x build is still in progress\")\n",
    "    else:\n",
    "        print(\"The recommender more_like_x is ACTIVE\")\n",
    "\n",
    "    # Recommender top_picks_for_you\n",
    "    version_response = personalize.describe_recommender(\n",
    "        recommenderArn = workshop_recommender_top_picks_arn\n",
    "    )\n",
    "    status_top_picks = version_response[\"recommender\"][\"status\"]\n",
    "\n",
    "    if status_top_picks == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(workshop_recommender_top_picks_arn))\n",
    "    elif status_top_picks == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(workshop_recommender_top_picks_arn))\n",
    "        break\n",
    "\n",
    "    if not status_top_picks == \"ACTIVE\":\n",
    "        print(\"The Top Picks for You recommender build is still in progress\")\n",
    "    else:\n",
    "        print(\"The Top Picks for You recommender is ACTIVE\")\n",
    "        \n",
    "    # Reranking Solution \n",
    "    version_response = personalize.describe_solution_version(\n",
    "        solutionVersionArn = workshop_rerank_solution_version_arn\n",
    "    )\n",
    "    status_rerank_solution = version_response[\"solutionVersion\"][\"status\"]\n",
    "\n",
    "    if status_rerank_solution == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(workshop_rerank_solution_version_arn))\n",
    "        \n",
    "    elif status_rerank_solution == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(workshop_rerank_solution_version_arn))\n",
    "        break\n",
    "\n",
    "    if not status_rerank_solution == \"ACTIVE\":\n",
    "        print(\"Rerank solution version build is still in progress\")\n",
    "    else:\n",
    "        print(\"The rerank solution is ACTIVE\")\n",
    "        \n",
    "    if status_more_like_x == \"ACTIVE\" and status_top_picks == 'ACTIVE' and status_rerank_solution == \"ACTIVE\":\n",
    "        break\n",
    "\n",
    "    print()\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a Campaign <a class=\"anchor\" id=\"deploy\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Once a solution version is created, it is possible to get recommendations from them, and to get a feel for their overall behavior.\n",
    "\n",
    "For real-time recommendations, after you prepare and import data and creating a solution, you are ready to deploy your solution version to generate recommendations. You deploy a solution version by creating an Amazon Personalize campaign. If you are getting batch recommendations, you don't need to create a campaign. For more information see [Getting batch recommendations and user segments](https://docs.aws.amazon.com/personalize/latest/dg/recommendations-batch.html).\n",
    "\n",
    "We will deploy a campaign for the solution version. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a campaign \n",
    "\n",
    "A campaign is a hosted solution version; an endpoint which you can query for recommendations. Pricing is set by estimating throughput capacity (requests from users for personalization per second). When deploying a campaign, you set a minimum throughput per second (TPS) value. This service, like many within AWS, will automatically scale based on demand, but if latency is critical, you may want to provision ahead for larger demand. For this POC and demo, all minimum throughput thresholds are set to 1. For more information, see the [pricing page](https://aws.amazon.com/personalize/pricing/).\n",
    "\n",
    "Once we're satisfied with our solution version, we need to create Campaigns for each solution version. When creating a campaign you specify the minimum transactions per second (`minProvisionedTPS`) that you expect to make against the service for this campaign. Personalize will automatically scale the inference endpoint up and down for the campaign to match demand but will never scale below `minProvisionedTPS`.\n",
    "\n",
    "Let's create a campaigns for our solution versions set at `minProvisionedTPS` of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    rerank_create_campaign_response = personalize.create_campaign(\n",
    "        name = workshop_rerank_campaign_name,\n",
    "        solutionVersionArn = workshop_rerank_solution_version_arn,\n",
    "        minProvisionedTPS = 1\n",
    "    )\n",
    "\n",
    "    workshop_rerank_campaign_arn = rerank_create_campaign_response['campaignArn']\n",
    "    print(json.dumps(rerank_create_campaign_response, indent=2))\n",
    "\n",
    "    print ('\\nCreating the personalize ranking campaign with workshop_rerank_campaign_arn = {}'.format(workshop_rerank_campaign_arn))\n",
    "\n",
    "except personalize.exceptions.ResourceAlreadyExistsException as e:\n",
    "    workshop_rerank_campaign_arn =  'arn:aws:personalize:'+region+':'+account_id+':campaign/'+workshop_rerank_campaign_name\n",
    "    print('The personalize ranking campaign {} already exists.'.format(workshop_rerank_campaign_arn))\n",
    "    print ('\\nWe will be using the existing personalize ranking campaign with workshop_rerank_campaign_arn = {}'.format(workshop_rerank_campaign_arn))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View campaign creation status\n",
    "\n",
    "This is how you view the status updates in the console:\n",
    "\n",
    "* In another browser tab you should already have the AWS Console open from opening this notebook instance. \n",
    "* Switch to that tab and search at the top for the service `Personalize`, then go to that service page. \n",
    "* Click `Dataset groups`.\n",
    "* Click the name of your dataset group.\n",
    "* Click `Recommenders`\n",
    "* Click `Custom Resources`\n",
    "* Click `Campaigns`.\n",
    "* You will now see a list of all of the campaigns you created above, including a column with the status of the campaign. Once it is `Active`, your campaign is ready to be queried.\n",
    "\n",
    "Or simply run the cell below to keep track of the campaign creation status of the campaign we created.\n",
    "\n",
    "While you are waiting for this to complete you can learn more about campaigns in [the documentation](https://docs.aws.amazon.com/personalize/latest/dg/campaigns.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "\n",
    "    version_response = personalize.describe_campaign(\n",
    "        campaignArn = workshop_rerank_campaign_arn\n",
    "    )\n",
    "    status = version_response['campaign']['status']\n",
    "\n",
    "    if status == 'ACTIVE':\n",
    "        print('Build succeeded for {}'.format(workshop_rerank_campaign_arn))\n",
    "    elif status == \"CREATE FAILED\":\n",
    "        print('Build failed for {}'.format(workshop_rerank_campaign_arn))\n",
    "        in_progress_campaigns.remove(workshop_rerank_campaign_arn)\n",
    "    \n",
    "    if status == 'ACTIVE' or status == 'CREATE FAILED':\n",
    "        break\n",
    "    else:\n",
    "        print('The campaign build is still in progress')\n",
    "        \n",
    "    time.sleep(60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate solution versions and recommenders\n",
    "\n",
    "Personalize calculates these metrics based on a subset of the training data. The image below illustrates how Personalize splits the data. Given 10 users, with 10 interactions each (a circle represents an interaction), the interactions are ordered from oldest to newest based on the timestamp. Personalize uses all of the interaction data from 90% of the users (blue circles) to train the solution version, and the remaining 10% for evaluation. For each of the users in the remaining 10%, 90% of their interaction data (green circles) is used as input for the call to the trained model. The remaining 10% of their data (orange circle) is compared to the output produced by the model and used to calculate the evaluation metrics.\n",
    "\n",
    "![personalize metrics](../../static/imgs/personalize_metrics.png)\n",
    "\n",
    "We recommend reading [the documentation](https://docs.aws.amazon.com/personalize/latest/dg/working-with-training-metrics.html) to understand the metrics, but we have also copied parts of the documentation below for convenience.\n",
    "\n",
    "You need to understand the following terms regarding evaluation in Personalize:\n",
    "\n",
    "* *Relevant recommendation* refers to a recommendation that matches a value in the testing data for the particular user.\n",
    "* *Rank* refers to the position of a recommended item in the list of recommendations. Position 1 (the top of the list) is presumed to be the most relevant to the user.\n",
    "* *Query* refers to the internal equivalent of a GetRecommendations call.\n",
    "\n",
    "The metrics produced by Personalize are:\n",
    "* **coverage**: The proportion of unique recommended items from all queries out of the total number of unique items in the training data (includes both the Items and Interactions datasets).\n",
    "* **mean_reciprocal_rank_at_25**: The [mean of the reciprocal ranks](https://en.wikipedia.org/wiki/Mean_reciprocal_rank) of the first relevant recommendation out of the top 25 recommendations over all queries. This metric is appropriate if you're interested in the single highest ranked recommendation.\n",
    "* **normalized_discounted_cumulative_gain_at_K**: Discounted gain assumes that recommendations lower on a list of recommendations are less relevant than higher recommendations. Therefore, each recommendation is discounted (given a lower weight) by a factor dependent on its position. To produce the [cumulative discounted gain](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) (DCG) at K, each relevant discounted recommendation in the top K recommendations is summed together. The normalized discounted cumulative gain (NDCG) is the DCG divided by the ideal DCG such that NDCG is between 0 - 1. (The ideal DCG is where the top K recommendations are sorted by relevance.) Amazon Personalize uses a weighting factor of 1/log(1 + position), where the top of the list is position 1. This metric rewards relevant items that appear near the top of the list, because the top of a list usually draws more attention.\n",
    "* **precision_at_K**: The number of relevant recommendations out of the top K recommendations divided by K. This metric rewards precise recommendation of the relevant items.\n",
    "\n",
    "Let's take a look at the evaluation metrics for each of the solutions produced in this notebook. Please note that your results might differ from the results described in the text of this notebook, due to the quality of the Movielens dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"More like X\" recommender metrics\n",
    "\n",
    "Retrieve the evaluation metrics for the \"More like X\" recommender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workshop_recommender_more_like_x_metrics_response = personalize.describe_recommender(\n",
    "    recommenderArn = workshop_recommender_more_like_x_arn\n",
    ")\n",
    "\n",
    "for metric in workshop_recommender_more_like_x_metrics_response['recommender']['modelMetrics']:\n",
    "    print (\"{}: {}\".format(workshop_recommender_more_like_x_metrics_response['recommender']['modelMetrics']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **coverage**: The proportion of unique recommended items from all queries out of the total number of unique items in the training data (includes both the Items and Interactions datasets).\n",
    "* **mean_reciprocal_rank_at_25**: The [mean of the reciprocal ranks](https://en.wikipedia.org/wiki/Mean_reciprocal_rank) of the first relevant recommendation out of the top 25 recommendations over all queries. This metric is appropriate if you're interested in the single highest ranked recommendation.\n",
    "* **normalized_discounted_cumulative_gain_at_K**: Discounted gain assumes that recommendations lower on a list of recommendations are less relevant than higher recommendations. Therefore, each recommendation is discounted (given a lower weight) by a factor dependent on its position. To produce the [cumulative discounted gain](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) (DCG) at K, each relevant discounted recommendation in the top K recommendations is summed together. The normalized discounted cumulative gain (NDCG) is the DCG divided by the ideal DCG such that NDCG is between 0 - 1. (The ideal DCG is where the top K recommendations are sorted by relevance.) Amazon Personalize uses a weighting factor of 1/log(1 + position), where the top of the list is position 1. This metric rewards relevant items that appear near the top of the list, because the top of a list usually draws more attention.\n",
    "* **precision_at_K**: The number of relevant recommendations out of the top K recommendations divided by K. This metric rewards precise recommendation of the relevant items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Top Picks for You\" recommender metrics\n",
    "\n",
    "Retrieve the evaluation metrics for the \"Top Pics For you\" Recommender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workshop_recommender_top_picks_metrics_response = personalize.describe_recommender(\n",
    "    recommenderArn = workshop_recommender_top_picks_arn\n",
    ")\n",
    "\n",
    "for metric in workshop_recommender_top_picks_metrics_response['recommender']['modelMetrics']:\n",
    "    print (\"{}: {}\".format(workshop_recommender_top_picks_metrics_response['recommender']['modelMetrics']))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **coverage**: The proportion of unique recommended items from all queries out of the total number of unique items in the training data (includes both the Items and Interactions datasets).\n",
    "* **mean_reciprocal_rank_at_25**: The [mean of the reciprocal ranks](https://en.wikipedia.org/wiki/Mean_reciprocal_rank) of the first relevant recommendation out of the top 25 recommendations over all queries. This metric is appropriate if you're interested in the single highest ranked recommendation.\n",
    "* **normalized_discounted_cumulative_gain_at_K**: Discounted gain assumes that recommendations lower on a list of recommendations are less relevant than higher recommendations. Therefore, each recommendation is discounted (given a lower weight) by a factor dependent on its position. To produce the [cumulative discounted gain](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) (DCG) at K, each relevant discounted recommendation in the top K recommendations is summed together. The normalized discounted cumulative gain (NDCG) is the DCG divided by the ideal DCG such that NDCG is between 0 - 1. (The ideal DCG is where the top K recommendations are sorted by relevance.) Amazon Personalize uses a weighting factor of 1/log(1 + position), where the top of the list is position 1. This metric rewards relevant items that appear near the top of the list, because the top of a list usually draws more attention.\n",
    "* **precision_at_K**: The number of relevant recommendations out of the top K recommendations divided by K. This metric rewards precise recommendation of the relevant items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personalized ranking metrics\n",
    "\n",
    "Retrieve the evaluation metrics for the personalized ranking solution version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_solution_metrics_response = personalize.get_solution_metrics(\n",
    "    solutionVersionArn = workshop_rerank_solution_version_arn\n",
    ")\n",
    "\n",
    "for metric in rerank_solution_metrics_response[\"metrics\"]:\n",
    "    print (\"{}: {}\".format(metric,rerank_solution_metrics_response[\"metrics\"][metric] ))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **coverage**: The proportion of unique recommended items from all queries out of the total number of unique items in the training data (includes both the Items and Interactions datasets).\n",
    "* **mean_reciprocal_rank_at_25**: The [mean of the reciprocal ranks](https://en.wikipedia.org/wiki/Mean_reciprocal_rank) of the first relevant recommendation out of the top 25 recommendations over all queries. This metric is appropriate if you're interested in the single highest ranked recommendation.\n",
    "* **normalized_discounted_cumulative_gain_at_K**: Discounted gain assumes that recommendations lower on a list of recommendations are less relevant than higher recommendations. Therefore, each recommendation is discounted (given a lower weight) by a factor dependent on its position. To produce the [cumulative discounted gain](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) (DCG) at K, each relevant discounted recommendation in the top K recommendations is summed together. The normalized discounted cumulative gain (NDCG) is the DCG divided by the ideal DCG such that NDCG is between 0 - 1. (The ideal DCG is where the top K recommendations are sorted by relevance.) Amazon Personalize uses a weighting factor of 1/log(1 + position), where the top of the list is position 1. This metric rewards relevant items that appear near the top of the list, because the top of a list usually draws more attention.\n",
    "* **precision_at_K**: The number of relevant recommendations out of the top K recommendations divided by K. This metric rewards precise recommendation of the relevant items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Using evaluation metrics <a class=\"anchor\" id=\"use\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "It is important to use evaluation metrics carefully. There are a number of factors to keep in mind.\n",
    "\n",
    "* If there is an existing recommendation system in place, this will have influenced the user's interaction history which you use to train your new solutions. This means the evaluation metrics are biased to favor the existing solution. If you work to push the evaluation metrics to match or exceed the existing solution, you may just be pushing the User Personalization to behave like the existing solution and might not end up with something better.\n",
    "\n",
    "\n",
    "Keeping in mind these factors, the evaluation metrics produced by Personalize are generally useful for two cases:\n",
    "1. Comparing the performance of solution versions trained on the same recipe, but with different values for the hyperparameters and features (impression data etc)\n",
    "1. Comparing the performance of solution versions trained on different recipes. Here also keep in mind that the recipes answer different use cases and comparing them to each other might not make sense in your solution.\n",
    "\n",
    "Properly evaluating a recommendation system is always best done through A/B testing while measuring actual business outcomes. Since recommendations generated by a system usually influence the user behavior which it is based on, it is better to run small experiments and apply A/B testing for longer periods of time. Over time, the bias from the existing model will fade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing useful variables <a class=\"anchor\" id=\"vars\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Before exiting this notebook, run the following cells to save the version ARNs for use in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store workshop_dataset_group_arn\n",
    "%store workshop_interactions_dataset_arn\n",
    "%store workshop_items_dataset_arn\n",
    "%store workshop_users_dataset_arn\n",
    "%store workshop_interactions_schema_arn\n",
    "%store workshop_items_schema_arn\n",
    "%store workshop_users_schema_arn\n",
    "%store workshop_recommender_top_picks_arn\n",
    "%store workshop_recommender_more_like_x_arn\n",
    "%store workshop_rerank_campaign_arn\n",
    "%store workshop_rerank_solution_arn\n",
    "%store workshop_rerank_solution_version_arn\n",
    "%store workshop_rerank_campaign_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're all set to move on to [the exploratory notebook `03_Inference_Layer.ipynb`](03_Inference_Layer.ipynb). Open it from the browser and you can start interacting with the Recommenders and Campaign and getting recommendations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
