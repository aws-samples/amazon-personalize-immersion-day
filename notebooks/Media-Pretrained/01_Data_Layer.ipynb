{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Validating Data <a class=\"anchor\" id=\"top\"></a>\n",
        "\n",
        "In this notebook, you will choose a dataset and prepare it for use with Amazon Personalize.\n",
        "\n",
        "1. [How to Use the Notebook](#usenotebook)\n",
        "1. [Define your Use Case](#usecase)\n",
        "1. [Choose a Dataset or Data Source](#source)\n",
        "1. [Prepare the Item Metadata](#prepare_items)\n",
        "1. [Prepare the Interactions Data](#prepare_interactions)\n",
        "1. [Prepare the User Metadata](#prepare_users)\n",
        "\n",
        "\n",
        "## How to Use the Notebook <a class=\"anchor\" id=\"usenotebook\"></a>\n",
        "\n",
        "The code is broken up into cells like the one below. There's a triangular Run button at the top of this page that you can click to execute each cell and move onto the next, or you can press `Shift` + `Enter` while in the cell to execute it and move onto the next one.\n",
        "\n",
        "As a cell is executing you'll notice a line to the side showcase an `*` while the cell is running or it will update to a number to indicate the last cell that completed executing after it has finished exectuting all the code within a cell.\n",
        "\n",
        "Simply follow the instructions below and execute the cells to get started with Amazon Personalize using case optimized recommenders.\n",
        "\n",
        "\n",
        "## Define your Use Case <a class=\"anchor\" id=\"usecase\"></a>\n",
        "[Back to top](#top)\n",
        "\n",
        "There are a few guidelines for scoping a problem suitable for Personalize. We recommend the values below as a starting point, although the [official limits](https://docs.aws.amazon.com/personalize/latest/dg/limits.html) lie a little lower.\n",
        "\n",
        "* Authenticated users\n",
        "* At least 50 unique users\n",
        "* At least 100 unique items\n",
        "* At least 2 dozen interactions for each user \n",
        "\n",
        "Most of the time this is easily attainable, and if you are low in one category, you can often make up for it by having a larger number in another category.\n",
        "\n",
        "The user-item-iteraction data is key for getting started with the service. This means we need to look for use cases that generate that kind of data, a few common examples are:\n",
        "\n",
        "1. Video-on-demand applications\n",
        "1. E-commerce platforms\n",
        "\n",
        "Defining your use-case will inform what data and what type of data you need.\n",
        "\n",
        "In this example we are going to be creating:\n",
        "\n",
        "1. Amazon Personalize VIDEO_ON_DEMAND Domain recommender for the [\"More Like X\"](https://docs.aws.amazon.com/personalize/latest/dg/VIDEO_ON_DEMAND-use-cases.html#more-like-y-use-case) use case.\n",
        "1. Amazon Personalize VIDEO_ON_DEMAND Domain recommender for the [\"Top picks for you\"](https://docs.aws.amazon.com/personalize/latest/dg/VIDEO_ON_DEMAND-use-cases.html#top-picks-use-case) use case.\n",
        "1. Amazon Personalize Custom Campaign for a personalized ranked list of movies, for instance shelf/rail/carousel based on some information (director, location, superhero franchise, etc...) \n",
        "\n",
        "All of these will be created within the same dataset group and with the same input data.\n",
        "\n",
        "In this notebook we will be importing interactions, user and item data into your environment, inspecting it and converting it to a format that will allow you use it in Amazon Personalize to train models to get personalized recommendations.\n",
        "\n",
        "\n",
        "## Choose a Dataset or Data Source <a class=\"anchor\" id=\"source\"></a>\n",
        "[Back to top](#top)\n",
        "\n",
        "Regardless of the use case, the algorithms all share a base of learning on user-item-interaction data which is defined by 3 core attributes:\n",
        "\n",
        "1. **UserID** - The user who interacted\n",
        "1. **ItemID** - The item the user interacted with\n",
        "1. **Timestamp** - The time at which the interaction occurred\n",
        "\n",
        "To begin, we are going to use the latest MovieLens dataset, this dataset has over 25 million interactions and a rich collection of metadata for items. There is also a smaller version of this dataset, which can be used to shorten training times, while still incorporating the same capabilities as the full dataset.\n",
        "\n",
        "Generally speaking your data will not arrive in a perfect form for Personalize, and will take some modification to be structured correctly. This notebook guides you through that process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will be using data from the [MovieLens project](https://grouplens.org/datasets/movielens/). Follow the link to learn more about the data and potential uses.\n",
        "\n",
        "First, you will download the dataset from the [MovieLens project](https://grouplens.org/datasets/movielens/) website and unzip it in a new folder using the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = \"poc_data\"\n",
        "!mkdir $data_dir\n",
        "\n",
        "!cd $data_dir && wget http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
        "!cd $data_dir && unzip ml-latest-small.zip\n",
        "dataset_dir = data_dir + \"/ml-latest-small/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Take a look at the data files you have downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!ls $dataset_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can look at the README.txt file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pygmentize $data_dir/ml-latest-small/README.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the Item Metadata <a class=\"anchor\" id=\"prepare_items\"></a>\n",
        "[Back to top](#top)\n",
        "\n",
        "Amazon Personalize uses 3 datasets, which usually come from different datasources. \n",
        "\n",
        "The item data consists of information about the content that is being interacted with, this generally comes from Content Management Systems (CMS). For the purpose of this workshop we will use the IMDB TT ID to provide a common identifier between the interactions data and the content metadata. Movielens provides its own identifier as well as a the IMDB TT ID (without the leading 'tt') in the 'links.csv' file. We will now convert the interaction data to utilize the IMDB TT id as its unique identifier.\n",
        "\n",
        "This will allow you to work with filters as well as supporting the [Top Picks for you Domain Recommender](https://docs.aws.amazon.com/personalize/latest/dg/VIDEO_ON_DEMAND-use-cases.html#top-picks-use-case), and complying with the [VIDEO_ON_DEMAND domain dataset and schema requirements](https://docs.aws.amazon.com/personalize/latest/dg/VIDEO-ON-DEMAND-datasets-and-schemas.html#VIDEO-ON-DEMAND-dataset-requirements)..\n",
        "\n",
        "Python ships with a broad collection of libraries and we need to import those as well as the ones installed to help us like [boto3](https://aws.amazon.com/sdk-for-python/) (AWS SDK for python) and [Pandas](https://pandas.pydata.org/)/[Numpy](https://numpy.org/)  which are core data science tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from time import sleep\n",
        "import json\n",
        "from datetime import datetime\n",
        "import boto3\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copy the IMDB item metadata that was added to this notebook instance during automated deployment of the workshop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir poc_data/imdb\n",
        "!cp ../../automation/ml_ops/poc_data/imdb/items.csv poc_data/imdb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, open the IMDB `items.csv` file and take a look at the first rows. This file has information about the movie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "original_data = pd.read_csv(data_dir + '/imdb/items.csv', sep=',', dtype={'PROMOTION': \"string\"})\n",
        "original_data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "original_data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This does not really tell us much about the dataset, so we will explore a bit more and look at the raw information. We can see that genres often appear in groups. That is fine for us as Personalize supports this structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "original_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "original_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looks good, we currently have no null values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's it! At this point the item data is ready to go, and we just need to save it as a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "items_filename = \"item-meta.csv\"\n",
        "original_data.to_csv((data_dir+\"/\"+items_filename), index=False, float_format='%.0f')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the Interactions data <a class=\"anchor\" id=\"prepare_interactions\"></a>\n",
        "[Back to top](#top)\n",
        "\n",
        "The next thing to be done is to load the data and confirm the data is in a good state.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, open the `ratings.csv` file and take a look at the first rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "original_data = pd.read_csv(dataset_dir + '/ratings.csv', sep=',', dtype={'userId': \"int64\", 'movieId': \"str\"})\n",
        "original_data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "original_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "original_data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This shows that we have a good range of values for `userId` and `movieId`. Next, it is always a good idea to confirm the data format. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "original_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "original_data.isnull().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "arb_time_stamp = original_data.iloc[50]['timestamp']\n",
        "print(arb_time_stamp)\n",
        "print(datetime.utcfromtimestamp(arb_time_stamp).strftime('%Y-%m-%d %H:%M:%S'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From this, you can see that there are a total of 964982681 entries in the dataset, with 4 columns, userId and timestamp are in int64 format, the rating is a float64 and the movieId is an object.\n",
        "\n",
        "To use Amazon Personalize, you need to save timestamps in [Unix Epoch](https://en.wikipedia.org/wiki/Unix_time) format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert the Interactions Data\n",
        "\n",
        "The interaction data generally is acquired from anaytics platforms that can identify individual interactions with content/items within a platform. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "links = pd.read_csv(dataset_dir + '/links.csv', sep=',', usecols=[0,1], encoding='latin-1', dtype={'movieId': \"str\", 'imdbId': \"str\", 'tmdbId': \"str\"})\n",
        "pd.set_option('display.max_rows', 25)\n",
        "links['imdbId'] = 'tt' + links['imdbId'].astype(object)\n",
        "links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "imdb_data = original_data.merge(links, on='movieId')\n",
        "imdb_data.drop(columns='movieId')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since this is a dataset of an explicit feedback movie ratings, it includes movies rated from 1 to 5. We want to include only moves that were \"liked\" by the users, and simulate a dataset of data that would be gathered by a VOD platform. In order to do that, we will filter out all interactions under 2 out of 5, and create two event types: \"Click\" and and \"Watch\". We will then assign all movies rated 2 and above as \"Click\" and movies rated 4 and above as both \"Click\" and \"Watch\". \n",
        "\n",
        "Note that for a real data set you would actually model based on implicit feedback such as clicks, watches and/or explicit feedback such as ratings, likes etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "watched_df = imdb_data.copy()\n",
        "watched_df = watched_df[watched_df['rating'] > 3]\n",
        "watched_df = watched_df[['userId', 'imdbId', 'timestamp']]\n",
        "watched_df['EVENT_TYPE']='Watch'\n",
        "watched_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clicked_df = imdb_data.copy()\n",
        "clicked_df = clicked_df[clicked_df['rating'] > 1]\n",
        "clicked_df = clicked_df[['userId', 'imdbId', 'timestamp']]\n",
        "clicked_df['EVENT_TYPE']='Click'\n",
        "clicked_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "interactions_df = clicked_df.copy()\n",
        "interactions_df = interactions_df.append(watched_df)\n",
        "interactions_df.sort_values(\"timestamp\", axis = 0, ascending = True, \n",
        "                 inplace = True, na_position ='last') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "interactions_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets look at what the new dataset looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "interactions_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Amazon Personalize has default column names for users, items, and timestamp. These default column names are `USER_ID`, `ITEM_ID`, `TIMESTAMP` and `EVENT_VALUE` for the [VIDEO_ON_DEMAND domain dataset](https://docs.aws.amazon.com/personalize/latest/dg/VIDEO-ON-DEMAND-datasets-and-schemas.html). The final modification to the dataset is to replace the existing column headers with the default headers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "interactions_df.rename(columns = {'userId':'USER_ID', 'imdbId':'ITEM_ID', \n",
        "                              'timestamp':'TIMESTAMP'}, inplace = True) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll be using a subset of the IMDB dataset for this workshop that has been cleaned to remove movies that don't have valid values for the metadata we are using in out ITEMs dataset (we'll work with this more in the net section), so we'll need to make sure we don't have any interactions that have IMDB movie ids that are not in our subset of the IMDB data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "movies = pd.read_csv(data_dir + '/imdb' + '/items.csv', sep=',', usecols=[0,1], encoding='latin-1', dtype={'movieId': \"str\", 'imdbId': \"str\", 'tmdbId': \"str\"})\n",
        "pd.set_option('display.max_rows', 25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, let's compare the number of ITEM_ID unique keys in the IMDB data to the ITEM_ID unique keys in the interactions.  They should be the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "movies.nunique(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "interactions_df.nunique(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The number of unique ITEM_IDs are not the same in the IMDB data and the interactions data, so we'll clean out the data points with ITEM_IDs that do not have item metadata from the interactions dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "interactions_df = interactions_df.merge(movies, on='ITEM_ID')\n",
        "interactions_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will also drop the `TITLE` column as it is not required in the interactions dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "interactions_df = interactions_df.drop(columns=['TITLE'])\n",
        "interactions_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's it! At this point the data is ready to go, and we just need to save it as a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "interactions_filename = \"interactions.csv\"\n",
        "interactions_df.to_csv((data_dir+\"/\"+interactions_filename), index=False, float_format='%.0f')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the User Metadata <a class=\"anchor\" id=\"prepare_users\"></a>\n",
        "[Back to top](#top)\n",
        "\n",
        "This will supporting the [Top Picks for you Domain Recommender](https://docs.aws.amazon.com/personalize/latest/dg/VIDEO_ON_DEMAND-use-cases.html#top-picks-use-case), and complying with the [VIDEO_ON_DEMAND domain dataset and schema requirements](https://docs.aws.amazon.com/personalize/latest/dg/VIDEO-ON-DEMAND-datasets-and-schemas.html#VIDEO-ON-DEMAND-dataset-requirements).\n",
        "\n",
        "The dataset does not have any user metadata so we will create a fake metadata field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get all unique user ids from the interaction dataset\n",
        "\n",
        "user_ids = interactions_df['USER_ID'].unique()\n",
        "user_data = pd.DataFrame()\n",
        "user_data[\"USER_ID\"]= user_ids\n",
        "user_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adding User Metadata\n",
        "\n",
        "The current dataset does not contain additiona user information. For this example, we'll randomly assign a membership level. For Ad Supported models this could indicate premium vs ad supported."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "possible_membership_levels = ['silver', 'gold']\n",
        "random = np.random.choice(possible_membership_levels, len(user_data.index), p=[0.5, 0.5])\n",
        "user_data[\"MEMBERLEVEL\"] = random\n",
        "user_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Saving the data as a CSV file\n",
        "users_filename = \"users.csv\"\n",
        "user_data.to_csv((data_dir+\"/\"+users_filename), index=False, float_format='%.0f')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use this data in the next labs. In order to use this data we will store these variables so subsequent notebooks can use this data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%store dataset_dir\n",
        "%store data_dir\n",
        "%store interactions_filename\n",
        "%store items_filename\n",
        "%store users_filename"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Go to the next notebook `02_Training_Layer_Recap.ipynb`](02_Training_Layer_Recap.ipynb) to continue."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "conda_python3",
      "language": "python",
      "name": "conda_python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
