{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Retail Demo Store <a class=\"anchor\" id=\"top\"></a>\n",
    "\n",
    "Congratulations! You have just been hired by the Retail Demo Store, which is a new online retail store that launched and is jumping into the crowded space of online sales. You have been hired into the Search & Discovery team, which leads efforts around personalization. Currently, most of your app does not provide a personalized experience, the products are presented in a static order for all users. In order to prevent customer churn, you are looking to add personalized experiences. \n",
    "\n",
    "You’ve been asked by the founders to:\n",
    "\n",
    "- Increase subscriber engagement by tailoring every experience to individual users\n",
    "- Help users discover newly released products\n",
    "- Increase the breadth of content offered to them from the Retail Demo Store catalog\n",
    "- Reduce the time to value by creating valuable recommendations in a short time\n",
    "\n",
    "Throughout the course of this workshop you will be exploring your datasets, building/training several recommendation models and implementing recommendations with API's.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> importing and training the datasets will take longer than we have in this workshop. In order to complete this workshop within the time set, we have already created several resources on your behalf.  However, the notebooks are designed in such a way that all the steps are included. If the resources have already been created, the cell will return information about the resources, if the resources have not been created, it will create them for you. \n",
    "</div>\n",
    "\n",
    "\n",
    "## In this notebook\n",
    "In this notebook, you will choose a dataset and prepare it for use with Amazon Personalize.\n",
    "\n",
    "1. [How to Use the Notebook](#usenotebook)\n",
    "1. [Introduction to Amazon Personalize Datasets](#datasets)\n",
    "1. [Choose a Dataset or Data Source](#source)\n",
    "1. [Configure an S3 bucket and an IAM role](#bucket_role)\n",
    "1. [Create dataset group](#group_dataset)\n",
    "1. [Create the Interactions Schema](#interact_schema)\n",
    "1. [Create the Items Schema](#items_schema)\n",
    "1. [Create the Users Schema](#users_schema)\n",
    "1. [Import the Interactions Data](#import_interactions)\n",
    "1. [Import the Items Metadata](#import_items)\n",
    "1. [Import the User Metadata](#import_users)\n",
    "1. [Storing Useful Variables](#vars)\n",
    "\n",
    "## How to Use the Notebook <a class=\"anchor\" id=\"usenotebook\"></a>\n",
    "\n",
    "### Executing cells\n",
    "\n",
    "The code is broken up into cells like the one below. There's a triangular **Run** button at the top of this page that you can click to execute each cell and move onto the next, or you can press `Shift` + `Enter` while in the cell to execute it and move onto the next one.\n",
    "\n",
    "As a cell is executing, you'll notice an `*` in the checkbox beside the cell. When the cell has finished running, the checkbox will contain a number to indicate the order the cell was executed in with respect to all the other cells in the notebook.\n",
    "\n",
    "Simply follow the instructions below and execute the cells to get started with Amazon Personalize.\n",
    "\n",
    "### Understanding the code\n",
    "\n",
    "This notebook can be used in two modalities:\n",
    "\n",
    "1. Train as you go by executing each cell. Some cells may take a long time to finish executing as they wait for resources to be created.\n",
    "2. Use this notebook with previously created resources. All or the majority of the resources will already be created, and cells will just retrieve the information of these existing resources to use them in following steps.\n",
    "\n",
    "Because of this, you will find that some cells have `try` and `except` blocks. In particular, most of them are handling a `ResourceAlreadyExistsException` exception. \n",
    "\n",
    "You can look at the code in the `try` block to get a good idea of how you can create a resource and understand how to use the Amazon Personalize SDK. The `except` block will let you know that the resource has been created and record the corresponding ARN, which is the Amazon unique identifier.\n",
    "\n",
    "This is an example of the `try` block for creating a dataset group, this code will execute without exceptions if the dataset group does not exist and raise an exception if the dataset group does already exist:\n",
    "\n",
    "```python\n",
    "try:     \n",
    "    # Try to create the dataset group, this block with exectute fully if the dataset group does not exist yet\n",
    "    \n",
    "    create_dataset_group_response = personalize.create_dataset_group(\n",
    "        name = workshop_dataset_group_name,\n",
    "        domain='ECOMMERCE'\n",
    "    )\n",
    "    workshop_dataset_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "    print(json.dumps(create_dataset_group_response, indent=2))\n",
    "    print ('\\nCreating the Dataset Group with dataset_group_arn = {}'.format(workshop_dataset_group_arn))\n",
    "```\n",
    "and this is the corresponding `except` block that will be executed if an exception is raised because the dataset group already exists. This block saves the ARN for the existing dataset group to use later and lets you know the resource already exists.\n",
    "\n",
    "```python\n",
    "except personalize.exceptions.ResourceAlreadyExistsException as e:\n",
    "    workshop_dataset_group_arn = 'arn:aws:personalize:'+region+':'+account_id+':dataset group/' + \n",
    "        workshop_dataset_group_name \n",
    "    print ('\\nThe the Dataset Group with dataset_group_arn = {} already exists'.format(\n",
    "        workshop_dataset_group_arn))\n",
    "    print ('\\nWe will be using the existing Dataset Group dataset_group_arn = {}'.format(\n",
    "        workshop_dataset_group_arn))\n",
    "```\n",
    "\n",
    "Depending on the resource, you may also find that sometimes the code will check from a list of resources to find if a resource exists and then use `if` and `else` blocks to either use the existing resource or create it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's build!\n",
    "\n",
    "Python ships with a broad collection of libraries and we need to import those as well as the ones installed to help us like [boto3](https://aws.amazon.com/sdk-for-python/) (AWS SDK for python) and [Pandas](https://pandas.pydata.org/)/[Numpy](https://numpy.org/)  which are core data science tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the latest version of botocore to ensure we have the latest features in the SDK\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install --upgrade --no-deps --force-reinstall botocore\n",
    "import os.path\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from time import sleep\n",
    "import csv\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "data_dir = \"poc_data\"\n",
    "!mkdir $data_dir\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the SDK to Personalize:\n",
    "personalize = boto3.client('personalize')\n",
    "personalize_runtime = boto3.client('personalize-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the account id and region to use later\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "print(\"account id:\", account_id)\n",
    "\n",
    "with open('/opt/ml/metadata/resource-metadata.json') as notebook_info:\n",
    "    data = json.load(notebook_info)\n",
    "    resource_arn = data['ResourceArn']\n",
    "    region = resource_arn.split(':')[3]\n",
    "print(\"region:\", region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is a workshop and the resources were created for you, we will retrieve the variables of the resources created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON files\n",
    "f = open('../../automation/ml_ops/domain/Retail-Pretrained/params.json')\n",
    "parameters = json.load(f)\n",
    "\n",
    "f = open('../../automation/ml_ops/domain/Retail-Pretrained/params_data.json')\n",
    "parameters_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workshop_dataset_group_name = parameters['datasetGroup']['serviceConfig']['name']\n",
    "\n",
    "interactions_schema_name = parameters['datasets']['interactions']['schema']['serviceConfig']['name']\n",
    "interactions_dataset_name = parameters['datasets']['interactions']['dataset']['serviceConfig']['name']\n",
    "\n",
    "items_schema_name = parameters['datasets']['items']['schema']['serviceConfig']['name']\n",
    "items_dataset_name = parameters['datasets']['items']['dataset']['serviceConfig']['name']\n",
    "\n",
    "users_schema_name = parameters['datasets']['users']['schema']['serviceConfig']['name']\n",
    "users_dataset_name = parameters['datasets']['users']['dataset']['serviceConfig']['name']\n",
    "\n",
    "#The following job names are the starting Strings of the job names that can be created\n",
    "interactions_import_job_name = 'dataset_import_interaction'\n",
    "items_import_job_name = 'dataset_import_item'\n",
    "users_import_job_name = 'dataset_import_user'\n",
    "\n",
    "for recommender in parameters['recommenders']:\n",
    "    # This is currently configured assuming only one recommender of each type, if there are multiple \n",
    "    # recommenders of the same type further configuration is needed.\n",
    "    if (recommender['serviceConfig']['recipeArn'] == 'arn:aws:personalize:::recipe/aws-ecomm-customers-who-viewed-x-also-viewed'):\n",
    "        recommender_customers_who_viewed_name =recommender['serviceConfig']['name'] \n",
    "    if (recommender['serviceConfig']['recipeArn'] == 'arn:aws:personalize:::recipe/aws-ecomm-recommended-for-you'):\n",
    "        recommender_recommended_for_you_name =recommender['serviceConfig']['name']\n",
    "        \n",
    "for solution in parameters['solutions']:\n",
    "    # This is currently configured assuming only one solution of this type, if there are multiple \n",
    "    # solutions of the same type further configuration is needed.\n",
    "    if (solution['serviceConfig']['recipeArn'] == 'arn:aws:personalize:::recipe/aws-personalized-ranking'):\n",
    "        workshop_rerank_solution_name = solution['serviceConfig']['name'] \n",
    "        # This is currently configured assuming only one campaign, if there are multiple campaigns \n",
    "        # further configuration is needed.\n",
    "        workshop_rerank_campaign_name = solution['campaigns'][0]['serviceConfig']['name'] \n",
    "          \n",
    "raw_interactions_file_path = parameters_data['data_files'][\"interactions_file_path\"]\n",
    "interactions_file_name = \"interactions.csv\"\n",
    "\n",
    "raw_users_file_path = parameters_data['data_files'][\"users_file_path\"]\n",
    "users_file_name = 'users.csv'\n",
    "\n",
    "raw_items_file_path = parameters_data['data_files'][\"items_file_path\"]\n",
    "items_file_name = 'items.csv'\n",
    "\n",
    "\n",
    "print (\"Copying User metadata\")\n",
    "user_metadata_file_path = parameters_data['data_files'][\"user_metadata_file_path\"]\n",
    "!aws s3 cp $user_metadata_file_path ./poc_data\n",
    "# this will delete the zipped file. -k is not supported with the current version of gzip on this instance.\n",
    "\n",
    "user_metadata_file_name = './poc_data/users.json'\n",
    "if (os.path.isfile(user_metadata_file_name)):\n",
    "    !rm $user_metadata_file_name  \n",
    "    \n",
    "zipped_user_user_metadata_file_name = user_metadata_file_name+'.gz'\n",
    "    \n",
    "!gzip -d $zipped_user_user_metadata_file_name\n",
    "\n",
    "print (\"Copying Item metadata\")\n",
    "item_metadata_file_path = parameters_data['data_files'][\"item_metadata_file_path\"]\n",
    "!aws s3 cp $item_metadata_file_path ./poc_data\n",
    "item_metadata_file_name = './poc_data/products.yaml'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will make sure we can use the SDK to interact with Amazon Personalize by describing some of the pre-created resources used in the workshop. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> If you have not pre-deployed resources and are building them as you go with this notebook, the below cell will raise an exception. You can continue with the notebook and create resources and train models as you go.\n",
    "</div>\n",
    "\n",
    "If you have not pre-deployed resources and are building them as you go with this notebook, the below cell will raise an exception. You can continue with the notebook and create resources and train models as you go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Describe a few resources using the SDK\n",
    "    recommender_customers_who_viewed_arn = 'arn:aws:personalize:'+region+':'+account_id+':recommender/'+recommender_customers_who_viewed_name \n",
    "    describe_response = personalize.describe_recommender(recommenderArn = recommender_customers_who_viewed_arn)\n",
    "\n",
    "    recommender_recommended_for_you_arn = 'arn:aws:personalize:'+region+':'+account_id+':recommender/'+recommender_recommended_for_you_name\n",
    "    describe_response = personalize.describe_recommender(recommenderArn = recommender_recommended_for_you_arn)\n",
    "\n",
    "    workshop_rerank_solution_arn = 'arn:aws:personalize:'+region+':'+account_id+':solution/'+workshop_rerank_solution_name\n",
    "    describe_response = personalize.describe_solution(solutionArn = workshop_rerank_solution_arn)\n",
    "    \n",
    "    print(\"SDK and resource check SUCCEEDED!\")\n",
    "    \n",
    "except:\n",
    "    print(\"SDK check FAILED. Proceed to the next cell if you will be uploading data and training models as you go.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Amazon Personalize Datasets <a class=\"anchor\" id=\"datasets\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "[Amazon Personalize](https://aws.amazon.com/personalize/) is a fully managed machine learning service that uses your data to generate item recommendations for your users. It can also generate user segments based on users’ affinity for certain items or item metadata.\n",
    "\n",
    "Regardless of the use case, the algorithms all learn user-item-interaction data, which is defined by 3 core attributes:\n",
    "\n",
    "1. **UserID** - The user who interacted\n",
    "1. **ItemID** - The item the user interacted with\n",
    "1. **Timestamp** - The time at which the interaction occurred\n",
    "\n",
    "Very often, your data will not arrive in a perfect form for Amazon Personalize from other systems (such as a product catalog, Custoemr Relationship Management (CRM)System, ...) and you will have to modify it to be structured correctly. This notebook guides you through that process.\n",
    "\n",
    "### Items data\n",
    "\n",
    "The item data consists of information about the products that users interact with, this data typically comes from the product catalog found in a Product Information Management (PIM) platform or an e-commerce system. For the purpose of this workshop, our items are products sold in the on-line retail store. To simulate our items data, we will be using a synthetic item dataset. This dataset is not mandatory, but providing good user metadata will ensure the best results in your trained models.\n",
    "\n",
    "### Interactions data\n",
    "\n",
    "The interaction data consists of information about the interactions the users of the fictional e-commerce store will have with the products sold there. This usually comes from analytics tools or Customer Data Platform's (CDP). The best interaction data for use for Amazon Personalize would include the sequential order of user behavior, what content was clicked on/purchased and the order it was interacted with. To simulate our interaction data, we will be using a synthetic interactions dataset. \n",
    "\n",
    "### User data\n",
    "\n",
    "The user data is what information you have about your users, it typically comes from Customer relationship management (CRM). We will be generating a small synthetic dataset to simulate this component of the workshop. This dataset is not mandatory, but providing good user metadata will ensure the best results in your trained models.\n",
    "\n",
    "In this notebook we will be importing interactions, user and item data into your environment, inspecting it and converting it to a format that allows you to use it in Amazon Personalize to train models to get personalized recommendations.\n",
    "\n",
    "In this notebook you will: upload and inspect your interaction, item nd user data, create a dataset group and upload the data to the different datasets.\n",
    "\n",
    "![Workflow](images/01_Data_Layer_Resources.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open and Explore the Simulated Retail Interactions Dataset\n",
    "\n",
    "For this example, we are using a synthetic dataset generated when you deployed your working environment via CloudFormation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_data = pd.read_csv(raw_interactions_file_path)\n",
    "interaction_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us see a few lines of the raw CSV data:\n",
    "\n",
    "- An ITEM_ID column of the item interacted with\n",
    "- A USER_ID column of the user who interacted\n",
    "- An EVENT_TYPE column which can be used to train different Personalize campaigns and also to filter on recommendations.\n",
    "- The custom DISCOUNT column, which is a contextual metadata field, that Personalize ranking and user recommendation campaigns can take into account to guess on the best next product.\n",
    "- A TIMESTAMP of when the interaction happened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chart the counts of each `EVENT_TYPE` generated for the interactions dataset. We're simulating a site where visitors heavily view/browse products and to a lesser degree add products to their cart and checkout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_attributes = interaction_data.select_dtypes(include = ['object'])\n",
    "\n",
    "plt.figure(figsize=(16,3))\n",
    "chart = sns.countplot(data = categorical_attributes, x = 'EVENT_TYPE')\n",
    "plt.xticks(rotation=90, horizontalalignment='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Amazon Personalize, you need to save timestamps in Unix Epoch format.\n",
    "\n",
    "Lets validate that the timestamp is actually in a Unix Epoch format by converting it into a more easily understood time/date format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arb_time_stamp = interaction_data.iloc[50]['TIMESTAMP']\n",
    "print('timestamp')\n",
    "print(arb_time_stamp)\n",
    "print()\n",
    "print('Date & Time')\n",
    "print(datetime.utcfromtimestamp(arb_time_stamp).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do some general summarization and inspection of the data to ensure that it will be helpful for Amazon Personalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to local storage\n",
    "interaction_data.to_csv((data_dir+\"/\"+interactions_file_name), index=False, float_format='%.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Users Dataset\n",
    "[Back to top](#top)\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> This is a synthetic dataset and since it is randomly assigned, will be of little value to our model, in a real world scenario this data would be accurate to the user data.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = pd.read_csv(raw_users_file_path)\n",
    "pd.set_option('display.max_rows', 5)\n",
    "\n",
    "user_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also leverage some syntectic user metadata to get some additional information about our users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_metadata_df = pd.read_json (user_metadata_file_name)\n",
    "user_metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to local storage\n",
    "user_data.to_csv((data_dir+\"/\"+users_file_name), index=False, float_format='%.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Items Dataset\n",
    "\n",
    "Let us look at the items dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_data = pd.read_csv(raw_items_file_path)\n",
    "pd.set_option('display.max_rows', 5)\n",
    "\n",
    "item_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to local storage\n",
    "item_data.to_csv((data_dir+\"/\"+items_file_name), index=False, float_format='%.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to load some additional metadata about each item that will help make the recommended items more readable in this workshop. This is data you'd typically find in an item catalog system/database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the item meta data\n",
    "with open(item_metadata_file_name) as f:\n",
    "    item_metadata_df = pd.json_normalize(yaml.load(f, Loader=yaml.FullLoader))[['id', 'name', 'category', 'style', 'featured']]\n",
    "    \n",
    "display (item_metadata_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Amazon Personalize Resources and Importing data <a class=\"anchor\" id=\"import\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure an S3 bucket and an IAM  role <a class=\"anchor\" id=\"bucket_role\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "So far, we have downloaded, manipulated, and saved the data onto the Amazon EBS instance attached to the instance running this Jupyter notebook.  \n",
    "\n",
    "By default, the Personalize service does not have permission to access the data we upload into  S3 buckets in our account. In order to grant access to the Amazon Personalize service to interact with our S3 Buckets, we need to set a Bucket Policy and create an IAM role that the Amazon Personalize service will assume. Let's set all of that up.\n",
    "\n",
    "Use the metadata stored on the instance underlying this Amazon SageMaker notebook, to determine the region it is operating in. If you are using a Jupyter notebook outside of Amazon SageMaker, simply define the region as a string below. The Amazon S3 bucket needs to be in the same region as the Amazon Personalize resources we have been creating so far.\n",
    "\n",
    "First, let us get the current notebook region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/opt/ml/metadata/resource-metadata.json') as notebook_info:\n",
    "    data = json.load(notebook_info)\n",
    "    resource_arn = data['ResourceArn']\n",
    "    region = resource_arn.split(':')[3]\n",
    "\n",
    "# To use a different region use:\n",
    "# region = <your_region>\n",
    "\n",
    "print('region:', region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon S3 bucket names are globally unique. To create a unique bucket name, the code below will append the string `personalize-poc-retail` to your AWS account number. Then it creates a bucket with this name in the region discovered in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "bucket_name = account_id + \"-\" + region + \"-\" + \"personalize-poc-retail\"\n",
    "\n",
    "#getting existing buckets in the account\n",
    "response = s3.list_buckets()\n",
    "\n",
    "if bucket_name in [x['Name'] for x in response['Buckets']]:\n",
    "    print(\"The bucket already exists.\")\n",
    "else:\n",
    "    if region == \"us-east-1\":\n",
    "        bucket_responese = s3.create_bucket(Bucket=bucket_name)\n",
    "    else:\n",
    "        bucket_responese = s3.create_bucket(\n",
    "            Bucket=bucket_name,\n",
    "            CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "print('bucket_name:', bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Personalize needs to be able to read the contents of your S3 bucket. Add a bucket policy which allows that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Id\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"personalize.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\",\n",
    "                \"s3:PutObject\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::{}\".format(bucket_name),\n",
    "                \"arn:aws:s3:::{}/*\".format(bucket_name)\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "bucket_current_policy = None\n",
    "\n",
    "try:\n",
    "    bucket_current_policy = s3.get_bucket_policy(Bucket=bucket_name)['Policy']\n",
    "    \n",
    "except s3.exceptions.from_code('NoSuchBucketPolicy') as e:    \n",
    "    print(\"There is no current Bucket Policy for bucket \" + bucket_name)\n",
    "    \n",
    "except Exception as e: \n",
    "    raise(e)\n",
    "\n",
    "if (bucket_current_policy and policy == json.loads(bucket_current_policy)):\n",
    "    print (\"The policy is already associated with the S3 Bucket.\")\n",
    "else:\n",
    "    print (\"Adding the policy to the bucket.\")\n",
    "    print(s3.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(policy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an IAM role\n",
    "\n",
    "Amazon Personalize also needs the ability to assume roles in AWS in order to have the permissions to execute certain tasks. Let's create an IAM role and attach the required policies to it. The code below attaches broad policies. You can use more restrictive policies for any production application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client(\"iam\")\n",
    "\n",
    "role_name = account_id+\"-PersonalizeS3-Immersion-Day\"\n",
    "assume_role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "            \"Service\": \"personalize.amazonaws.com\"\n",
    "          },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create policy\n",
    "\n",
    "s3_access_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": {\n",
    "            \"Sid\" : \"myStatement\" ,\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::{}\".format(bucket_name),\n",
    "                \"arn:aws:s3:::{}/*\".format(bucket_name)\n",
    "            ],\n",
    "            \"Action\": \"s3:*\"\n",
    "        }\n",
    "}\n",
    "\n",
    "try: \n",
    "\n",
    "    policy_response = iam.create_policy(\n",
    "        PolicyName='restrictedS3Access',\n",
    "        PolicyDocument=json.dumps(s3_access_policy_document),\n",
    "        Description='Restricst access to only workshop S3 bucket'\n",
    "    )\n",
    "\n",
    "    s3_access_policy_arn = policy_response['Policy']['Arn']\n",
    "\n",
    "    print (\"s3_access_policy_arn:{}\".format(s3_access_policy_arn))\n",
    "except:\n",
    "    s3_access_policy_arn = 'arn:aws:iam::{}:policy/restrictedS3Access'.format(account_id)\n",
    "    print ('The policy {} already exists.'.format(s3_access_policy_arn))\n",
    "    print ('Using the existing policy')\n",
    "\n",
    "\n",
    "try:\n",
    "    create_role_response = iam.create_role(\n",
    "        RoleName = role_name,\n",
    "        AssumeRolePolicyDocument = json.dumps(assume_role_policy_document),\n",
    "    );\n",
    "    role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "    \n",
    "    print (\"10s pause to allow role to be fully consistent.\")\n",
    "    time.sleep(10)\n",
    "    \n",
    "except iam.exceptions.EntityAlreadyExistsException as e:\n",
    "    print('Warning: role already exists:', e)\n",
    "    role_arn = iam.get_role(\n",
    "        RoleName = role_name\n",
    "    )[\"Role\"][\"Arn\"];\n",
    "\n",
    "print('IAM Role: {}\\n'.format(role_arn))\n",
    "    \n",
    "# Attach the policy if it is not previously attached:\n",
    "if (s3_access_policy_arn in [ x['PolicyArn'] for x in iam.list_attached_role_policies( RoleName = role_name)['AttachedPolicies']]):\n",
    "    print ('The policy {} is already attached to this role.'.format(s3_access_policy_arn))\n",
    "else:\n",
    "    print (\"Attaching the role_policy: {}\".format(s3_access_policy_arn))\n",
    "    attach_response = iam.attach_role_policy(\n",
    "        RoleName = role_name,\n",
    "        PolicyArn = s3_access_policy_arn\n",
    "    );\n",
    "    print (\"30s pause to allow role to be fully consistent.\")\n",
    "    time.sleep(30)\n",
    "    print('Done.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3\n",
    "\n",
    "Now that your Amazon S3 bucket has been created, upload the CSV files of our 3 datasets (Item, Interaction, and User).]\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> We will cover real-time data in a future notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_file_path = data_dir + \"/\" + interactions_file_name\n",
    "\n",
    "try:\n",
    "    s3.get_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=interactions_file_path,\n",
    "    )\n",
    "    print(\"{} already exists in the bucket {}\".format(interactions_file_path, bucket_name))\n",
    "except s3.exceptions.NoSuchKey:\n",
    "    # Uploading the file if it does not already exist\n",
    "    boto3.Session().resource('s3').Bucket(bucket_name).Object(interactions_file_name).upload_file(interactions_file_path)\n",
    "    print(\"File {} uploaded to bucket {}\".format(interactions_file_name, bucket_name))\n",
    "\n",
    "items_file_path = data_dir + \"/\" + items_file_name\n",
    "try:\n",
    "    s3.get_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=items_file_name,\n",
    "    )\n",
    "    print(\"{} already exists in the bucket {}\".format(items_file_path, bucket_name))\n",
    "except s3.exceptions.NoSuchKey:\n",
    "    # Uploading the file if it does not already exist     \n",
    "    boto3.Session().resource('s3').Bucket(bucket_name).Object(items_file_name).upload_file(items_file_path)\n",
    "    print(\"File {} uploaded to bucket {}\".format(items_file_name, bucket_name))\n",
    "\n",
    "users_file_path = data_dir + \"/\" + users_file_name\n",
    "try:\n",
    "    s3.get_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=users_file_name,\n",
    "    )\n",
    "    print(\"{} already exists in the bucket {}\".format(users_file_path, bucket_name))\n",
    "except s3.exceptions.NoSuchKey:\n",
    "    # Uploading the file if it does not already exist\n",
    "    boto3.Session().resource('s3').Bucket(bucket_name).Object(users_file_name).upload_file(users_file_path)\n",
    "    print(\"File {} uploaded to bucket {}\".format(users_file_name, bucket_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset group <a class=\"anchor\" id=\"group_dataset\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "The highest level of isolation and abstraction with Amazon Personalize is a *dataset group*. Information stored within one of these dataset groups has no impact on any other dataset group or models created from one – they are completely isolated. This allows you to run many experiments and is part of how we keep your models private and fully trained only on your data. \n",
    "\n",
    "Before importing the data prepared earlier, there needs to be a dataset group and a dataset added to it that handles the interactions.\n",
    "\n",
    "Dataset groups can house the following types of information:\n",
    "\n",
    "* User-item-interactions\n",
    "* Event streams (real-time interactions)\n",
    "* User metadata\n",
    "* Item metadata\n",
    "\n",
    "We need to create the dataset group that will contain our three datasets.\n",
    "\n",
    "Your dataset group can be one of the following types:\n",
    "\n",
    "* A Domain dataset group, where you create preconfigured resources for different business domains and use cases, such as getting recommendations for similar videos (VIDEO_ON_DEMAND domain) or best selling items (ECOMMERCE domain). You choose your business domain, import your data, and create recommenders. You use recommenders in your application to get recommendations. Use a [Domain dataset group](https://docs.aws.amazon.com/personalize/latest/dg/domain-dataset-groups.html) if you have a video on demand or e-commerce application and want Amazon Personalize to find the best configurations for your use cases. If you start with a Domain dataset group, you can also add custom resources such as solutions with solution versions trained with recipes for custom use cases.\n",
    "\n",
    "\n",
    "* A [Custom dataset group](https://docs.aws.amazon.com/personalize/latest/dg/custom-dataset-groups.html), where you create configurable resources for custom use cases and batch recommendation workflows. You select a recipe, train a solution version (model), and deploy the solution version with a campaign. You use a campaign in your application to get recommendations. Use a Custom dataset group if you don't have a video on demand or e-commerce application or want to configure and manage only custom resources, or want to get recommendations in a batch workflow. If you start with a Custom dataset group, you can't associate it with a domain later. Instead, create a new Domain dataset group.\n",
    "\n",
    "You can create and manage Domain dataset groups and Custom dataset groups with the AWS console, the AWS Command Line Interface (AWS CLI), or programmatically with the AWS SDKs.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> If you are running this as part of an AWS workshop, the resources have been created ahead of time, this is to eliminate the time spent waiting for the data to import, models to train and recommenders to deploy. In these notebooks will check to see if the resources exist and use them. You may see “Resource X Already exists” messages, if you run these notebooks in your own account, it will create these resources, which will add approximately 90 minutes to this workshop.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset Group\n",
    "The following cell will create a new dataset group with the name `personalize-poc-retail`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:     \n",
    "    # Try to create the dataset group, this block with exectute fully if the dataset group does not exist yet\n",
    "    \n",
    "    create_dataset_group_response = personalize.create_dataset_group(\n",
    "        name = workshop_dataset_group_name,\n",
    "        domain='ECOMMERCE'\n",
    "    )\n",
    "    workshop_dataset_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "    print(json.dumps(create_dataset_group_response, indent=2))\n",
    "    print ('\\nCreating the Dataset Group with dataset_group_arn = {}'.format(workshop_dataset_group_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException as e:\n",
    "    # if the dataset group already exists, get the unique identifier workshop_dataset_group_arn \n",
    "    # from the existing resource\n",
    "    \n",
    "    workshop_dataset_group_arn = 'arn:aws:personalize:'+region+':'+account_id+':dataset-group/'+workshop_dataset_group_name \n",
    "    print ('\\nThe the Dataset Group with dataset_group_arn = {} already exists'.format(workshop_dataset_group_arn))\n",
    "    print ('\\nWe will be using the existing Dataset Group dataset_group_arn = {}'.format(workshop_dataset_group_arn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Dataset Group to Have ACTIVE Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use the dataset group, it must be active. This can take a minute or two. Execute the cell below and wait for it to show the ACTIVE status. It checks the status of the dataset group every 60 seconds, up to a maximum of 3 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_group_response = personalize.describe_dataset_group(\n",
    "        datasetGroupArn = workshop_dataset_group_arn\n",
    "    )\n",
    "    status = describe_dataset_group_response[\"datasetGroup\"][\"status\"]\n",
    "    print(\"DatasetGroup: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a dataset group, you can create a dataset for the interaction data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Interactions Schema <a class=\"anchor\" id=\"interact_schema\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Now that we've loaded and prepared our three datasets, we'll need to configure the Amazon Personalize service to understand our data so that it can be used to train models for generating recommendations. Amazon Personalize requires a schema for each dataset, so it can map the columns in our CSVs to fields for model training. Each schema is declared in JSON using the [Apache Avro](https://avro.apache.org/) format. \n",
    "\n",
    "First, define a schema to tell Amazon Personalize what type of dataset you are uploading. There are several reserved and mandatory keywords required in the schema, based on the type of dataset. More detailed information can be found in the [documentation](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html).\n",
    "\n",
    "Here, you will create a schema for interactions data, which requires the `USER_ID`, `ITEM_ID`, and `TIMESTAMP` fields. These must be defined in the same order in the schema as they appear in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset we also have an `EVENT_TYPE` column that includes multiple common eCommerce event types (`View`, `AddToCart`, `Purchase`, and so on).\n",
    "\n",
    "The interactions dataset supports metadata columns. Interaction metadata columns are a way to provide contextual details that are specific to an interaction, such as the user's current device type (phone, tablet, desktop, set-top box, etc), the user's current location (city, region, metro code, etc), current weather conditions, and so on. For this dataset, we have a `DISCOUNT` column that indicates whether the user is interacting with an item that is currently discounted (`Yes`/`No`). It's being used to learn a user's affinity for items that are on sale or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Interactions\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"EVENT_TYPE\",  # \"View\", \"Purchase\", etc.\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TIMESTAMP\",\n",
    "            \"type\": \"long\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"DISCOUNT\",  # This is the contextual metadata - \"Yes\" or \"No\".\n",
    "            \"type\": \"string\",\n",
    "            \"categorical\": True,\n",
    "        },\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Try to create the interactions dataset schema, this block with exectute fully \n",
    "    # if the interactions dataset schema does not exist yet\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = interactions_schema_name,\n",
    "        schema = json.dumps(interactions_schema),\n",
    "        domain='ECOMMERCE'\n",
    "    )\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "    workshop_interactions_schema_arn = create_schema_response['schemaArn']\n",
    "    print ('\\nCreating the Interactions Schema with workshop_interactions_schema_arn = {}'.format(workshop_interactions_schema_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    # if the interactions dataset schema already exists, get the unique identifier workshop_interactions_schema_arn\n",
    "    # from the existing resource \n",
    "    \n",
    "    workshop_interactions_schema_arn = 'arn:aws:personalize:'+region+':'+account_id+':schema/'+interactions_schema_name \n",
    "    print('The schema {} already exists.'.format(workshop_interactions_schema_arn))\n",
    "    print ('\\nWe will be using the existing Interactions Schema with workshop_interactions_schema_arn = {}'.format(workshop_interactions_schema_arn))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the interactions dataset\n",
    "\n",
    "With a schema created, you can create a dataset within the dataset group. Note that this does not load the data yet, but creates a schema of what the data looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Try to create the interactions dataset, this block with exectute fully \n",
    "    # if the interactions dataset does not exist yet\n",
    "    \n",
    "    dataset_type = 'INTERACTIONS'\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = interactions_dataset_name,\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = workshop_dataset_group_arn,\n",
    "        schemaArn = workshop_interactions_schema_arn\n",
    "    )\n",
    "\n",
    "    workshop_interactions_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "    print ('\\nCreating the Interactions Dataset with workshop_interactions_dataset_arn = {}'.format(workshop_interactions_dataset_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    # if the interactions dataset already exists, get the unique identifier workshop_interactions_dataset_arn \n",
    "    # from the existing resource \n",
    "    workshop_interactions_dataset_arn =  'arn:aws:personalize:'+region+':'+account_id+':dataset/'+workshop_dataset_group_name+'/INTERACTIONS'\n",
    "    print('The Interactions Dataset {} already exists.'.format(workshop_interactions_dataset_arn))\n",
    "    print ('\\nWe will be using the existing Interactions Dataset with workshop_interactions_dataset_arn = {}'.format(workshop_interactions_dataset_arn))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Items (Products) schema<a class=\"anchor\" id=\"items_schema\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "First, define a schema to tell Amazon Personalize what type of dataset you are uploading. There are several reserved and mandatory keywords required in the schema, based on the type of dataset. More detailed information can be found in the [documentation](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html).\n",
    "\n",
    "The items dataset schema requires an `ITEM_ID` column and at least one metadata column. Up to 50 metadata columns can be added to the items dataset.\n",
    "\n",
    "For this dataset we have three metadata columns: `PRICE`, `CATEGORY_L1`, `CATEGORY_L2`, `PRODUCT_DESCRIPTION`, and `GENDER` (see schema definition in the cell below). \n",
    "\n",
    "We mapped the `category` and `style` fields from the items catalog to the `CATEGORY_L1` and `CATEGORY_L2` columns to indicate category levels. The `gender_affinity` field used to indicate Women's and Men's products (clothing, footwear, etc.) has been mapped to the `GENDER` column in the schema. \n",
    "\n",
    "Note that `CATEGORY_L1`, `CATEGORY_L2`, and `GENDER` are annotated as being categorical (`\"categorical\": true`). This tells Personalize to interpret the column value for each row as one or more categorical values, where the `|` character can be used to separate values. For example, `value1|value2|value3`. The `PRODUCT_DESCRIPTION` column is annotated as being a text column (`\"textual\": true`). This tells Personalize that this column contains unstructured text. A natural language processing (NLP) model is used to extract features from the textual column to use as features in the model. Including a textual column in your items dataset can significantly enhance the relevancy of recommendations. Currently, only one textual column can be included in the items dataset and the text must be in English. For more information, please refer to [the documentation](https://docs.aws.amazon.com/personalize/latest/dg/items-datasets.html#text-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Items\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"PRICE\",\n",
    "            \"type\": \"float\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"CATEGORY_L1\",\n",
    "            \"type\": \"string\",\n",
    "            \"categorical\": True\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"CATEGORY_L2\",\n",
    "            \"type\": \"string\",\n",
    "            \"categorical\": True\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"PRODUCT_DESCRIPTION\",\n",
    "            \"type\": \"string\",\n",
    "            \"textual\": True\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"GENDER\",\n",
    "            \"type\": \"string\",\n",
    "            \"categorical\": True\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Try to create the items dataset schema, this block with exectute fully \n",
    "    # if the items dataset schema does not exist yet\n",
    "    \n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = items_schema_name,\n",
    "        schema = json.dumps(items_schema),\n",
    "        domain='ECOMMERCE'\n",
    "    )\n",
    "    workshop_items_schema_arn = create_schema_response['schemaArn']\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "\n",
    "    print ('\\nCreating the Items Schema with workshop_items_schema_arn = {}'.format(workshop_items_schema_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    # if the items dataset schema already exists, get the unique identifier workshop_items_schema_arn \n",
    "    # from the existing resource \n",
    "    \n",
    "    workshop_items_schema_arn = 'arn:aws:personalize:'+region+':'+account_id+':schema/'+items_schema_name \n",
    "    print('The schema {} already exists.'.format(workshop_items_schema_arn))\n",
    "    print ('\\nWe will be using the existing Items Schema with workshop_items_schema_arn = {}'.format(workshop_items_schema_arn))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Items Dataset\n",
    "With a schema created, you can create a dataset within the dataset group. Note that this does not load the data yet, but creates a schema of what the data looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Try to create the items dataset, this block with execute fully if the items dataset does not exist yet\n",
    "    \n",
    "    dataset_type = \"ITEMS\"\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = items_dataset_name,\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = workshop_dataset_group_arn,\n",
    "        schemaArn = workshop_items_schema_arn\n",
    "    )\n",
    "\n",
    "    workshop_items_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "\n",
    "    print ('\\nCreating the Items Dataset with workshop_items_dataset_arn = {}'.format(workshop_items_dataset_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    # if the items dataset already exists, get the unique identifier workshop_items_dataset_arn \n",
    "    # from the existing resource \n",
    "    \n",
    "    workshop_items_dataset_arn =  'arn:aws:personalize:'+region+':'+account_id+':dataset/'+workshop_dataset_group_name+'/ITEMS'\n",
    "    print('The Items Dataset {} already exists.'.format(workshop_items_dataset_arn))\n",
    "    print ('\\nWe will be using the existing Items Dataset with workshop_items_dataset_arn = {}'.format(workshop_items_dataset_arn))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Users schema<a class=\"anchor\" id=\"users_schema\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "First, define a schema to tell Amazon Personalize what type of dataset you are uploading. There are several reserved and mandatory keywords required in the schema, based on the type of dataset. More detailed information can be found in the [documentation](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html).\n",
    "\n",
    "Here, you will create a schema for user data, which requires the `USER_ID`, and an additonal metadata field. For this dataset we have metadata columns for `AGE` and `GENDER`. These must be defined in the same order in the schema as they appear in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Users\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"AGE\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"GENDER\",\n",
    "            \"type\": \"string\",\n",
    "            \"categorical\": True,\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Try to create the users dataset schema, this block with exectute fully \n",
    "    # if the users dataset schema does not exist yet\n",
    "    \n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = users_schema_name,\n",
    "        schema = json.dumps(users_schema),\n",
    "        domain='ECOMMERCE'\n",
    "    )\n",
    "    \n",
    "    workshop_users_schema_arn = create_schema_response['schemaArn']\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "\n",
    "    print ('\\nCreating the Users Schema with workshop_users_schema_arn = {}'.format(workshop_users_schema_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    # if the users dataset schema already exists, get the unique identifier workshop_users_schema_arn \n",
    "    # from the existing resource \n",
    "\n",
    "    workshop_users_schema_arn = 'arn:aws:personalize:'+region+':'+account_id+':schema/'+users_schema_name \n",
    "    print('The schema {} already exists.'.format(workshop_users_schema_arn))\n",
    "    print ('\\nWe will be using the existing Users Schema with workshop_users_schema_arn = {}'.format(workshop_users_schema_arn))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Users Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Try to create the users dataset, this block with exectute fully if the users dataset does not exist yet\n",
    "    \n",
    "    dataset_type = \"USERS\"\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = users_dataset_name,\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = workshop_dataset_group_arn,\n",
    "        schemaArn = workshop_users_schema_arn\n",
    "    )\n",
    "\n",
    "    workshop_users_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "\n",
    "    print ('\\nCreating the Users Dataset with workshop_users_dataset_arn = {}'.format(workshop_users_dataset_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    # if the users dataset already exists, get the unique identifier workshop_users_dataset_arn\n",
    "    # from the existing resource \n",
    "    \n",
    "    workshop_users_dataset_arn =  'arn:aws:personalize:'+region+':'+account_id+':dataset/'+workshop_dataset_group_name+'/USERS'\n",
    "    print('The Users Dataset {} already exists.'.format(workshop_users_dataset_arn))\n",
    "    print ('\\nWe will be using the existing Users Dataset with workshop_users_dataset_arn = {}'.format(workshop_users_dataset_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wait untill all the datasets have been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_time = time.time() + 6*60*60 # 6 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_response = personalize.describe_dataset(\n",
    "        datasetArn = workshop_interactions_dataset_arn\n",
    "    )\n",
    "    status_interaction_dataset =  describe_dataset_response[\"dataset\"]['status']\n",
    "    print(\"Interactions Dataset: {}\".format(status_interaction_dataset))\n",
    "    \n",
    "    if status_interaction_dataset == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(workshop_interactions_dataset_arn))\n",
    "        \n",
    "    elif status_interaction_dataset == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(workshop_interactions_dataset_arn))\n",
    "        break\n",
    "        \n",
    "    if not status_interaction_dataset == \"ACTIVE\":\n",
    "        print(\"The interaction dataset creation is still in progress\")\n",
    "    else:\n",
    "        print(\"The interaction dataset  is ACTIVE\")\n",
    "        \n",
    "\n",
    "    describe_dataset_response = personalize.describe_dataset(\n",
    "        datasetArn = workshop_items_dataset_arn\n",
    "    )\n",
    "    status_item_dataset =  describe_dataset_response[\"dataset\"]['status']\n",
    "    print(\"Items Dataset: {}\".format(status_item_dataset))\n",
    "    \n",
    "    if status_item_dataset == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(workshop_items_dataset_arn))\n",
    "        \n",
    "    elif status_item_dataset == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(workshop_items_dataset_arn))\n",
    "        break\n",
    "        \n",
    "    if not status_item_dataset == \"ACTIVE\":\n",
    "        print(\"The item dataset creation is still in progress\")\n",
    "    else:\n",
    "        print(\"The item dataset  is ACTIVE\")\n",
    "    \n",
    "    describe_dataset_response = personalize.describe_dataset(\n",
    "        datasetArn = workshop_users_dataset_arn\n",
    "    )\n",
    "    status_user_dataset =  describe_dataset_response[\"dataset\"]['status']\n",
    "    print(\"Users Dataset: {}\".format(status_user_dataset))\n",
    "    \n",
    "    if status_user_dataset == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(workshop_users_dataset_arn))\n",
    "        \n",
    "    elif status_user_dataset == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(workshop_users_dataset_arn))\n",
    "        break\n",
    "        \n",
    "    if not status_user_dataset == \"ACTIVE\":\n",
    "        print(\"The user dataset creation is still in progress\")\n",
    "    else:\n",
    "        print(\"The user dataset  is ACTIVE\")\n",
    "    \n",
    "    if status_interaction_dataset == \"ACTIVE\" and status_item_dataset == \"ACTIVE\" and status_user_dataset == 'ACTIVE':\n",
    "        break\n",
    "        \n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Interactions <a class=\"anchor\" id=\"import_interactions\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Earlier you created the dataset group and dataset to house your information, now you will execute an import job that will load the item data from the S3 bucket into the Amazon Personalize dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the import job already exists\n",
    "\n",
    "# List the import jobs\n",
    "interactions_dataset_import_jobs = personalize.list_dataset_import_jobs(\n",
    "    datasetArn=workshop_interactions_dataset_arn,\n",
    "    maxResults=100\n",
    ")['datasetImportJobs']\n",
    "\n",
    "#check if there is an existing job with the prefix\n",
    "job_exists = False  \n",
    "job_arn = None\n",
    "\n",
    "for job in interactions_dataset_import_jobs:\n",
    "    if (interactions_import_job_name in job['jobName']):\n",
    "        job_exists = True\n",
    "        job_arn = job['datasetImportJobArn']\n",
    "    \n",
    "if (job_exists):\n",
    "    workshop_interactions_dataset_import_job_arn = job_arn\n",
    "    print('The Interactions Import Job {} already exists.'.format(workshop_interactions_dataset_import_job_arn))\n",
    "    print ('\\nWe will be using the existing Interactions Import Job with workshop_interactions_dataset_import_job_arn = {}'.format(workshop_interactions_dataset_import_job_arn))\n",
    "        \n",
    "else:\n",
    "    # If there is no import job with the prefix, create it:   \n",
    "    create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "        jobName = interactions_import_job_name,\n",
    "        datasetArn = workshop_interactions_dataset_arn,\n",
    "        dataSource = {\n",
    "            \"dataLocation\": \"s3://{}/{}\".format(bucket_name, interactions_file_name)\n",
    "        },\n",
    "        roleArn = role_arn\n",
    "    )\n",
    "    workshop_interactions_dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "    print(json.dumps(create_dataset_import_job_response, indent=2))\n",
    "    \n",
    "    print ('\\nImporting the Interactions Data with workshop_interactions_dataset_import_job_arn = {}'.format(workshop_interactions_dataset_import_job_arn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Item Metadata <a class=\"anchor\" id=\"import_items\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Earlier you created the dataset group and dataset to house your information, now you will execute an import job that will load the item data from the S3 bucket into the Amazon Personalize dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the import job already exists\n",
    "\n",
    "# List the import jobs\n",
    "items_dataset_import_jobs = personalize.list_dataset_import_jobs(\n",
    "    datasetArn=workshop_items_dataset_arn,\n",
    "    maxResults=100\n",
    ")['datasetImportJobs']\n",
    "\n",
    "job_exists = False\n",
    "job_arn = None\n",
    "\n",
    "#check if there is an existing job with the prefix\n",
    "for job in items_dataset_import_jobs:\n",
    "    if (items_import_job_name in job['jobName']):\n",
    "        job_exists = True\n",
    "        job_arn = job['datasetImportJobArn']\n",
    "    \n",
    "if (job_exists):\n",
    "    workshop_items_dataset_import_job_arn =  job_arn\n",
    "    print('The Items Import Job {} already exists.'.format(workshop_items_dataset_import_job_arn))\n",
    "    print ('\\nWe will be using the existing Items Import Job with workshop_items_dataset_import_job_arn = {}'.format(workshop_items_dataset_import_job_arn))\n",
    "        \n",
    "else:\n",
    "    # If there is no import job with the prefix, create it:    \n",
    "    create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "        jobName = items_import_job_name,\n",
    "        datasetArn = workshop_items_dataset_arn,\n",
    "        dataSource = {\n",
    "            \"dataLocation\": \"s3://{}/{}\".format(bucket_name, items_file_name)\n",
    "        },\n",
    "        roleArn = role_arn\n",
    "    )\n",
    "\n",
    "    workshop_items_dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "    print(json.dumps(create_dataset_import_job_response, indent=2))\n",
    "    print ('\\nImporting the Items Data with workshop_items_dataset_import_job_arn = {}'.format(workshop_items_dataset_import_job_arn))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the User Metadata <a class=\"anchor\" id=\"import_users\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Earlier you created the dataset group and dataset to house your information, now you will execute an import job that will load the user data from the S3 bucket into the Amazon Personalize dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the import job already exists\n",
    "\n",
    "# List the import jobs\n",
    "users_dataset_import_jobs = personalize.list_dataset_import_jobs(\n",
    "    datasetArn=workshop_users_dataset_arn,\n",
    "    maxResults=100\n",
    ")['datasetImportJobs']\n",
    "\n",
    "#check if there is an existing job with the prefix\n",
    "job_exists = False \n",
    "job_arn = None      \n",
    "for job in users_dataset_import_jobs:\n",
    "    if (users_import_job_name in job['jobName']):\n",
    "        job_exists = True\n",
    "        job_arn = job['datasetImportJobArn']\n",
    "\n",
    "if (job_exists):\n",
    "    workshop_users_dataset_import_job_arn =  job_arn\n",
    "    print('The Users Import Job {} already exists.'.format(workshop_users_dataset_import_job_arn))\n",
    "    print ('\\nWe will be using the existing Users Import Job with workshop_users_dataset_import_job_arn = {}'.format(workshop_users_dataset_import_job_arn))\n",
    "        \n",
    "else:\n",
    "    # If there is no import job with the prefix, create it:  \n",
    "    create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "        jobName = users_import_job_name,\n",
    "        datasetArn = workshop_users_dataset_arn,\n",
    "        dataSource = {\n",
    "            \"dataLocation\": \"s3://{}/{}\".format(bucket_name, users_file_name)\n",
    "        },\n",
    "        roleArn = role_arn\n",
    "    )\n",
    "\n",
    "    workshop_users_dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "    print(json.dumps(create_dataset_import_job_response, indent=2))\n",
    "    \n",
    "    print ('\\nImporting the Users Data with workshop_users_dataset_import_job_arn = {}'.format(workshop_users_dataset_import_job_arn))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for Import Jobs to Complete\n",
    "\n",
    "Before we can use the dataset, the import job must be active. Execute the cell below and wait for it to show the ACTIVE status. It checks the status of the import job every minute, up to a maximum of 6 hours.\n",
    "\n",
    "It will take 10-15 minutes for the import jobs to complete. While you're waiting you can learn more about Datasets and Schemas in [the documentation](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html).\n",
    "\n",
    "We will wait for all three jobs to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = time.time() + 6*60*60 # 10 hours\n",
    "while time.time() < max_time:\n",
    "\n",
    "    # Interactions dataset import\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = workshop_interactions_dataset_import_job_arn\n",
    "    )\n",
    "    status_interactions_import = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    \n",
    "    if status_interactions_import == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(workshop_interactions_dataset_import_job_arn))\n",
    "        \n",
    "    elif status_interactions_import == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(workshop_interactions_dataset_import_job_arn))\n",
    "        break\n",
    "        \n",
    "    if not status_interactions_import == \"ACTIVE\":\n",
    "        print(\"The interactions dataset import is still in progress\")\n",
    "    else:\n",
    "        print(\"The interactions dataset import is ACTIVE\")\n",
    "\n",
    "    # Items dataset import\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = workshop_items_dataset_import_job_arn\n",
    "    )\n",
    "    status_items_import = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    \n",
    "    if status_items_import == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(workshop_items_dataset_import_job_arn))\n",
    "        \n",
    "    elif status_items_import == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(workshop_items_dataset_import_job_arn))\n",
    "        break\n",
    "        \n",
    "    if not status_items_import == \"ACTIVE\":\n",
    "        print(\"The items dataset import is still in progress\")\n",
    "    else:\n",
    "        print(\"The items dataset import is ACTIVE\")\n",
    "        \n",
    "        \n",
    "   # Users dataset import  \n",
    "    describe_users_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = workshop_users_dataset_import_job_arn\n",
    "    )\n",
    "    status_users_import = describe_users_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    \n",
    "    if status_users_import == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(workshop_users_dataset_import_job_arn))\n",
    "        \n",
    "    elif status_users_import == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(workshop_users_dataset_import_job_arn))\n",
    "        break\n",
    "        \n",
    "    if not status_users_import == \"ACTIVE\":\n",
    "        print(\"The user dataset import is still in progress\")\n",
    "    else:\n",
    "        print(\"The user dataset import is ACTIVE\")\n",
    "        \n",
    "\n",
    "    if status_interactions_import == \"ACTIVE\" and status_items_import == 'ACTIVE' and status_users_import  == 'ACTIVE':\n",
    "        break\n",
    "\n",
    "    print()\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all imports now complete you can  start training recommenders and solutions. Run the cell below before moving on to store a few values for usage in the next notebooks. After completing that cell open notebook `02_Training_Layer.ipynb` to continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Useful Variables <a class=\"anchor\" id=\"vars\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Before exiting this notebook, run the following cells to save the version ARNs for use in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store data_dir\n",
    "%store interactions_file_name\n",
    "%store items_file_name\n",
    "%store users_file_name\n",
    "%store workshop_dataset_group_arn\n",
    "%store workshop_interactions_dataset_arn\n",
    "%store workshop_items_dataset_arn\n",
    "%store workshop_users_dataset_arn\n",
    "%store workshop_interactions_schema_arn\n",
    "%store workshop_items_schema_arn\n",
    "%store workshop_users_schema_arn\n",
    "%store workshop_rerank_solution_name\n",
    "%store workshop_rerank_campaign_name\n",
    "\n",
    "%store recommender_customers_who_viewed_name\n",
    "%store recommender_recommended_for_you_name\n",
    "\n",
    "%store region\n",
    "%store account_id\n",
    "%store role_name\n",
    "%store role_arn\n",
    "\n",
    "%store bucket_name\n",
    "\n",
    "%store item_metadata_df\n",
    "%store user_metadata_df\n",
    "\n",
    "%store s3_access_policy_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to the next notebook `02_Training_Layer.ipynb`](02_Training_Layer.ipynb) to continue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
