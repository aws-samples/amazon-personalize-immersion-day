{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Getting Started\n",
    "\n",
    "The [Maintaining Personalized Experiences with Machine Learning](https://aws.amazon.com/solutions/implementations/maintaining-personalized-experiences-with-ml/) solution helps you to create custom recommendation at scale. This solution streamlines and accelerates the development and deployment of your personalization workloads throug end-to-end automation and scheduling of updates for resources within the Amazon Personalize service.\n",
    "\n",
    "The solution is launched as a CloudFormation template, and deploys the following infrastructure:\n",
    "\n",
    "![Architecture Diagram](../../static/imgs/personalize-solution-architecture.jpg)\n",
    "\n",
    "1. An Amazon Simple Storage Service (Amazon S3) bucket used to store personalization data and configuration files.\n",
    "1. An AWS Lambda function initiated when new or updated personalization configuration is uploaded to the personalization data bucket.\n",
    "1. An AWS Step Functions workflow to manage all of the resources of an Amazon Personalize dataset group (including datasets, schemas, event tracker, filters, solutions, campaigns, and batch inference jobs).\n",
    "1. Amazon CloudWatch metrics for Amazon Personalize for each new trained solution version are added to help you evaluate the performance of a model over time.\n",
    "1. An Amazon Simple Notification Service (SNS) topic and subscription to notify an administrator when the maintenance workflow has completed via email.\n",
    "1. Amazon DynamoDB tracks the scheduled events configured for Amazon Personalize to fully or partially retrain Amazon Personalize solutions, import or reimport datasets, and perform batch inference jobs.\n",
    "1. An AWS Step Functions workflow tracks the current running scheduled events, and invoke step functions to perform Amazon Personalize solution maintenance (creating new solution versions, updating campaigns), import updated datasets, and perform batch inference.\n",
    "1. A set of maintenance AWS step functions to create new dataset import jobs on schedule; perform Amazon Personalize solution FULL retraining on schedule (and update associated campaigns); perform Amazon Personalize solution UPDATE retraining on schedule (and update associated campaigns); and create batch inference jobs.\n",
    "1. An Amazon EventBridge event bus, where resource status notification updates are posted throughout the AWS Step functions workflow\n",
    "1. A command line interface (CLI) lets existing resources be imported and allows schedules to be established for resources that already exist in Amazon Personalize\n",
    "\n",
    "# Deploying the Solution\n",
    "\n",
    "Run the next cell to generate a \"Launch Stack\" link for the region your notebook is running in. For the purposes of this lab, please do not change the stack name (but feel free to provide an Email address in the prompt for parameters, to enable notifications.\n",
    "\n",
    "**Note**: if notifications are desired, you must provide an email address **and** accept the SNS subscription confirmation via email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Run this cell to generate a \"Launch Stack\" link for your region (or use the generated, us-east-1 link below)\n",
    "import boto3\n",
    "import solution_helper\n",
    "\n",
    "# Adjust this to the name you would like your stack to have\n",
    "solution_helper.STACK_NAME = \"PersonalizeStack\"\n",
    "\n",
    "# Display the Quick Launch Link\n",
    "solution_helper.show_quick_launch_link()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you wait for the stack to deploy (it takes about five minutes), check out some of the Solution Details below, then proceed to the next step.\n",
    "\n",
    "# Solution Details\n",
    "\n",
    "- Visit the [Solution Landing Page](https://aws.amazon.com/solutions/implementations/maintaining-personalized-experiences-with-ml/) to learn more about the solution\n",
    "- Check out the [Solution Implementation Guide](https://docs.aws.amazon.com/solutions/latest/maintaining-personalized-experiences-with-ml/welcome.html) to learn how to use and operate the solution\n",
    "- View the [Solution Source Code](https://github.com/aws-solutions/maintaining-personalized-experiences-with-machine-learning) if you need to customize and extend the solution for your own use case\n",
    "\n",
    "## Uploading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the bucket that the CloudFormation stack deployed. We will be uploading our data and configuration file(s) to trigger the automation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the status of the CloudFormation Stack\n",
    "# Deployment takes about five minutes. \n",
    "from importlib import reload \n",
    "reload(solution_helper)\n",
    "\n",
    "solution_helper.wait_for_stack()\n",
    "bucket_name = solution_helper.get_stack_bucket()\n",
    "table_name = solution_helper.get_stack_dynamo()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the bucket name, we can copy over our data. There are three ways to upload your datasets to S3:\n",
    "\n",
    "1. Using the AWS Console\n",
    "2. Using the AWS CLI \n",
    "3. Using the boto3 SDK (we do this next) \n",
    "\n",
    "Using the drop-down, select your domain and click \"Deploy\" to have the SDK upload the sample datasets of your choice. Note that the datasets will be uploaded, but until the configuration file is created, they will not be imported into Amazon Personalize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to display a dropdown for each domain, and select `Deploy` to deploy the sample datasets for the desired domain\n",
    "solution_helper.show_dataset_uploader(bucket_name=bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the State Machine Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have the solution automate resource creation in Amazon Personalize, we need to provide it with a parameters file that will tell our state machine which names and configurations we want in our Amazon Personalize deployment. \n",
    "\n",
    "The solution requires this file to be valid JSON and the file name must end in `.json`.\n",
    "\n",
    "Using the drop-down, select your domain, and click \"Generate\" to have the notebook display your configuration. Once you have reviewed the configuration, click \"Upload\" to start creating resources in Amazon Personalize. Once again, there are three ways to upload your configuration to S3:\n",
    "\n",
    "1. Using the AWS Console\n",
    "2. Using the AWS CLI \n",
    "3. Using the boto3 SDK (we do this next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to display a dropdown for each domain, and select `Show` then `Deploy` to deploy the sample datasets for the desired domain\n",
    "from importlib import reload; reload(solution_helper)\n",
    "solution_helper.show_config_uploader(bucket_name=bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating your MLOps pipeline\n",
    "\n",
    "The fastest way to check on the status of your pipeline is to go to the CloudWatch dashboard. In the AWS Console, you can navigate to the [Amazon CloudWatch console](https://console.aws.amazon.com/cloudwatch/home?#dashboards:). Here, a dashboard will be deployed with the name `PersonalizeSolution-PersonalizeSolution-<region>` where `<region>` is the region in which your dashboard is deployed. This dashboard tracks:\n",
    "\n",
    "1. The number of configuration files processed (tracking success and failure)\n",
    "2. The number of workflow jobs run (tracking success and failure)\n",
    "3. Metrics around scheduler job creation\n",
    "4. Metrics for Amazon Personalize resource creation\n",
    "\n",
    "The solution also ensures that as solution versions are created in Amazon Personalize, Amazon CloudWatch metrics for each offline metric is created. This allows you to track the performance of your model in a directional sense. Depending on the stack name and solution trained, these metrics will be availalble as:\n",
    "\n",
    "- `personalize_solution_<stack_name>` > `SolutionMetrics` > `<solutionArn>` > `<metric_name>`\n",
    "\n",
    "The metrics documented [here](https://docs.aws.amazon.com/personalize/latest/dg/working-with-training-metrics.html) are published to CloudWatch by the solution as new Solution Versions are trained.\n",
    "\n",
    "### Checking the status of workflow(s) in the AWS StepFunctions console:\n",
    "\n",
    "The solution deploys four step functions into your account\n",
    "\n",
    "1. `Personalize Workflow` - The main MLOps workflow\n",
    "2. `Personalize Dataset Import` - The workflow used to generate new dataset imports on schedule\n",
    "3. `Personalize Solution Maintenance` - The workflow used to create new solution versions (FULL or UPDATE retraining) and batch inference jobs on schedule\n",
    "4. `Personalize Scheduler` - The workflow used to to schedule dataset imports and solution maintenance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to get the ARNs / console links of the step functions state machines listed above\n",
    "from importlib import reload\n",
    "reload(solution_helper)\n",
    "stepfunctions = solution_helper.show_stack_stepfunctions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional information\n",
    "\n",
    "Model training time will vary, and will take at least 30 minutes to complete. \n",
    "\n",
    "You can check the status of any of the state machines above in the console by:\n",
    "\n",
    "1. Clicking **any execution** in the main MLOps workflow (1) step function to view its status\n",
    "\n",
    "2. You can see which steps are currently running, highlited in blue\n",
    "\n",
    "The solutions' step function definitions are tuned to automatically retry each step by querying the describe service APIs with a backoff rate, and accommodate retryable service quotas [documented here](https://docs.aws.amazon.com/personalize/latest/dg/limits.html)\n",
    "\n",
    "The main personalize workflow will take about 30 minutes to finish executing, which includes importing the datasets, training at least one solution, and deploying a campaign. **Note:** we are only training a SIMS model due to time constrains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduling\n",
    "\n",
    "You can set up scheduling for dataset import, solution version retraining, and batch inference jobs. To do so, upload a configuration file with scheduling enabled or use the `aws-solutions-scheduler` included in the solution to enable scheduling on an existing resource.\n",
    "\n",
    "## Listing Schedules \n",
    "\n",
    "A list of schedules can be obtained by using the `list` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to list schedules: \n",
    "region = solution_helper.REGION\n",
    "stack = solution_helper.STACK_NAME\n",
    "\n",
    "!aws-solutions-scheduler --stack {stack} --region {region} list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe a Schedule\n",
    "\n",
    "A schedule can be described by using the `describe` command and specifying the scheduled `task` name. Depending on the demo data and configuration deployed, you should use one of the following task names: \n",
    "\n",
    "- **CPG**: `personalize-dataset-import-immersion_day_cpg`\n",
    "- **Media**: `personalize-dataset-import-immersion_day_media`\n",
    "- **Retail**: `personalize-dataset-import-immersion_day_retail`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this task name to one of the task names listed above!\n",
    "task_name = \"personalize-dataset-import-immersion_day_retail\"\n",
    "\n",
    "!aws-solutions-scheduler --stack {stack} --region {region} describe --task {task_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update a Schedule\n",
    "\n",
    "A schedule can be updated (or imported) using the `import-dataset-group` command. The solution supports cron-style expressions, as documented [here](https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-create-rule-schedule.html#eb-cron-expressions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dataset group to be to immersion_day_media, immersion_day_cpg, or immersion_day_retail\n",
    "dataset_group = \"immersion_day_retail\"\n",
    "domain = dataset_group.split(\"_\")[-1]\n",
    "\n",
    "!aws-solutions-scheduler --stack {stack} --region {region} import-dataset-group --dataset-group {dataset_group} --path train/immersionday/{domain}/config.json --import-schedule \"cron(0 0 ? * 2 *)\" --full-schedule \"immersion_{domain}_sims@cron(0 0 ? * 1 *)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the update command, re-run the `list` and `describe` commands to view the updated list of tasks and display them.\n",
    "\n",
    "## Deactivate or Activate a Schedule\n",
    "\n",
    "Deactivating a schedule can be useful for maintenance operations.\n",
    "\n",
    "### Deactivation \n",
    "\n",
    "Activated schedules can be deactivated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws-solutions-scheduler --stack {stack} --region {region} deactivate --task solution-maintenance-full-immersion_{domain}_sims >/dev/null && aws-solutions-scheduler --stack {stack} --region {region} describe --task solution-maintenance-full-immersion_{domain}_sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation\n",
    "\n",
    "Deactivated schedules can be activated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws-solutions-scheduler --stack {stack} --region {region} activate --task solution-maintenance-full-immersion_{domain}_sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "\n",
    "The stack can be cleaned up following the instructions [here](https://docs.aws.amazon.com/solutions/latest/maintaining-personalized-experiences-with-ml/uninstall-the-solution.html). This will remove the stack and schedules, but leave all resources untouched in Amazon Personalize. This process is automated for you by running the cells below.\n",
    "\n",
    "## Cleanup Resources in Amazon Personalize:\n",
    "\n",
    "Run the next cell to delete all the immersion day resources in Amazon Personalize created by this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_helper.delete_personalize_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Schedules\n",
    "\n",
    "Stop all scheduled tasks managed by the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Deactivating Tasks:\")\n",
    "!aws-solutions-scheduler --stack {stack} --region {region} list | jq -c \".tasks[]|@text\" | xargs -L1 aws-solutions-scheduler --stack {stack} --region {region} deactivate --task \n",
    "print(\"Task Status:\")\n",
    "!aws-solutions-scheduler --stack {stack} --region {region} list | jq -c \".tasks[]|@text\" | xargs -L1 aws-solutions-scheduler --stack {stack} --region {region} describe --task "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Stack Resources\n",
    "\n",
    "You can now delete the stack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Deleting Stack: {solution_helper.STACK_NAME}\")\n",
    "solution_helper.STACK_NAME = \"PersonalizeStack\"\n",
    "!aws cloudformation delete-stack --stack-name {solution_helper.STACK_NAME}\n",
    "!aws cloudformation wait stack-delete-complete --stack-name {solution_helper.STACK_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are sure the stack has been deleted, run the next cell to delete the bucket and dynamodb table created by the stack (by default, the solution will leave these resources in your account so as to not delete data unexpectedly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Deleting Bucket: {bucket_name}\")\n",
    "!aws s3 rb s3://{bucket_name} --force\n",
    "\n",
    "print(f\"Deleting Table: {table_name}\")\n",
    "!aws dynamodb delete-table --table-name {table_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the solution resources have been cleaned from your account"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_latest_p37",
   "language": "python",
   "name": "conda_mxnet_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
