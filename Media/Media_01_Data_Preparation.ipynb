{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to UnicornFlix!\n",
    "\n",
    "Congratulations! You have just been hired by UnicornFlix, which is a new Direct to Consumer Streaming service that launched and is jumping into the crowded space of Video on Demand/Free Ad Supported TV (FAST) providers. You have been hired into the Search & Discovery team which leads efforts around personalization. Currently most of your app does not provide a personalized experience, the movies are presented in a static order for all users. In order to prevent customer churn, you are looking to add personalized experiences. \n",
    "\n",
    "Youâ€™ve been asked by the founders to:\n",
    "\n",
    "- Increase subscriber engagement by tailoring every experience to individual users\n",
    "- Help users discover newly released content\n",
    "- Increase the breadth of content offered to them from the UnicornFlix catalog\n",
    "- Reduce the time to value by creating valuable recommendations in a short time\n",
    "\n",
    "Throughout the this workshop you will be exploring your datasets, building/training several recommendation models and implementing recommendations with API's.\n",
    "\n",
    "\n",
    "## In this Notebook\n",
    "\n",
    "In this notebook, you will choose a dataset and prepare it for use with Amazon Personalize.\n",
    "\n",
    "1. [How to Use the Notebook](#usenotebook)\n",
    "1. [Introduction to Amazon Personalize Datasets](#datasets)\n",
    "1. [Prepare the Item Metadata](#prepare_items)\n",
    "1. [Prepare the Interactions Data](#prepare_interactions)\n",
    "1. [Prepare the User Metadata](#prepare_users)\n",
    "\n",
    "\n",
    "## How to Use the Notebook <a class=\"anchor\" id=\"usenotebook\"></a>\n",
    "\n",
    "### Executing cells\n",
    "The code is broken up into cells like the one below. There's a triangular Run button at the top of this page that you can click to execute each cell and move onto the next, or you can press `Shift` + `Enter` while in the cell to execute it and move onto the next one.\n",
    "\n",
    "As a cell is executing you'll notice a line to the side showcase an `*` while the cell is running or it will update to a number to indicate the last cell that completed executing after it has finished exectuting all the code within a cell.\n",
    "\n",
    "Simply follow the instructions below and execute the cells to get started with Amazon Personalize.\n",
    "\n",
    "### Understanding the code\n",
    "\n",
    "This notebook can be used in two modalities:\n",
    "\n",
    "1. Train as you go by executing each cell. Some cells may take a long time to finish executing as they wait for resources to be created. You will have this experience if you deployed the 'simple'  Amazon CloudFormation template ([personalizeIDSimple.yaml](../personalizeIDSimple.yaml)).\n",
    "\n",
    "2. Go through notebook with previously created resources. All or the majority of the resources will already be created and cells will just retrieve the information of these existing resources to use them in following steps. You will have this experience if you deployed the 'pretrained' Amazon CloudFormation template ([PersonalizeIDPretrained.yaml](../PersonalizeIDPretrained.yaml)).\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> If you deployed the 'pretrained' template, you must wait at least 3 hours for all resources to tbe trained before proceeding with the workshop.\n",
    "</div>\n",
    "\n",
    "Because of this, you will find that some cells have `try` and `except` blocks. In particular most of them are handling a `ResourceAlreadyExistsException` exception. \n",
    "\n",
    "You can look at the code in the `try` block to get a good idea of how you can create a resource and understand how to use the Amazon Personalize SDK. The `except` block will let you know that the resource has been created and record the corresponding ARN, which is the Amazon unique identifier.\n",
    "\n",
    "This is an example of the `try` block for creating a dataset group, this code will execute without exceptions if the dataset group does not exist and raise an exception if the dataset group does already exist:\n",
    "\n",
    "```python\n",
    "try: \n",
    "    create_dataset_group_response = personalize.create_dataset_group(\n",
    "        name = workshop_dataset_group_name,\n",
    "        domain='VIDEO_ON_DEMAND'\n",
    "    )\n",
    "\n",
    "    workshop_dataset_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "    print(json.dumps(create_dataset_group_response, indent=2))\n",
    "    print ('\\nCreating the Dataset Group with dataset_group_arn = {}'.format(workshop_dataset_group_arn))\n",
    "    \n",
    "```\n",
    "and this is the corresponding `except` block that will be exectuted if an exeption is raised becuse the dataset group already exists. This block saves the ARN for the existig dataset group to use later and lets you know the resource already exists.\n",
    "\n",
    "```python\n",
    "except personalize.exceptions.ResourceAlreadyExistsException as e:\n",
    "    workshop_dataset_group_arn = 'arn:aws:personalize:'+region+':'+account_id+':dataset group/' + \n",
    "        workshop_dataset_group_name \n",
    "    print ('\\nThe the Dataset Group with dataset_group_arn = {} already exists'.format(\n",
    "        workshop_dataset_group_arn))\n",
    "    print ('\\nWe will be using the existing Dataset Group dataset_group_arn = {}'.format(\n",
    "        workshop_dataset_group_arn))\n",
    "```\n",
    "\n",
    "Depending on the resource, you may also find that sometimes the code will check from a list of resourses to find if a resource exists and then use `if` and `else` blocks to either use the existing resource or create it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's build!\n",
    "\n",
    "Python ships with a broad collection of libraries and we need to import those as well as the ones installed to help us like [boto3](https://aws.amazon.com/sdk-for-python/) (AWS SDK for python) and [Pandas](https://pandas.pydata.org/)/[Numpy](https://numpy.org/)  which are core data science tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (24.0)\n",
      "Collecting botocore\n",
      "  Using cached botocore-1.34.93-py3-none-any.whl.metadata (5.7 kB)\n",
      "Using cached botocore-1.34.93-py3-none-any.whl (12.2 MB)\n",
      "Installing collected packages: botocore\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.34.93\n",
      "    Uninstalling botocore-1.34.93:\n",
      "      Successfully uninstalled botocore-1.34.93\n",
      "Successfully installed botocore-1.34.93\n",
      "Found existing installation: awscli 1.32.93\n",
      "Uninstalling awscli-1.32.93:\n",
      "  Successfully uninstalled awscli-1.32.93\n",
      "Collecting awscli\n",
      "  Using cached awscli-1.32.93-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: botocore==1.34.93 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from awscli) (1.34.93)\n",
      "Requirement already satisfied: docutils<0.17,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from awscli) (0.16)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from awscli) (0.10.0)\n",
      "Requirement already satisfied: PyYAML<6.1,>=3.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from awscli) (6.0.1)\n",
      "Requirement already satisfied: colorama<0.4.7,>=0.2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from awscli) (0.4.4)\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from awscli) (4.7.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore==1.34.93->awscli) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore==1.34.93->awscli) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore==1.34.93->awscli) (2.0.7)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rsa<4.8,>=3.1.2->awscli) (0.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.34.93->awscli) (1.16.0)\n",
      "Using cached awscli-1.32.93-py3-none-any.whl (4.4 MB)\n",
      "Installing collected packages: awscli\n",
      "Successfully installed awscli-1.32.93\n",
      "Found existing installation: numexpr 2.10.0\n",
      "Uninstalling numexpr-2.10.0:\n",
      "  Successfully uninstalled numexpr-2.10.0\n",
      "Collecting numexpr\n",
      "  Using cached numexpr-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from numexpr) (1.22.4)\n",
      "Using cached numexpr-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (376 kB)\n",
      "Installing collected packages: numexpr\n",
      "Successfully installed numexpr-2.10.0\n"
     ]
    }
   ],
   "source": [
    "# Get the latest version of botocore to ensure we have the latest features in the SDK\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install --upgrade --no-deps --force-reinstall botocore\n",
    "!pip uninstall -y awscli\n",
    "!pip install awscli\n",
    "!pip uninstall -y numexpr\n",
    "!pip install numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import time\n",
    "from time import sleep\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory â€˜poc_dataâ€™: File exists\n",
      "mkdir: cannot create directory â€˜poc_data/imdbâ€™: File exists\n"
     ]
    }
   ],
   "source": [
    "# Make directories\n",
    "data_dir = \"poc_data\"\n",
    "!mkdir $data_dir\n",
    "imdb_dir = data_dir+'/imdb'\n",
    "!mkdir $imdb_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure the SDK to Personalize:\n",
    "personalize = boto3.client('personalize')\n",
    "personalize_runtime = boto3.client('personalize-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "account id: 381491864570\n",
      "region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "# Get the account id and region to use later\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "print(\"account id:\", account_id)\n",
    "\n",
    "with open('/opt/ml/metadata/resource-metadata.json') as notebook_info:\n",
    "    data = json.load(notebook_info)\n",
    "    resource_arn = data['ResourceArn']\n",
    "    region = resource_arn.split(':')[3]\n",
    "print(\"region:\", region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is a workshop and the resources were created for you, we will retrieve the variables of the resources created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('./params.json')\n",
    "parameters = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "workshop_dataset_group_name = parameters['datasetGroup']['serviceConfig']['name']\n",
    "\n",
    "interactions_schema_name = parameters['datasets']['interactions']['schema']['serviceConfig']['name']\n",
    "interactions_dataset_name = parameters['datasets']['interactions']['dataset']['serviceConfig']['name']\n",
    "\n",
    "items_schema_name = parameters['datasets']['items']['schema']['serviceConfig']['name']\n",
    "items_dataset_name = parameters['datasets']['items']['dataset']['serviceConfig']['name']\n",
    "\n",
    "users_schema_name = parameters['datasets']['users']['schema']['serviceConfig']['name']\n",
    "users_dataset_name = parameters['datasets']['users']['dataset']['serviceConfig']['name']\n",
    "\n",
    "#The following job names are the starting Strings of the job names that can be created\n",
    "interactions_import_job_name = 'dataset_import_interaction'\n",
    "items_import_job_name = 'dataset_import_item'\n",
    "users_import_job_name = 'dataset_import_user'\n",
    "\n",
    "items_filename = \"items.csv\"\n",
    "interactions_filename = \"interactions.csv\"\n",
    "users_filename = \"users.csv\"\n",
    "\n",
    "for recommender in parameters['recommenders']:\n",
    "    # This is currently configured assuming only one recommender of each type, if there are multiple \n",
    "    # recommenders of the same type further configuration is needed.\n",
    "    if (recommender['serviceConfig']['recipeArn'] == 'arn:aws:personalize:::recipe/aws-vod-more-like-x'):\n",
    "        recommender_more_like_x_name =recommender['serviceConfig']['name'] \n",
    "    if (recommender['serviceConfig']['recipeArn'] == 'arn:aws:personalize:::recipe/aws-vod-top-picks'):\n",
    "        recommender_top_picks_for_you_name =recommender['serviceConfig']['name']\n",
    "        \n",
    "for solution in parameters['solutions']:\n",
    "    # This is currently configured assuming only one solution of this type, if there are multiple \n",
    "    # solutions of the same type further configuration is needed.\n",
    "    if (solution['serviceConfig']['recipeArn'] == 'arn:aws:personalize:::recipe/aws-personalized-ranking'):\n",
    "        workshop_rerank_solution_name = solution['serviceConfig']['name'] \n",
    "        # This is currently configured assuming only one campaign, if there are multiple campaigns \n",
    "        # further configuration is needed.\n",
    "        workshop_rerank_campaign_name = solution['campaigns'][0]['serviceConfig']['name'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure we can use the SDK to interact with Amazon Personalize by describing some of the pre-created resources used in the workshop. If you have not pre-deployed resources and are building them as you go with this notebook, the below cell will raise an exception. You can continue with the notebook and create resources and train models as you go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some or all pretrained resources not found. Proceed to the next cell if you will be uploading data and training models as you go.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Describe a few resources using the SDK\n",
    "    more_like_x_arn = 'arn:aws:personalize:'+region+':'+account_id+':recommender/'+recommender_more_like_x_name \n",
    "    describe_response = personalize.describe_recommender(recommenderArn = more_like_x_arn)\n",
    "\n",
    "    top_picks_arn = 'arn:aws:personalize:'+region+':'+account_id+':recommender/'+recommender_top_picks_for_you_name\n",
    "    describe_response = personalize.describe_recommender(recommenderArn = top_picks_arn)\n",
    "\n",
    "    rerank_arn = 'arn:aws:personalize:'+region+':'+account_id+':solution/'+workshop_rerank_solution_name\n",
    "    describe_response = personalize.describe_solution(solutionArn = rerank_arn)\n",
    "    \n",
    "    print(\"All resources have been successfully pretrained!\")\n",
    "    \n",
    "except:\n",
    "    print(\"Some or all pretrained resources not found. Proceed to the next cell if you will be uploading data and training models as you go.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Amazon Personalize Datasets <a class=\"anchor\" id=\"datasets\"></a>\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of the use case, the algorithms all share a base of learning on user-item-interaction data which is defined by 3 core attributes:\n",
    "\n",
    "1. **UserID** - The user who interacted\n",
    "1. **ItemID** - The item the user interacted with\n",
    "1. **Timestamp** - The time at which the interaction occurred\n",
    "\n",
    "In this notebook we will be importing interactions, user and item data into your environment, inspecting it and converting it to a format that will allow you use it in Amazon Personalize to train models to get personalized recommendations.\n",
    "\n",
    "The following diagram shows the resources that we will create. with the section we are building  in this notebook highlighted in blue with a dashed outline.\n",
    "\n",
    "![Workflow](images/01_Data_Layer_Resources.jpg)\n",
    "\n",
    "Generally speaking your data will not arrive in a perfect form for Personalize, and will take some modification to be structured correctly. This notebook guides you through that process.\n",
    "\n",
    "### Items data\n",
    "\n",
    "The item data consists of information about the content that is being interacted with, this generally comes from Content Management Systems (CMS). \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> This dataset is not manatory for Amazon Personalize to generate recommendations, but providing good item metadata will ensure the best results in your trained models.\n",
    "</div>\n",
    "\n",
    "### Item Interactions data\n",
    "\n",
    "* Interations data: we use the ml-latest-small dataset from the [Movielens](https://grouplens.org/datasets/movielens/) project as a proxy for user-item interactions. \n",
    "\n",
    "The item interaction data concists of information about the interactions the users of the fictional app will have with the content. This usually comes from analytics tools or Customer Data Platform's (CDP). The best interaction data for use for Amazon Personalize would include the sequential order of user beavior, what content was watched/clicked on and the order it was interacted with. To simulate our interaction data, we will be using data from the [MovieLens project](https://grouplens.org/datasets/movielens/). Movielens offers multiple versions of their dataset, for the purposes of this workshop we will be using the reduced version of this dataset (approx 100,000 ratings and 3,600 tag applications applied to 9,000 movies by 600 users).\n",
    "\n",
    "This dataset is manatory for Amazon Personalize to generate recommendations.\n",
    "\n",
    "### User data\n",
    "\n",
    "The user data is what information you have about you users, it usually comes from Customer relationship management (CRM) or Subscriber management systems. Since there is no user data included in the MovieLens data, we will be generating a small synthetic dataset to simulate this component of the workshop. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> This dataset is not manatory for Amazon Personalize to generate recommendations, but providing good item metadata will ensure the best results in your trained models.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Item Metadata <a class=\"anchor\" id=\"prepare_items\"></a>\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order provide additional metadata, and also to provide a consistent experience for our users we leverage a subset of the IMDb Essential Metadata for Movies/TV/OTT dataset. IMDb is the world's most popular and authoritative source for information on movies, TV shows, and celebrities and powers entertainment experiences around the world. License IMDb entertainment metadata from over 10 million movies, TV series, and Video Game titles including 12 million cast and crew, 1 billion star ratings, and global box office grosses from Box Office Mojo. All IMDb data products are updated daily and easily accessed through AWS Data Exchange. \n",
    "\n",
    "The IMDb Essential Metadata for Movies/TV/OTT dataset, which contains \n",
    "\n",
    "- 9+ million titles\n",
    "- 12+ million names\n",
    "- Film, TV, music and celebrities\n",
    "- 1 billion ratings from the worldâ€™s largest entertainment fan community\n",
    "\n",
    "IMDb has [multiple datasets available in the Amazon Data Exchange](https://aws.amazon.com/marketplace/seller-profile?id=0af153a3-339f-48c2-8b42-3b9fa26d3367). <img src=\"./images/IMDb_Logo_Rectangle.png\" alt=\"IMDb logo\" style=\"width:50px;\"/>\n",
    "\n",
    "For this workshop we have extracted the subset of data we needed and prepared it for use with the following information from the IMDb Essential Metadata for Movies/TV/OTT (Bulk data) dataset.\n",
    "\n",
    "TITLE                      \n",
    "YEAR                       \n",
    "IMDB_RATING                \n",
    "IMDB_NUMBEROFVOTES         \n",
    "PLOT                       \n",
    "US_MATURITY_RATING_STRING  \n",
    "US_MATURITY_RATING         \n",
    "GENRES \n",
    "\n",
    "In addition we added two fields that will help us with our fictional use case that are not derived from the  IMDb dataset\n",
    "\n",
    "CREATION_TIMESTAMP         \n",
    "PROMOTION\n",
    "\n",
    "For the purpose of this workshop we will use the IMDb TT ID to provide a common identifier between the interactions data and the content metadata. Movielens provides its own identifier as well as a the IMDb TT ID (without the leading 'tt') in the 'links.csv' file. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b>Your use of IMDb data is for the sole purpose of completing the AWS workshop and/or tutorial. Any use of IMDb data outside of the AWS workshop and/or tutorial requires a data license from IMDb. To obtain a data license, please contact: imdb-licensing-support@imdb.com. You will not (and will not allow a third party to) (i) use IMDb data, or any derivative works thereof, for any purpose; (ii) copy, sublicense, rent, sell, lease or otherwise transfer or distribute IMDb data or any portion thereof to any person or entity for any purpose not permitted within the workshop and/or tutorial; (iii) decompile, disassemble, or otherwise reverse engineer or attempt to reconstruct or discover any source code or underlying ideas or algorithms of IMDb data by any means whatsoever; or (iv) knowingly remove any product identification, copyright or other notices from IMDb data.</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the subsection of IMDb item metadata to this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-27 21:58:01--  https://d2peeor3oplhc6.cloudfront.net/personalize-immersionday-media/datasets/items.csv\n",
      "Resolving d2peeor3oplhc6.cloudfront.net (d2peeor3oplhc6.cloudfront.net)... 108.138.61.83, 108.138.61.141, 108.138.61.221, ...\n",
      "Connecting to d2peeor3oplhc6.cloudfront.net (d2peeor3oplhc6.cloudfront.net)|108.138.61.83|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2320258 (2.2M) [text/csv]\n",
      "Saving to: â€˜poc_data/imdb/items.csv.4â€™\n",
      "\n",
      "100%[======================================>] 2,320,258   --.-K/s   in 0.02s   \n",
      "\n",
      "2024-04-27 21:58:01 (93.0 MB/s) - â€˜poc_data/imdb/items.csv.4â€™ saved [2320258/2320258]\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # copy IMDB data\n",
    "!wget -P $imdb_dir https://d2peeor3oplhc6.cloudfront.net/personalize-immersionday-media/datasets/items.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load the IMDB `items.csv` file and take a look at the first rows. This file has information about the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>IMDB_RATING</th>\n",
       "      <th>IMDB_NUMBEROFVOTES</th>\n",
       "      <th>PLOT</th>\n",
       "      <th>US_MATURITY_RATING_STRING</th>\n",
       "      <th>US_MATURITY_RATING</th>\n",
       "      <th>GENRES</th>\n",
       "      <th>CREATION_TIMESTAMP</th>\n",
       "      <th>PROMOTION</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ITEM_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tt0906325</th>\n",
       "      <td>Bagboy</td>\n",
       "      <td>2007</td>\n",
       "      <td>4</td>\n",
       "      <td>741</td>\n",
       "      <td>A teenager enters the competitive world of gro...</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>13</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>1167609600</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tt0906665</th>\n",
       "      <td>Sukiyaki Western Django</td>\n",
       "      <td>2007</td>\n",
       "      <td>6</td>\n",
       "      <td>14582</td>\n",
       "      <td>A nameless gunfighter arrives in a town ripped...</td>\n",
       "      <td>R</td>\n",
       "      <td>17</td>\n",
       "      <td>Action|Western</td>\n",
       "      <td>1167609600</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tt0907657</th>\n",
       "      <td>Once</td>\n",
       "      <td>2007</td>\n",
       "      <td>8</td>\n",
       "      <td>109787</td>\n",
       "      <td>A modern-day musical about a busker and an imm...</td>\n",
       "      <td>R</td>\n",
       "      <td>17</td>\n",
       "      <td>Drama|Music|Romance</td>\n",
       "      <td>1167609600</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tt0910905</th>\n",
       "      <td>In the Electric Mist</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>16592</td>\n",
       "      <td>A detective in post-Katrina New Orleans has a ...</td>\n",
       "      <td>R</td>\n",
       "      <td>17</td>\n",
       "      <td>Crime|Drama|Mystery|Thriller</td>\n",
       "      <td>1230768000</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tt0910554</th>\n",
       "      <td>Frequently Asked Questions About Time Travel</td>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "      <td>32051</td>\n",
       "      <td>While drinking at their local pub, three socia...</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>13</td>\n",
       "      <td>Comedy|Sci-Fi</td>\n",
       "      <td>1230768000</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  TITLE  YEAR  IMDB_RATING  \\\n",
       "ITEM_ID                                                                      \n",
       "tt0906325                                        Bagboy  2007            4   \n",
       "tt0906665                       Sukiyaki Western Django  2007            6   \n",
       "tt0907657                                          Once  2007            8   \n",
       "tt0910905                          In the Electric Mist  2009            6   \n",
       "tt0910554  Frequently Asked Questions About Time Travel  2009            7   \n",
       "\n",
       "           IMDB_NUMBEROFVOTES  \\\n",
       "ITEM_ID                         \n",
       "tt0906325                 741   \n",
       "tt0906665               14582   \n",
       "tt0907657              109787   \n",
       "tt0910905               16592   \n",
       "tt0910554               32051   \n",
       "\n",
       "                                                        PLOT  \\\n",
       "ITEM_ID                                                        \n",
       "tt0906325  A teenager enters the competitive world of gro...   \n",
       "tt0906665  A nameless gunfighter arrives in a town ripped...   \n",
       "tt0907657  A modern-day musical about a busker and an imm...   \n",
       "tt0910905  A detective in post-Katrina New Orleans has a ...   \n",
       "tt0910554  While drinking at their local pub, three socia...   \n",
       "\n",
       "          US_MATURITY_RATING_STRING  US_MATURITY_RATING  \\\n",
       "ITEM_ID                                                   \n",
       "tt0906325                     PG-13                  13   \n",
       "tt0906665                         R                  17   \n",
       "tt0907657                         R                  17   \n",
       "tt0910905                         R                  17   \n",
       "tt0910554                     PG-13                  13   \n",
       "\n",
       "                                 GENRES  CREATION_TIMESTAMP PROMOTION  \n",
       "ITEM_ID                                                                \n",
       "tt0906325                        Comedy          1167609600     false  \n",
       "tt0906665                Action|Western          1167609600     false  \n",
       "tt0907657           Drama|Music|Romance          1167609600     false  \n",
       "tt0910905  Crime|Drama|Mystery|Thriller          1230768000     false  \n",
       "tt0910554                 Comedy|Sci-Fi          1230768000     false  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_data = pd.read_csv(data_dir + '/imdb/items.csv', sep=',', dtype={'PROMOTION': \"string\"},index_col=0)\n",
    "item_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>IMDB_RATING</th>\n",
       "      <th>IMDB_NUMBEROFVOTES</th>\n",
       "      <th>US_MATURITY_RATING</th>\n",
       "      <th>CREATION_TIMESTAMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9298.000000</td>\n",
       "      <td>9298.000000</td>\n",
       "      <td>9.298000e+03</td>\n",
       "      <td>9298.000000</td>\n",
       "      <td>9.298000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1994.564207</td>\n",
       "      <td>6.639923</td>\n",
       "      <td>7.364282e+04</td>\n",
       "      <td>13.540977</td>\n",
       "      <td>7.751767e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18.575302</td>\n",
       "      <td>1.070418</td>\n",
       "      <td>1.472090e+05</td>\n",
       "      <td>4.647646</td>\n",
       "      <td>5.861922e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1902.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.900000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.145917e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1988.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.561000e+03</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.679936e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1999.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.227600e+04</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>9.151488e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2008.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.404900e+04</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.199146e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2018.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.303709e+06</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.514765e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              YEAR  IMDB_RATING  IMDB_NUMBEROFVOTES  US_MATURITY_RATING  \\\n",
       "count  9298.000000  9298.000000        9.298000e+03         9298.000000   \n",
       "mean   1994.564207     6.639923        7.364282e+04           13.540977   \n",
       "std      18.575302     1.070418        1.472090e+05            4.647646   \n",
       "min    1902.000000     1.000000        1.900000e+01            0.000000   \n",
       "25%    1988.000000     6.000000        7.561000e+03            8.000000   \n",
       "50%    1999.000000     7.000000        2.227600e+04           17.000000   \n",
       "75%    2008.000000     7.000000        7.404900e+04           17.000000   \n",
       "max    2018.000000    10.000000        2.303709e+06           18.000000   \n",
       "\n",
       "       CREATION_TIMESTAMP  \n",
       "count        9.298000e+03  \n",
       "mean         7.751767e+08  \n",
       "std          5.861922e+08  \n",
       "min         -2.145917e+09  \n",
       "25%          5.679936e+08  \n",
       "50%          9.151488e+08  \n",
       "75%          1.199146e+09  \n",
       "max          1.514765e+09  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not really tell us much about the dataset, so we will explore a bit more and look at the raw information. We can see that genres often appear in groups. That is fine for us as Personalize supports this structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9298 entries, tt0906325 to tt6911608\n",
      "Data columns (total 10 columns):\n",
      " #   Column                     Non-Null Count  Dtype \n",
      "---  ------                     --------------  ----- \n",
      " 0   TITLE                      9298 non-null   object\n",
      " 1   YEAR                       9298 non-null   int64 \n",
      " 2   IMDB_RATING                9298 non-null   int64 \n",
      " 3   IMDB_NUMBEROFVOTES         9298 non-null   int64 \n",
      " 4   PLOT                       9298 non-null   object\n",
      " 5   US_MATURITY_RATING_STRING  9298 non-null   object\n",
      " 6   US_MATURITY_RATING         9298 non-null   int64 \n",
      " 7   GENRES                     9298 non-null   object\n",
      " 8   CREATION_TIMESTAMP         9298 non-null   int64 \n",
      " 9   PROMOTION                  9298 non-null   string\n",
      "dtypes: int64(5), object(4), string(1)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "item_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our Catalog of titles that our service offers. We also have some movies that the UnicornFlix marketing department would like to ensure are promoted in our recommendations. Amazon Personalize has a feature that allows you to promote items into recommendations, and set the balance of promoted items vs recommendations (we will cover this in detail in `03_Inference_Layer.ipynb`. Since we are in Las Vegas, lets create a promotion for movies about or set in Las Vegas. First we will find the movies in our catalog that feature or are set in/about Las Vegas and set the metadata field to true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>IMDB_RATING</th>\n",
       "      <th>IMDB_NUMBEROFVOTES</th>\n",
       "      <th>PLOT</th>\n",
       "      <th>US_MATURITY_RATING_STRING</th>\n",
       "      <th>US_MATURITY_RATING</th>\n",
       "      <th>GENRES</th>\n",
       "      <th>CREATION_TIMESTAMP</th>\n",
       "      <th>PROMOTION</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ITEM_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tt0066995</th>\n",
       "      <td>Diamonds Are Forever</td>\n",
       "      <td>1971</td>\n",
       "      <td>7</td>\n",
       "      <td>94484</td>\n",
       "      <td>A diamond smuggling investigation leads James ...</td>\n",
       "      <td>PG</td>\n",
       "      <td>8</td>\n",
       "      <td>Action|Adventure|Thriller</td>\n",
       "      <td>31536000</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tt0475394</th>\n",
       "      <td>Smokin' Aces</td>\n",
       "      <td>2006</td>\n",
       "      <td>7</td>\n",
       "      <td>137003</td>\n",
       "      <td>When a Las Vegas performer-turned-snitch named...</td>\n",
       "      <td>R</td>\n",
       "      <td>17</td>\n",
       "      <td>Action|Comedy|Crime|Drama|Thriller</td>\n",
       "      <td>1136073600</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tt0435705</th>\n",
       "      <td>Next</td>\n",
       "      <td>2007</td>\n",
       "      <td>6</td>\n",
       "      <td>147254</td>\n",
       "      <td>A Las Vegas magician who can see into the futu...</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>13</td>\n",
       "      <td>Action|Sci-Fi|Thriller</td>\n",
       "      <td>1167609600</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tt0120434</th>\n",
       "      <td>Vegas Vacation</td>\n",
       "      <td>1997</td>\n",
       "      <td>6</td>\n",
       "      <td>43985</td>\n",
       "      <td>In the fourth outing for the vacation franchis...</td>\n",
       "      <td>PG</td>\n",
       "      <td>8</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>852076800</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tt0120669</th>\n",
       "      <td>Fear and Loathing in Las Vegas</td>\n",
       "      <td>1998</td>\n",
       "      <td>8</td>\n",
       "      <td>257029</td>\n",
       "      <td>An oddball journalist and his psychopathic law...</td>\n",
       "      <td>R</td>\n",
       "      <td>17</td>\n",
       "      <td>Adventure|Comedy|Drama</td>\n",
       "      <td>883612800</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tt1411697</th>\n",
       "      <td>The Hangover Part II</td>\n",
       "      <td>2011</td>\n",
       "      <td>6</td>\n",
       "      <td>459287</td>\n",
       "      <td>Two years after the bachelor party in Las Vega...</td>\n",
       "      <td>R</td>\n",
       "      <td>17</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>1293840000</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tt2231253</th>\n",
       "      <td>Wild Card</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>50527</td>\n",
       "      <td>When a Las Vegas bodyguard with lethal skills ...</td>\n",
       "      <td>R</td>\n",
       "      <td>17</td>\n",
       "      <td>Action|Crime|Drama|Thriller</td>\n",
       "      <td>1420070400</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tt2239832</th>\n",
       "      <td>Think Like a Man Too</td>\n",
       "      <td>2014</td>\n",
       "      <td>6</td>\n",
       "      <td>19623</td>\n",
       "      <td>All the couples are back for a wedding in Las ...</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>13</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "      <td>1388534400</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tt3450650</th>\n",
       "      <td>Paul Blart: Mall Cop 2</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>33796</td>\n",
       "      <td>After six years of keeping our malls safe, Pau...</td>\n",
       "      <td>PG</td>\n",
       "      <td>8</td>\n",
       "      <td>Action|Comedy|Crime|Family</td>\n",
       "      <td>1420070400</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tt1204975</th>\n",
       "      <td>Last Vegas</td>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>127303</td>\n",
       "      <td>Four friends take a break from their day-to-da...</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>13</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "      <td>1356998400</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    TITLE  YEAR  IMDB_RATING  \\\n",
       "ITEM_ID                                                        \n",
       "tt0066995            Diamonds Are Forever  1971            7   \n",
       "tt0475394                    Smokin' Aces  2006            7   \n",
       "tt0435705                            Next  2007            6   \n",
       "tt0120434                  Vegas Vacation  1997            6   \n",
       "tt0120669  Fear and Loathing in Las Vegas  1998            8   \n",
       "...                                   ...   ...          ...   \n",
       "tt1411697            The Hangover Part II  2011            6   \n",
       "tt2231253                       Wild Card  2015            6   \n",
       "tt2239832            Think Like a Man Too  2014            6   \n",
       "tt3450650          Paul Blart: Mall Cop 2  2015            4   \n",
       "tt1204975                      Last Vegas  2013            7   \n",
       "\n",
       "           IMDB_NUMBEROFVOTES  \\\n",
       "ITEM_ID                         \n",
       "tt0066995               94484   \n",
       "tt0475394              137003   \n",
       "tt0435705              147254   \n",
       "tt0120434               43985   \n",
       "tt0120669              257029   \n",
       "...                       ...   \n",
       "tt1411697              459287   \n",
       "tt2231253               50527   \n",
       "tt2239832               19623   \n",
       "tt3450650               33796   \n",
       "tt1204975              127303   \n",
       "\n",
       "                                                        PLOT  \\\n",
       "ITEM_ID                                                        \n",
       "tt0066995  A diamond smuggling investigation leads James ...   \n",
       "tt0475394  When a Las Vegas performer-turned-snitch named...   \n",
       "tt0435705  A Las Vegas magician who can see into the futu...   \n",
       "tt0120434  In the fourth outing for the vacation franchis...   \n",
       "tt0120669  An oddball journalist and his psychopathic law...   \n",
       "...                                                      ...   \n",
       "tt1411697  Two years after the bachelor party in Las Vega...   \n",
       "tt2231253  When a Las Vegas bodyguard with lethal skills ...   \n",
       "tt2239832  All the couples are back for a wedding in Las ...   \n",
       "tt3450650  After six years of keeping our malls safe, Pau...   \n",
       "tt1204975  Four friends take a break from their day-to-da...   \n",
       "\n",
       "          US_MATURITY_RATING_STRING  US_MATURITY_RATING  \\\n",
       "ITEM_ID                                                   \n",
       "tt0066995                        PG                   8   \n",
       "tt0475394                         R                  17   \n",
       "tt0435705                     PG-13                  13   \n",
       "tt0120434                        PG                   8   \n",
       "tt0120669                         R                  17   \n",
       "...                             ...                 ...   \n",
       "tt1411697                         R                  17   \n",
       "tt2231253                         R                  17   \n",
       "tt2239832                     PG-13                  13   \n",
       "tt3450650                        PG                   8   \n",
       "tt1204975                     PG-13                  13   \n",
       "\n",
       "                                       GENRES  CREATION_TIMESTAMP PROMOTION  \n",
       "ITEM_ID                                                                      \n",
       "tt0066995           Action|Adventure|Thriller            31536000      true  \n",
       "tt0475394  Action|Comedy|Crime|Drama|Thriller          1136073600      true  \n",
       "tt0435705              Action|Sci-Fi|Thriller          1167609600      true  \n",
       "tt0120434                              Comedy           852076800      true  \n",
       "tt0120669              Adventure|Comedy|Drama           883612800      true  \n",
       "...                                       ...                 ...       ...  \n",
       "tt1411697                              Comedy          1293840000      true  \n",
       "tt2231253         Action|Crime|Drama|Thriller          1420070400      true  \n",
       "tt2239832                      Comedy|Romance          1388534400      true  \n",
       "tt3450650          Action|Comedy|Crime|Family          1420070400      true  \n",
       "tt1204975                        Comedy|Drama          1356998400      true  \n",
       "\n",
       "[26 rows x 10 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = item_data['PLOT'].str.contains('las vegas', case=False, na=False)\n",
    "item_data.loc[mask, 'PROMOTION'] = 'true'\n",
    "item_metadata = item_data\n",
    "item_data[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets confirm that the changes we have made, have not introduced any null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TITLE                        0\n",
       "YEAR                         0\n",
       "IMDB_RATING                  0\n",
       "IMDB_NUMBEROFVOTES           0\n",
       "PLOT                         0\n",
       "US_MATURITY_RATING_STRING    0\n",
       "US_MATURITY_RATING           0\n",
       "GENRES                       0\n",
       "CREATION_TIMESTAMP           0\n",
       "PROMOTION                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, we currently have no null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! At this point the item data is ready to go, and we just need to save it as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "item_data.to_csv((data_dir+\"/\"+items_filename), index=True, float_format='%.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Item Interactions data \n",
    "\n",
    "First, you will download the dataset from the [MovieLens project](https://grouplens.org/datasets/movielens/) website and unzip it in a new folder using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-27 21:58:01--  http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 978202 (955K) [application/zip]\n",
      "Saving to: â€˜ml-latest-small.zip.7â€™\n",
      "\n",
      "100%[======================================>] 978,202     2.35MB/s   in 0.4s   \n",
      "\n",
      "2024-04-27 21:58:02 (2.35 MB/s) - â€˜ml-latest-small.zip.7â€™ saved [978202/978202]\n",
      "\n",
      "Archive:  ml-latest-small.zip\n",
      "  inflating: ml-latest-small/links.csv  \n",
      "  inflating: ml-latest-small/tags.csv  \n",
      "  inflating: ml-latest-small/ratings.csv  \n",
      "  inflating: ml-latest-small/README.txt  \n",
      "  inflating: ml-latest-small/movies.csv  \n"
     ]
    }
   ],
   "source": [
    "!cd $data_dir && wget http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
    "!cd $data_dir && unzip -o ml-latest-small.zip \n",
    "dataset_dir = data_dir + \"/ml-latest-small/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the data files you have downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "links.csv  movies.csv  ratings.csv  README.txt\ttags.csv\n"
     ]
    }
   ],
   "source": [
    "!ls $dataset_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the README.txt file and licensing, do not skip over usage license!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary\n",
      "=======\n",
      "\n",
      "This dataset (ml-latest-small) describes 5-star rating and free-text tagging activity from [MovieLens](http://movielens.org), a movie recommendation service. It contains 100836 ratings and 3683 tag applications across 9742 movies. These data were created by 610 users between March 29, 1996 and September 24, 2018. This dataset was generated on September 26, 2018.\n",
      "\n",
      "Users were selected at random for inclusion. All selected users had rated at least 20 movies. No demographic information is included. Each user is represented by an id, and no other information is provided.\n",
      "\n",
      "The data are contained in the files `links.csv`, `movies.csv`, `ratings.csv` and `tags.csv`. More details about the contents and use of all these files follows.\n",
      "\n",
      "This is a *development* dataset. As such, it may change over time and is not an appropriate dataset for shared research results. See available *benchmark* datasets if that is your intent.\n",
      "\n",
      "This and other GroupLens data sets are publicly available for download at <http://grouplens.org/datasets/>.\n",
      "\n",
      "\n",
      "Usage License\n",
      "=============\n",
      "\n",
      "Neither the University of Minnesota nor any of the researchers involved can guarantee the correctness of the data, its suitability for any particular purpose, or the validity of results based on the use of the data set. The data set may be used for any research purposes under the following conditions:\n",
      "\n",
      "* The user may not state or imply any endorsement from the University of Minnesota or the GroupLens Research Group.\n",
      "* The user must acknowledge the use of the data set in publications resulting from the use of the data set (see below for citation information).\n",
      "* The user may redistribute the data set, including transformations, so long as it is distributed under these same license conditions.\n",
      "* The user may not use this information for any commercial or revenue-bearing purposes without first obtaining permission from a faculty member of the GroupLens Research Project at the University of Minnesota.\n",
      "* The executable software scripts are provided \"as is\" without warranty of any kind, either expressed or implied, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose. The entire risk as to the quality and performance of them is with you. Should the program prove defective, you assume the cost of all necessary servicing, repair or correction.\n",
      "\n",
      "In no event shall the University of Minnesota, its affiliates or employees be liable to you for any damages arising out of the use or inability to use these programs (including but not limited to loss of data or data being rendered inaccurate).\n",
      "\n",
      "If you have any further questions or comments, please email <grouplens-info@umn.edu>\n",
      "\n",
      "\n",
      "Citation\n",
      "========\n",
      "\n",
      "To acknowledge use of the dataset in publications, please cite the following paper:\n",
      "\n",
      "> F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4: 19:1â€“19:19. <https://doi.org/10.1145/2827872>\n",
      "\n",
      "\n",
      "Further Information About GroupLens\n",
      "===================================\n",
      "\n",
      "GroupLens is a research group in the Department of Computer Science and Engineering at the University of Minnesota. Since its inception in 1992, GroupLens's research projects have explored a variety of fields including:\n",
      "\n",
      "* recommender systems\n",
      "* online communities\n",
      "* mobile and ubiquitious technologies\n",
      "* digital libraries\n",
      "* local geographic information systems\n",
      "\n",
      "GroupLens Research operates a movie recommender based on collaborative filtering, MovieLens, which is the source of these data. We encourage you to visit <http://movielens.org> to try it out! If you have exciting ideas for experimental work to conduct on MovieLens, send us an email at <grouplens-info@cs.umn.edu> - we are always interested in working with external collaborators.\n",
      "\n",
      "\n",
      "Content and Use of Files\n",
      "========================\n",
      "\n",
      "Formatting and Encoding\n",
      "-----------------------\n",
      "\n",
      "The dataset files are written as [comma-separated values](http://en.wikipedia.org/wiki/Comma-separated_values) files with a single header row. Columns that contain commas (`,`) are escaped using double-quotes (`\"`). These files are encoded as UTF-8. If accented characters in movie titles or tag values (e.g. MisÃ©rables, Les (1995)) display incorrectly, make sure that any program reading the data, such as a text editor, terminal, or script, is configured for UTF-8.\n",
      "\n",
      "\n",
      "User Ids\n",
      "--------\n",
      "\n",
      "MovieLens users were selected at random for inclusion. Their ids have been anonymized. User ids are consistent between `ratings.csv` and `tags.csv` (i.e., the same id refers to the same user across the two files).\n",
      "\n",
      "\n",
      "Movie Ids\n",
      "---------\n",
      "\n",
      "Only movies with at least one rating or tag are included in the dataset. These movie ids are consistent with those used on the MovieLens web site (e.g., id `1` corresponds to the URL <https://movielens.org/movies/1>). Movie ids are consistent between `ratings.csv`, `tags.csv`, `movies.csv`, and `links.csv` (i.e., the same id refers to the same movie across these four data files).\n",
      "\n",
      "\n",
      "Ratings Data File Structure (ratings.csv)\n",
      "-----------------------------------------\n",
      "\n",
      "All ratings are contained in the file `ratings.csv`. Each line of this file after the header row represents one rating of one movie by one user, and has the following format:\n",
      "\n",
      "    userId,movieId,rating,timestamp\n",
      "\n",
      "The lines within this file are ordered first by userId, then, within user, by movieId.\n",
      "\n",
      "Ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).\n",
      "\n",
      "Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n",
      "\n",
      "\n",
      "Tags Data File Structure (tags.csv)\n",
      "-----------------------------------\n",
      "\n",
      "All tags are contained in the file `tags.csv`. Each line of this file after the header row represents one tag applied to one movie by one user, and has the following format:\n",
      "\n",
      "    userId,movieId,tag,timestamp\n",
      "\n",
      "The lines within this file are ordered first by userId, then, within user, by movieId.\n",
      "\n",
      "Tags are user-generated metadata about movies. Each tag is typically a single word or short phrase. The meaning, value, and purpose of a particular tag is determined by each user.\n",
      "\n",
      "Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n",
      "\n",
      "\n",
      "Movies Data File Structure (movies.csv)\n",
      "---------------------------------------\n",
      "\n",
      "Movie information is contained in the file `movies.csv`. Each line of this file after the header row represents one movie, and has the following format:\n",
      "\n",
      "    movieId,title,genres\n",
      "\n",
      "Movie titles are entered manually or imported from <https://www.themoviedb.org/>, and include the year of release in parentheses. Errors and inconsistencies may exist in these titles.\n",
      "\n",
      "Genres are a pipe-separated list, and are selected from the following:\n",
      "\n",
      "* Action\n",
      "* Adventure\n",
      "* Animation\n",
      "* Children's\n",
      "* Comedy\n",
      "* Crime\n",
      "* Documentary\n",
      "* Drama\n",
      "* Fantasy\n",
      "* Film-Noir\n",
      "* Horror\n",
      "* Musical\n",
      "* Mystery\n",
      "* Romance\n",
      "* Sci-Fi\n",
      "* Thriller\n",
      "* War\n",
      "* Western\n",
      "* (no genres listed)\n",
      "\n",
      "\n",
      "Links Data File Structure (links.csv)\n",
      "---------------------------------------\n",
      "\n",
      "Identifiers that can be used to link to other sources of movie data are contained in the file `links.csv`. Each line of this file after the header row represents one movie, and has the following format:\n",
      "\n",
      "    movieId,imdbId,tmdbId\n",
      "\n",
      "movieId is an identifier for movies used by <https://movielens.org>. E.g., the movie Toy Story has the link <https://movielens.org/movies/1>.\n",
      "\n",
      "imdbId is an identifier for movies used by <http://www.imdb.com>. E.g., the movie Toy Story has the link <http://www.imdb.com/title/tt0114709/>.\n",
      "\n",
      "tmdbId is an identifier for movies used by <https://www.themoviedb.org>. E.g., the movie Toy Story has the link <https://www.themoviedb.org/movie/862>.\n",
      "\n",
      "Use of the resources listed above is subject to the terms of each provider.\n",
      "\n",
      "\n",
      "Cross-Validation\n",
      "----------------\n",
      "\n",
      "Prior versions of the MovieLens dataset included either pre-computed cross-folds or scripts to perform this computation. We no longer bundle either of these features with the dataset, since most modern toolkits provide this as a built-in feature. If you wish to learn about standard approaches to cross-fold computation in the context of recommender systems evaluation, see [LensKit](http://lenskit.org) for tools, documentation, and open-source code examples.\n"
     ]
    }
   ],
   "source": [
    "!pygmentize $data_dir/ml-latest-small/README.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary data we are interested in for a recommendation use case is the actual interactions that the users had with the titles(items). \n",
    "\n",
    "Open the `ratings.csv` file and take a look at the some rows from throughout the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5101</th>\n",
       "      <td>33</td>\n",
       "      <td>1027</td>\n",
       "      <td>3.0</td>\n",
       "      <td>939716819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5950</th>\n",
       "      <td>42</td>\n",
       "      <td>459</td>\n",
       "      <td>3.0</td>\n",
       "      <td>996220584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44748</th>\n",
       "      <td>298</td>\n",
       "      <td>3698</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1450452365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91495</th>\n",
       "      <td>593</td>\n",
       "      <td>53464</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1181007732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92437</th>\n",
       "      <td>597</td>\n",
       "      <td>2100</td>\n",
       "      <td>4.0</td>\n",
       "      <td>941641110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28592</th>\n",
       "      <td>199</td>\n",
       "      <td>296</td>\n",
       "      <td>4.0</td>\n",
       "      <td>940372462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9096</th>\n",
       "      <td>62</td>\n",
       "      <td>118696</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1523111252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>65</td>\n",
       "      <td>89774</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1494767219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59897</th>\n",
       "      <td>387</td>\n",
       "      <td>5013</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1094876931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68956</th>\n",
       "      <td>448</td>\n",
       "      <td>2018</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1019126704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId movieId  rating   timestamp\n",
       "5101       33    1027     3.0   939716819\n",
       "5950       42     459     3.0   996220584\n",
       "44748     298    3698     2.5  1450452365\n",
       "91495     593   53464     2.0  1181007732\n",
       "92437     597    2100     4.0   941641110\n",
       "28592     199     296     4.0   940372462\n",
       "9096       62  118696     4.0  1523111252\n",
       "9971       65   89774     4.0  1494767219\n",
       "59897     387    5013     4.0  1094876931\n",
       "68956     448    2018     4.0  1019126704"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction_data = pd.read_csv(dataset_dir + '/ratings.csv', sep=',', dtype={'userId': \"int64\", 'movieId': \"str\"})\n",
    "interaction_data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Amazon Personalize, you need to save timestamps in Unix Epoch format.\n",
    "\n",
    "Lets validate that the timestamp is actually in a Unix Epoch format by converting it into a more easily understood time/date format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp\n",
      "964982681\n",
      "\n",
      "Date & Time\n",
      "2000-07-30 18:44:41\n"
     ]
    }
   ],
   "source": [
    "arb_time_stamp = interaction_data.iloc[50]['timestamp']\n",
    "print('timestamp')\n",
    "print(arb_time_stamp)\n",
    "print()\n",
    "print('Date & Time')\n",
    "print(datetime.utcfromtimestamp(arb_time_stamp).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do some general summarization and inspection of the data to ensure that it will be helpful for Amazon Personalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId       False\n",
       "movieId      False\n",
       "rating       False\n",
       "timestamp    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction_data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100836 entries, 0 to 100835\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   userId     100836 non-null  int64  \n",
      " 1   movieId    100836 non-null  object \n",
      " 2   rating     100836 non-null  float64\n",
      " 3   timestamp  100836 non-null  int64  \n",
      "dtypes: float64(1), int64(2), object(1)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "interaction_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you can see is that the Movielens dataset is that this dataset contains a userId, a movieId, the rating that the user gave the movie and the time the made this interaction. For the purposes of our fictional service UnicornFlix this data will stand in for our applications interaction data, which would actually be the click stream data of the titles that were watched, in the order they watched them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the Interactions Data\n",
    "\n",
    "The interaction data generally is acquired from anaytics or CDP platforms that can identify individual interactions with content/items within a platform. \n",
    "\n",
    "We need to do a few things to get this dataset ready to subsitute for our services interaction data.\n",
    "\n",
    "First off, the movieId is a unique identifier provided by Movielens for each title. However as we saw above IMDb has a much richer set of metadata about the content catalog. In order to use the IMDb data we will need to use a common  identifier between our items and our interactions dataset, which is the IMDb imdbId. To do this Movielens provides the 'links.csv' file which helps convert between the two identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>imdbId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>tt0114709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>tt0113497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>tt0113228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>tt0114885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>tt0113041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9737</th>\n",
       "      <td>193581</td>\n",
       "      <td>tt5476944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9738</th>\n",
       "      <td>193583</td>\n",
       "      <td>tt5914996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9739</th>\n",
       "      <td>193585</td>\n",
       "      <td>tt6397426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9740</th>\n",
       "      <td>193587</td>\n",
       "      <td>tt8391976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9741</th>\n",
       "      <td>193609</td>\n",
       "      <td>tt0101726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9742 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     movieId     imdbId\n",
       "0          1  tt0114709\n",
       "1          2  tt0113497\n",
       "2          3  tt0113228\n",
       "3          4  tt0114885\n",
       "4          5  tt0113041\n",
       "...      ...        ...\n",
       "9737  193581  tt5476944\n",
       "9738  193583  tt5914996\n",
       "9739  193585  tt6397426\n",
       "9740  193587  tt8391976\n",
       "9741  193609  tt0101726\n",
       "\n",
       "[9742 rows x 2 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = pd.read_csv(dataset_dir + '/links.csv', sep=',', usecols=[0,1], encoding='latin-1', dtype={'movieId': \"str\", 'imdbId': \"str\", 'tmdbId': \"str\"})\n",
    "pd.set_option('display.max_rows', 25)\n",
    "links['imdbId'] = 'tt' + links['imdbId'].astype(object)\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this provides a method to identify what the IMDb id is for every title in our interactions dataset, now we will convert the ratings.csv data to utilize the IMDb ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>imdbId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "      <td>tt0114709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "      <td>tt0113228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "      <td>tt0113277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "      <td>tt0114369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "      <td>tt0114814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100831</th>\n",
       "      <td>610</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1493848402</td>\n",
       "      <td>tt4972582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100832</th>\n",
       "      <td>610</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493850091</td>\n",
       "      <td>tt4425200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100833</th>\n",
       "      <td>610</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1494273047</td>\n",
       "      <td>tt5052448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100834</th>\n",
       "      <td>610</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493846352</td>\n",
       "      <td>tt3315342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100835</th>\n",
       "      <td>610</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1493846415</td>\n",
       "      <td>tt4630562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100836 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  rating   timestamp     imdbId\n",
       "0            1     4.0   964982703  tt0114709\n",
       "1            1     4.0   964981247  tt0113228\n",
       "2            1     4.0   964982224  tt0113277\n",
       "3            1     5.0   964983815  tt0114369\n",
       "4            1     5.0   964982931  tt0114814\n",
       "...        ...     ...         ...        ...\n",
       "100831     610     4.0  1493848402  tt4972582\n",
       "100832     610     5.0  1493850091  tt4425200\n",
       "100833     610     5.0  1494273047  tt5052448\n",
       "100834     610     5.0  1493846352  tt3315342\n",
       "100835     610     3.0  1493846415  tt4630562\n",
       "\n",
       "[100836 rows x 4 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data = interaction_data.merge(links, on='movieId')\n",
    "imdb_data.drop(columns='movieId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a interactions dataset that matches our item catalog dataset. \n",
    "\n",
    "### Simulating a interaction dataset \n",
    "\n",
    "We are going to make one more modification to make the MoviesLens dataset more like the analytics data that a video streaming service would see in their interactions. MoviesLens is an explicit movie rating dataset, which means users are presented a movie and asked to give it a rating. For recommendation systems/personalization, the industry has moved on to using more implicit data. This is due to many reasons including low numbers of customers rating titles and customers tastes changing over time. Some of the benefits of implicit interaction data is that it is the actual behavior of all users and changes over time as their viewing behavior changes.\n",
    "\n",
    "To convert the explicit interaction MovieLens ratings dataset into our fictional streaming service UnicornFlix's implicit dataset we are going to create a synthetic dataset using the ratings in MovieLens. \n",
    "\n",
    "- Implicit interactions are inherently positive interactions so we will be dropping any rating that is below 2 stars \n",
    "- Ratings of 2 and 3 stars are neutral to slightly positive, we are going to create synthetic \"Click\" events to simulate a viewer clicking on a title in the UnicornFlix app\n",
    "- Ratings of 4 and 5 are overwhelmingly positive, we will use these to create synthetic \"Watch\" and \"Click\" events to simulate a viewer both clicking on a title and watching at least 80% of a title in the UnicornFlix app\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> This will be directionaly accurate, but is not a good substitute for actual temporal based interaction data, the order that viewers rated movies on the MovieLens website is not as good as the order of interactions on an actual Video On Demand Streaming app. For more information about the importance of the temporal interaction data see\n",
    "https://www.amazon.science/publications/temporal-contextual-recommendation-in-real-time\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>imdbId</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>EVENT_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>tt0114709</td>\n",
       "      <td>964982703</td>\n",
       "      <td>Watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tt0113228</td>\n",
       "      <td>964981247</td>\n",
       "      <td>Watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>tt0113277</td>\n",
       "      <td>964982224</td>\n",
       "      <td>Watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>tt0114369</td>\n",
       "      <td>964983815</td>\n",
       "      <td>Watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>tt0114814</td>\n",
       "      <td>964982931</td>\n",
       "      <td>Watch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId     imdbId  timestamp EVENT_TYPE\n",
       "0       1  tt0114709  964982703      Watch\n",
       "1       1  tt0113228  964981247      Watch\n",
       "2       1  tt0113277  964982224      Watch\n",
       "3       1  tt0114369  964983815      Watch\n",
       "4       1  tt0114814  964982931      Watch"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "watched_df = imdb_data.copy()\n",
    "watched_df = watched_df[watched_df['rating'] > 3]\n",
    "watched_df = watched_df[['userId', 'imdbId', 'timestamp']]\n",
    "watched_df['EVENT_TYPE']='Watch'\n",
    "watched_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>imdbId</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>EVENT_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>tt0114709</td>\n",
       "      <td>964982703</td>\n",
       "      <td>Click</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tt0113228</td>\n",
       "      <td>964981247</td>\n",
       "      <td>Click</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>tt0113277</td>\n",
       "      <td>964982224</td>\n",
       "      <td>Click</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>tt0114369</td>\n",
       "      <td>964983815</td>\n",
       "      <td>Click</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>tt0114814</td>\n",
       "      <td>964982931</td>\n",
       "      <td>Click</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId     imdbId  timestamp EVENT_TYPE\n",
       "0       1  tt0114709  964982703      Click\n",
       "1       1  tt0113228  964981247      Click\n",
       "2       1  tt0113277  964982224      Click\n",
       "3       1  tt0114369  964983815      Click\n",
       "4       1  tt0114814  964982931      Click"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicked_df = imdb_data.copy()\n",
    "clicked_df = clicked_df[clicked_df['rating'] > 1]\n",
    "clicked_df = clicked_df[['userId', 'imdbId', 'timestamp']]\n",
    "clicked_df['EVENT_TYPE']='Click'\n",
    "clicked_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interactions_df = clicked_df.copy()\n",
    "\n",
    "interactions_df = pd.concat([interactions_df, watched_df])\n",
    "interactions_df.sort_values(\"timestamp\", axis = 0, ascending = True, \n",
    "                 inplace = True, na_position ='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at what the new dataset looks like and ensure that the data reflects our fictional streaming services streaming analytics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>imdbId</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>EVENT_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66679</th>\n",
       "      <td>429</td>\n",
       "      <td>tt0112679</td>\n",
       "      <td>828124615</td>\n",
       "      <td>Watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66681</th>\n",
       "      <td>429</td>\n",
       "      <td>tt0109676</td>\n",
       "      <td>828124615</td>\n",
       "      <td>Click</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66719</th>\n",
       "      <td>429</td>\n",
       "      <td>tt0101414</td>\n",
       "      <td>828124615</td>\n",
       "      <td>Watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66718</th>\n",
       "      <td>429</td>\n",
       "      <td>tt0096895</td>\n",
       "      <td>828124615</td>\n",
       "      <td>Watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66717</th>\n",
       "      <td>429</td>\n",
       "      <td>tt0099348</td>\n",
       "      <td>828124615</td>\n",
       "      <td>Watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81477</th>\n",
       "      <td>514</td>\n",
       "      <td>tt3778644</td>\n",
       "      <td>1537674946</td>\n",
       "      <td>Click</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81336</th>\n",
       "      <td>514</td>\n",
       "      <td>tt0076729</td>\n",
       "      <td>1537757040</td>\n",
       "      <td>Click</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81335</th>\n",
       "      <td>514</td>\n",
       "      <td>tt0081529</td>\n",
       "      <td>1537757059</td>\n",
       "      <td>Click</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81092</th>\n",
       "      <td>514</td>\n",
       "      <td>tt0109508</td>\n",
       "      <td>1537799250</td>\n",
       "      <td>Watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81092</th>\n",
       "      <td>514</td>\n",
       "      <td>tt0109508</td>\n",
       "      <td>1537799250</td>\n",
       "      <td>Click</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158371 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId     imdbId   timestamp EVENT_TYPE\n",
       "66679     429  tt0112679   828124615      Watch\n",
       "66681     429  tt0109676   828124615      Click\n",
       "66719     429  tt0101414   828124615      Watch\n",
       "66718     429  tt0096895   828124615      Watch\n",
       "66717     429  tt0099348   828124615      Watch\n",
       "...       ...        ...         ...        ...\n",
       "81477     514  tt3778644  1537674946      Click\n",
       "81336     514  tt0076729  1537757040      Click\n",
       "81335     514  tt0081529  1537757059      Click\n",
       "81092     514  tt0109508  1537799250      Watch\n",
       "81092     514  tt0109508  1537799250      Click\n",
       "\n",
       "[158371 rows x 4 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Amazon Personalize has default column names for users, items, and timestamp. These default column names are `USER_ID`, `ITEM_ID`, `TIMESTAMP` and `EVENT_VALUE` for the [VIDEO_ON_DEMAND domain dataset](https://docs.aws.amazon.com/personalize/latest/dg/VIDEO-ON-DEMAND-datasets-and-schemas.html). The final modification to the dataset is to replace the existing column headers with the default headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interactions_df.rename(columns = {'userId':'USER_ID', 'imdbId':'ITEM_ID', \n",
    "                              'timestamp':'TIMESTAMP'}, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using a subset of the IMDB dataset for this workshop that has been cleaned to remove movies that don't have valid values for the metadata we are using in out ITEMs dataset (we'll work with this more in the net section), so we'll need to make sure we don't have any interactions that have IMDB movie ids that are not in our subset of the IMDB data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "movies = pd.read_csv(data_dir + '/imdb' + '/items.csv', sep=',', usecols=[0,1], encoding='latin-1', dtype={'movieId': \"str\", 'imdbId': \"str\", 'tmdbId': \"str\"})\n",
    "pd.set_option('display.max_rows', 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's compare the number of `ITEM_ID` unique keys in the IMDB data to the `ITEM_ID` unique keys in the interactions.  They should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ITEM_ID    9298\n",
       "TITLE      8981\n",
       "dtype: int64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.nunique(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of unique ITEM_IDs are not the same in the IMDB data and the interactions data, so we'll clean out the data points with ITEM_IDs that do not have item metadata from the interactions dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 157395 entries, 0 to 157394\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   USER_ID     157395 non-null  int64 \n",
      " 1   ITEM_ID     157395 non-null  object\n",
      " 2   TIMESTAMP   157395 non-null  int64 \n",
      " 3   EVENT_TYPE  157395 non-null  object\n",
      " 4   TITLE       157395 non-null  object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 6.0+ MB\n"
     ]
    }
   ],
   "source": [
    "interactions_df = interactions_df.merge(movies, on='ITEM_ID')\n",
    "interactions_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also drop the `TITLE` column as it is not required in the interactions dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 157395 entries, 0 to 157394\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   USER_ID     157395 non-null  int64 \n",
      " 1   ITEM_ID     157395 non-null  object\n",
      " 2   TIMESTAMP   157395 non-null  int64 \n",
      " 3   EVENT_TYPE  157395 non-null  object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "interactions_df = interactions_df.drop(columns=['TITLE'])\n",
    "interactions_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! At this point the data is ready to go, and we just need to save it as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interactions_df.to_csv((data_dir+\"/\"+interactions_filename), index=False, float_format='%.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the User Metadata <a class=\"anchor\" id=\"prepare_users\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "The dataset does not have any user metadata so we will create a synthetic metadata field that would be an example of the type of user metadata UnicornFlix may have in their CRM/Subcriber management system. This data will be used both for training of the models, but also can be used for inference filters, which will be covered in a later notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>610 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     USER_ID\n",
       "0        429\n",
       "1        107\n",
       "2        191\n",
       "3         99\n",
       "4         54\n",
       "..       ...\n",
       "605       77\n",
       "606       25\n",
       "607      596\n",
       "608      184\n",
       "609      331\n",
       "\n",
       "[610 rows x 1 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all unique user ids from the interaction dataset\n",
    "\n",
    "user_ids = interactions_df['USER_ID'].unique()\n",
    "user_data = pd.DataFrame()\n",
    "user_data[\"USER_ID\"]= user_ids\n",
    "user_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding User Metadata\n",
    "\n",
    "The current dataset does not contain additiona user information. For this example, we'll randomly assign a membership level. For Ad Supported models this could indicate premium vs ad supported.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> NOTE: This is a synthetic dataset and since it is randomly assigned, will be of little value to our mode, in a real world scenario this data would be accurate to the user data.\n",
    "</div>\n",
    "\n",
    "NOTE: This is a synthetic dataset and since it is randomly assigned, will be of little value to our mode, in a real world scenario this data would be accurate to the user data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER_ID</th>\n",
       "      <th>MEMBERLEVEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>429</td>\n",
       "      <td>gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>191</td>\n",
       "      <td>silver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99</td>\n",
       "      <td>gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>silver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>77</td>\n",
       "      <td>gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>25</td>\n",
       "      <td>gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>596</td>\n",
       "      <td>gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>184</td>\n",
       "      <td>gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>331</td>\n",
       "      <td>silver</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>610 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     USER_ID MEMBERLEVEL\n",
       "0        429        gold\n",
       "1        107        gold\n",
       "2        191      silver\n",
       "3         99        gold\n",
       "4         54      silver\n",
       "..       ...         ...\n",
       "605       77        gold\n",
       "606       25        gold\n",
       "607      596        gold\n",
       "608      184        gold\n",
       "609      331      silver\n",
       "\n",
       "[610 rows x 2 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_membership_levels = ['silver', 'gold']\n",
    "random = np.random.choice(possible_membership_levels, len(user_data.index), p=[0.5, 0.5])\n",
    "user_data[\"MEMBERLEVEL\"] = random\n",
    "user_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! At this point the data is ready to go, and we just need to save it as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving the data as a CSV file\n",
    "user_data.to_csv((data_dir+\"/\"+users_filename), index=False, float_format='%.0f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating Amazon Personalize Resources and Importing data <a class=\"anchor\" id=\"import\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure an S3 bucket and an IAM  role <a class=\"anchor\" id=\"bucket_role\"></a>\n",
    "\n",
    "So far, we have downloaded, manipulated, and saved the data onto the Amazon EBS instance attached to the instance running this Jupyter notebook.  \n",
    "\n",
    "By default, the Personalize service does not have permission to access the data we upload into  S3 buckets in our account. In order to grant access to the Amazon Personalize service to interact with our S3 Buckets, we need to set a Bucket Policy and create an IAM role that the Amazon Personalize service will assume. If you are running this notebook without also running the Pretrained cloud formation template in the root folder then you will need to grant this Notebook substantial permissions in order to be able to run the code correctly.\n",
    "\n",
    "If you are running this with the pretrained cloud formation template `PersonalizeIDPretrained.yaml` then the notebook will not need that level of permissioning and will obtain the permissions from the bucket created as part of the automation process for demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the metadata stored on the instance underlying this Amazon SageMaker notebook, to determine the region it is operating in. If you are using a Jupyter notebook outside of Amazon SageMaker, simply define the region as a string below. The Amazon S3 bucket needs to be in the same region as the Amazon Personalize resources we have been creating so far.\n",
    "\n",
    "First, let us get the current notebook region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "with open('/opt/ml/metadata/resource-metadata.json') as notebook_info:\n",
    "    data = json.load(notebook_info)\n",
    "    resource_arn = data['ResourceArn']\n",
    "    region = resource_arn.split(':')[3]\n",
    "\n",
    "# To use a different region use:\n",
    "# region = <your_region>\n",
    "\n",
    "print('region:', region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon S3 bucket names are globally unique. To create a unique bucket name, the code below will append the string `personalize-poc-media` to your AWS account number. Then it creates a bucket with this name in the region discovered in the previous cell. Note if you have already created a bucket as part of the cloud formation automation then the cell below will return information on that previously created bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket created as part of cloud formation template found\n",
      "bucket_name: test-stack-3-mlopsstack-1ksuxvta-personalizebucket-evtxjufeydvb\n"
     ]
    }
   ],
   "source": [
    "# Configure the SDK to SSM:\n",
    "ssm = boto3.client('ssm')\n",
    "s3 = boto3.client('s3')\n",
    "bucket_found = False\n",
    "try:\n",
    "    personalizes3bucket = ssm.get_parameter(Name='/cloudformation/personalize-s3-bucket', WithDecryption=False)\n",
    "    bucket_name = personalizes3bucket['Parameter']['Value']\n",
    "    print('Bucket created as part of cloud formation template found')\n",
    "    print('bucket_name:', bucket_name)\n",
    "    bucket_found=True\n",
    "except:\n",
    "    \n",
    "    account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "    bucket_name = account_id + \"-\" + region + \"-\" + \"personalize-poc-media\"\n",
    "\n",
    "    #getting existing buckets in the account\n",
    "    response = s3.list_buckets()\n",
    "\n",
    "    if bucket_name in [x['Name'] for x in response['Buckets']]:\n",
    "        print(\"The bucket already exists.\")\n",
    "    else:\n",
    "        if region == \"us-east-1\":\n",
    "            bucket_responese = s3.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            bucket_responese = s3.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "                )\n",
    "    print('bucket_name:', bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Amazon Personalize needs to be able to read the contents of your S3 bucket. The policy which enables personalize to access the contents of the S3 bucket is below.\n",
    "\n",
    "```python\n",
    "policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Id\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"personalize.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": [\n",
    "                \"s3:*Object\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::{}\".format(bucket_name),\n",
    "                \"arn:aws:s3:::{}/*\".format(bucket_name)\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "This S3 bucket policy allows Amazon Personalize to be able to read the contents of your S3 bucket. The code below creates and adds the policy to the bucket it created in the last step. If the bucket was created as part of the automation script then it merely shows the bucket that it created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy for bucket created as part of cloud formation template:\n",
      "{'Version': '2012-10-17', 'Id': 'PersonalizeS3BucketAccessPolicy', 'Statement': [{'Effect': 'Allow', 'Principal': {'Service': 'personalize.amazonaws.com'}, 'Action': ['s3:GetObject', 's3:PutObject', 's3:ListBucket'], 'Resource': ['arn:aws:s3:::test-stack-3-mlopsstack-1ksuxvta-personalizebucket-evtxjufeydvb', 'arn:aws:s3:::test-stack-3-mlopsstack-1ksuxvta-personalizebucket-evtxjufeydvb/*']}]}\n"
     ]
    }
   ],
   "source": [
    "if bucket_found:\n",
    "    bucket_current_policy = s3.get_bucket_policy(Bucket=bucket_name)['Policy']\n",
    "    print(\"Policy for bucket created as part of cloud formation template:\")\n",
    "    print(json.loads(bucket_current_policy))\n",
    "else:\n",
    "    policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Id\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Sid\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"personalize.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": [\n",
    "                    \"s3:GetObject\",\n",
    "                    \"s3:ListBucket\",\n",
    "                    \"s3:PutObject\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    \"arn:aws:s3:::{}\".format(bucket_name),\n",
    "                    \"arn:aws:s3:::{}/*\".format(bucket_name)\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    bucket_current_policy = None\n",
    "\n",
    "    try:\n",
    "        bucket_current_policy = s3.get_bucket_policy(Bucket=bucket_name)['Policy']\n",
    "\n",
    "    except s3.exceptions.from_code('NoSuchBucketPolicy') as e:    \n",
    "        print(\"There is no current Bucket Policy for bucket \" + bucket_name)\n",
    "\n",
    "    except Exception as e: \n",
    "        raise(e)\n",
    "\n",
    "    if (bucket_current_policy and policy == json.loads(bucket_current_policy)):\n",
    "        print (\"The policy is already associated with the S3 Bucket.\")\n",
    "    else:\n",
    "        print (\"Adding the policy to the bucket.\")\n",
    "        print(s3.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(policy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create an IAM role\n",
    "\n",
    "Amazon Personalize also needs the ability to assume roles in AWS in order to have the permissions to execute certain tasks. Let's create an IAM role and attach the required policies to it. The code below attaches broad policies. You should use [more restrictive, least-privilege policies for production applications](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-least-privilege). If you have run the cloud formation template the role will have been automatically created and we will simply obtain that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The policy arn:aws:iam::381491864570:policy/restrictedS3Access already exists.\n",
      "Using the existing policy\n",
      "Warning: role already exists: An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name 381491864570-PersonalizeS3-Immersion-Day already exists.\n",
      "IAM Role: arn:aws:iam::381491864570:role/381491864570-PersonalizeS3-Immersion-Day\n",
      "\n",
      "The policy arn:aws:iam::381491864570:policy/restrictedS3Access is already attached to this role.\n"
     ]
    }
   ],
   "source": [
    "if bucket_found:\n",
    "    print(\"Cloud formation template used - skipping creation of IAM role and needed policies for Personalize as they were already created as part of the automation script\")\n",
    "    role_arn_info = ssm.get_parameter(Name='/cloudformation/personalize-iam-role-arn', WithDecryption=False)\n",
    "    role_arn = role_arn_info['Parameter']['Value']\n",
    "    role_name = role_arn.split('/')[1]\n",
    "else:\n",
    "    iam = boto3.client(\"iam\")\n",
    "\n",
    "    role_name = account_id+\"-PersonalizeS3-Immersion-Day\"\n",
    "    assume_role_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                \"Service\": \"personalize.amazonaws.com\"\n",
    "              },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create policy\n",
    "\n",
    "    s3_access_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": {\n",
    "                \"Sid\" : \"myStatement\" ,\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Resource\": [\n",
    "                    \"arn:aws:s3:::{}\".format(bucket_name),\n",
    "                    \"arn:aws:s3:::{}/*\".format(bucket_name)\n",
    "                ],\n",
    "                \"Action\": \"s3:*\"\n",
    "            }\n",
    "    }\n",
    "\n",
    "    try: \n",
    "\n",
    "        policy_response = iam.create_policy(\n",
    "            PolicyName='restrictedS3Access',\n",
    "            PolicyDocument=json.dumps(s3_access_policy_document),\n",
    "            Description='Restricts access to only workshop S3 bucket'\n",
    "        )\n",
    "\n",
    "        s3_access_policy_arn = policy_response['Policy']['Arn']\n",
    "\n",
    "        print (\"s3_access_policy_arn:{}\".format(s3_access_policy_arn))\n",
    "    except:\n",
    "        s3_access_policy_arn = 'arn:aws:iam::{}:policy/restrictedS3Access'.format(account_id)\n",
    "        print ('The policy {} already exists.'.format(s3_access_policy_arn))\n",
    "        print ('Using the existing policy')\n",
    "\n",
    "\n",
    "    try:\n",
    "        create_role_response = iam.create_role(\n",
    "            RoleName = role_name,\n",
    "            AssumeRolePolicyDocument = json.dumps(assume_role_policy_document),\n",
    "        );\n",
    "        role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "\n",
    "        print (\"10s pause to allow role to be fully consistent.\")\n",
    "        time.sleep(10)\n",
    "\n",
    "    except iam.exceptions.EntityAlreadyExistsException as e:\n",
    "        print('Warning: role already exists:', e)\n",
    "        role_arn = iam.get_role(\n",
    "            RoleName = role_name\n",
    "        )[\"Role\"][\"Arn\"];\n",
    "\n",
    "    print('IAM Role: {}\\n'.format(role_arn))\n",
    "\n",
    "    # Attach the policy if it is not previously attached:\n",
    "    if (s3_access_policy_arn in [ x['PolicyArn'] for x in iam.list_attached_role_policies( RoleName = role_name)['AttachedPolicies']]):\n",
    "        print ('The policy {} is already attached to this role.'.format(s3_access_policy_arn))\n",
    "    else:\n",
    "        print (\"Attaching the role_policy: {}\".format(s3_access_policy_arn))\n",
    "        attach_response = iam.attach_role_policy(\n",
    "            RoleName = role_name,\n",
    "            PolicyArn = s3_access_policy_arn\n",
    "        );\n",
    "        print (\"30s pause to allow role to be fully consistent.\")\n",
    "        time.sleep(30)\n",
    "        print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3\n",
    "\n",
    "Now that your Amazon S3 bucket has been created, upload the CSV files of our 3 datasets (Item, Interaction and User).\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> NOTE: We will cover how to import real-time data in a future notebook..\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "media/interactions.csv already exists in the bucket test-stack-3-mlopsstack-1ksuxvta-personalizebucket-evtxjufeydvb\n",
      "media/items.csv already exists in the bucket test-stack-3-mlopsstack-1ksuxvta-personalizebucket-evtxjufeydvb\n",
      "media/users.csv already exists in the bucket test-stack-3-mlopsstack-1ksuxvta-personalizebucket-evtxjufeydvb\n"
     ]
    }
   ],
   "source": [
    "interactions_file_path = data_dir + \"/\" + interactions_filename\n",
    "\n",
    "try:\n",
    "    s3.get_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=\"media/interactions.csv\",\n",
    "    )\n",
    "    print(\"{} already exists in the bucket {}\".format(\"media/interactions.csv\", bucket_name))\n",
    "except s3.exceptions.NoSuchKey:\n",
    "    # Uploading the file if it does not already exist\n",
    "    boto3.Session().resource('s3').Bucket(bucket_name).Object(\"media/interactions.csv\").upload_file(interactions_file_path)\n",
    "    print(\"File {} uploaded to bucket {}\".format(\"media/interactions.csv\", bucket_name))\n",
    "\n",
    "items_file_path = data_dir + \"/\" + items_filename\n",
    "try:\n",
    "    s3.get_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=\"media/items.csv\",\n",
    "    )\n",
    "    print(\"{} already exists in the bucket {}\".format(\"media/items.csv\", bucket_name))\n",
    "except s3.exceptions.NoSuchKey:\n",
    "    # Uploading the file if it does not already exist     \n",
    "    boto3.Session().resource('s3').Bucket(bucket_name).Object(\"media/items.csv\").upload_file(items_file_path)\n",
    "    print(\"File {} uploaded to bucket {}\".format(\"media/items.csv\", bucket_name))\n",
    "\n",
    "users_file_path = data_dir + \"/\" + users_filename\n",
    "try:\n",
    "    s3.get_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=\"media/users.csv\",\n",
    "    )\n",
    "    print(\"{} already exists in the bucket {}\".format(\"media/users.csv\", bucket_name))\n",
    "except s3.exceptions.NoSuchKey:\n",
    "    # Uploading the file if it does not already exist\n",
    "    boto3.Session().resource('s3').Bucket(bucket_name).Object(\"media/users.csv\").upload_file(users_file_path)\n",
    "    print(\"File {} uploaded to bucket {}\".format(\"media/users.csv\", bucket_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this data in the next labs. In order to use this data we will store these variables so subsequent notebooks can use this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'dataset_dir' (str)\n",
      "Stored 'data_dir' (str)\n",
      "Stored 'interactions_filename' (str)\n",
      "Stored 'items_filename' (str)\n",
      "Stored 'users_filename' (str)\n",
      "Stored 'workshop_rerank_solution_name' (str)\n",
      "Stored 'workshop_rerank_campaign_name' (str)\n",
      "Stored 'recommender_more_like_x_name' (str)\n",
      "Stored 'recommender_top_picks_for_you_name' (str)\n",
      "Stored 'region' (str)\n",
      "Stored 'account_id' (str)\n",
      "Stored 'workshop_dataset_group_name' (str)\n",
      "Stored 'interactions_schema_name' (str)\n",
      "Stored 'items_schema_name' (str)\n",
      "Stored 'users_schema_name' (str)\n",
      "Stored 'interactions_dataset_name' (str)\n",
      "Stored 'items_dataset_name' (str)\n",
      "Stored 'users_dataset_name' (str)\n",
      "Stored 'items_import_job_name' (str)\n",
      "Stored 'interactions_import_job_name' (str)\n",
      "Stored 'users_import_job_name' (str)\n",
      "Stored 'bucket_name' (str)\n",
      "Stored 'role_name' (str)\n",
      "Stored 'role_arn' (str)\n"
     ]
    }
   ],
   "source": [
    "%store dataset_dir\n",
    "%store data_dir\n",
    "%store interactions_filename\n",
    "%store items_filename\n",
    "%store users_filename\n",
    "%store workshop_rerank_solution_name\n",
    "%store workshop_rerank_campaign_name\n",
    "%store recommender_more_like_x_name\n",
    "%store recommender_top_picks_for_you_name\n",
    "%store region\n",
    "%store account_id\n",
    "%store workshop_dataset_group_name\n",
    "%store interactions_schema_name\n",
    "%store items_schema_name\n",
    "%store users_schema_name\n",
    "%store interactions_dataset_name\n",
    "%store items_dataset_name\n",
    "%store users_dataset_name\n",
    "%store items_import_job_name\n",
    "%store interactions_import_job_name\n",
    "%store users_import_job_name\n",
    "%store bucket_name\n",
    "%store role_name\n",
    "%store role_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to the next notebook `Media_02_Data_Import`](Media_02_Data_Import.ipynb) to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
