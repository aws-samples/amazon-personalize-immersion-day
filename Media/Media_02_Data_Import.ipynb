{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Personalize Resources and Importing Data\n",
    "\n",
    "\n",
    "## Outline\n",
    "\n",
    "In this notebook, you will create a Dataset Group and import the data you prepared in the previous notebook [`01_Data_Preparation.ipynb`](01_Data_Preparation.ipynb)  to Amazon Personalize.\n",
    "\n",
    "To run this notebook, you need to have run [the previous notebook: `01_Data_Preparation.ipynb`](01_Data_Preparation.ipynb), where you prepared 3 datasets (item interactions, items and users).\n",
    "\n",
    "1. [Creating Amazon Personalize Resources and Importing data](#import)\n",
    "1. [Configure IAM role](#iam_role)\n",
    "1. [Create the Dataset Group](#group_dataset)\n",
    "1. [Create the Interactions Schema](#interact_schema)\n",
    "1. [Create the Items(Movies) Schema](#items_schema)\n",
    "1. [Create the Users Schema](#users_schema)\n",
    "1. [Import the interactions data](#import_interactions)\n",
    "1. [Import the Item Metadata](#import_items)\n",
    "1. [Import the User Metadata](#import_users)\n",
    "\n",
    "In this notebook we will be uploading data to Amazon Personalize.\n",
    "\n",
    "\n",
    "![Workflow](images/01_Data_Layer_Resources.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get variables from the precious notebook\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import time\n",
    "from time import sleep\n",
    "import json\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure the SDK to Personalize:\n",
    "personalize = boto3.client('personalize')\n",
    "personalize_runtime = boto3.client('personalize-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dataset Group <a class=\"anchor\" id=\"group_dataset\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "The highest level of isolation and abstraction with Amazon Personalize is a *dataset group*. Information stored within one of these dataset groups has no impact on any other dataset group or models created from one - they are completely isolated. This allows you to run many experiments and is part of how we keep your models private and fully trained only on your data. \n",
    "\n",
    "Before importing the data prepared earlier, there needs to be a dataset group and a dataset added to it that handles the interactions.\n",
    "\n",
    "Dataset groups can house the following types of information:\n",
    "\n",
    "* User-item-interactions\n",
    "* Event streams (real-time interactions)\n",
    "* User metadata\n",
    "* Item metadata\n",
    "\n",
    "We need to create the dataset group that will contain our three datasets.\n",
    "\n",
    "Your dataset group can be one of the following types:\n",
    "\n",
    "* A Domain dataset group, where you create preconfigured resources for different business domains and use cases, such as getting recommendations for similar videos (VIDEO_ON_DEMAND domain) or best selling items (ECOMMERCE domain). You choose your business domain, import your data, and create recommenders. You use recommenders in your application to get recommendations. Use a [Domain dataset group](https://docs.aws.amazon.com/personalize/latest/dg/domain-dataset-groups.html) if you have a video on demand or e-commerce application and want Amazon Personalize to find the best configurations for your use cases. If you start with a Domain dataset group, you can also add custom resources such as solutions with solution versions trained with recipes for custom use cases.\n",
    "\n",
    "\n",
    "* A [Custom dataset group](https://docs.aws.amazon.com/personalize/latest/dg/custom-dataset-groups.html), where you create configurable resources for custom use cases and batch recommendation workflows. You choose a recipe, train a solution version (model), and deploy the solution version with a campaign. You use a campaign in your application to get recommendations. Use a Custom dataset group if you don't have a video on demand or e-commerce application or want to configure and manage only custom resources, or want to get recommendations in a batch workflow. If you start with a Custom dataset group, you can't associate it with a domain later. Instead, create a new Domain dataset group.\n",
    "\n",
    "You can create and manage Domain dataset groups and Custom dataset groups with the AWS console, the AWS Command Line Interface (AWS CLI), or programmatically with the AWS SDKs.\n",
    "\n",
    "In this workshop we will be creating a domain dataset group.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> If you run these notebooks in your own account, Amazon SageMaker will create these resources, deploying these resources will take aprox. 90 minutes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will create a new dataset group with the name `personalize-poc-movielens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The the Dataset Group with dataset_group_arn = arn:aws:personalize:us-east-1:381491864570:dataset-group/personalize-immersion-day-media already exists\n",
      "\n",
      "We will be using the existing Dataset Group dataset_group_arn = arn:aws:personalize:us-east-1:381491864570:dataset-group/personalize-immersion-day-media\n"
     ]
    }
   ],
   "source": [
    "try:     \n",
    "    # Try to create the dataset group, this block with exectute fully if the dataset group does not exist yet\n",
    "    \n",
    "    create_dataset_group_response = personalize.create_dataset_group(\n",
    "        name = workshop_dataset_group_name,\n",
    "        domain='VIDEO_ON_DEMAND'\n",
    "    )\n",
    "\n",
    "    workshop_dataset_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "    print(json.dumps(create_dataset_group_response, indent=2))\n",
    "    print ('\\nCreating the Dataset Group with dataset_group_arn = {}'.format(workshop_dataset_group_arn))\n",
    "\n",
    "except personalize.exceptions.ResourceAlreadyExistsException as e:\n",
    "    # if the dataset group already exists, get the unique identifier workshop_dataset_group_arn \n",
    "    # from the existing resource\n",
    "    \n",
    "    workshop_dataset_group_arn = 'arn:aws:personalize:'+region+':'+account_id+':dataset-group/'+workshop_dataset_group_name \n",
    "    print ('\\nThe the Dataset Group with dataset_group_arn = {} already exists'.format(workshop_dataset_group_arn))\n",
    "    print ('\\nWe will be using the existing Dataset Group dataset_group_arn = {}'.format(workshop_dataset_group_arn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Wait for Dataset Group to have ACTIVE Status \n",
    "\n",
    "Before we can use the Dataset Group to create more resources below, it must be active. This can take a minute or two. Execute the cell below and wait for it to show the ACTIVE status. It checks the status of the dataset group every 30 seconds, up to a maximum of 3 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetGroup: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_group_response = personalize.describe_dataset_group(\n",
    "        datasetGroupArn = workshop_dataset_group_arn\n",
    "    )\n",
    "    status = describe_dataset_group_response[\"datasetGroup\"][\"status\"]\n",
    "    print(\"DatasetGroup: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Now that you have a dataset group, you can create a dataset for the interaction data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Interactions Schema <a class=\"anchor\" id=\"interact_schema\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Now that we've loaded and prepared our three datasets we'll configure the Amazon Personalize service to understand our data so that it can be used to train models for generating recommendations. Amazon Personalize requires a schema for each dataset, so it can map the columns in our CSVs to fields for model training. Each schema is declared in JSON using the [Apache Avro](https://avro.apache.org/) format. \n",
    "\n",
    "First, define a schema to tell Amazon Personalize what type of dataset you are uploading. There are several mandatory fields that are required in the schema, depending on the type of dataset. More detailed information can be found in the [documentation](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html).\n",
    "\n",
    "The interactions dataset has three required columns: `ITEM_ID`, `USER_ID`, and `TIMESTAMP`. The `TIMESTAMP` represents when the user interated with an item and must be expressed in Unix timestamp format (seconds). For this dataset we also have an `EVENT_TYPE` column. These must be defined in the same order in the schema as they appear in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The schema arn:aws:personalize:us-east-1:381491864570:schema/workshop_movielens_interactions_schema already exists.\n",
      "\n",
      "We will be using the existing Interactions Schema with workshop_interactions_schema_arn = arn:aws:personalize:us-east-1:381491864570:schema/workshop_movielens_interactions_schema\n"
     ]
    }
   ],
   "source": [
    "interactions_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Interactions\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"EVENT_TYPE\", # \"Watch\", \"Click\", etc.\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TIMESTAMP\",\n",
    "            \"type\": \"long\"\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Try to create the interactions dataset schema, this block with exectute fully \n",
    "    # if the interactions dataset schema does not exist yet\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = interactions_schema_name,\n",
    "        schema = json.dumps(interactions_schema),\n",
    "        domain='VIDEO_ON_DEMAND'\n",
    "    )\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "    workshop_interactions_schema_arn = create_schema_response['schemaArn']\n",
    "    print ('\\nCreating the Interactions Schema with workshop_interactions_schema_arn = {}'.format(workshop_interactions_schema_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    # if the interactions dataset schema already exists, get the unique identifier workshop_interactions_schema_arn\n",
    "    # from the existing resource \n",
    "    \n",
    "    workshop_interactions_schema_arn = 'arn:aws:personalize:'+region+':'+account_id+':schema/'+interactions_schema_name \n",
    "    print('The schema {} already exists.'.format(workshop_interactions_schema_arn))\n",
    "    print ('\\nWe will be using the existing Interactions Schema with workshop_interactions_schema_arn = {}'.format(workshop_interactions_schema_arn))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Interactions Dataset <a class=\"anchor\" id=\"interact_data\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "With a schema created, you can create a dataset within the dataset group. Note that this does not load the data yet, but creates a schema of what the data looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Interactions Dataset arn:aws:personalize:us-east-1:381491864570:dataset/personalize-immersion-day-media/INTERACTIONS already exists.\n",
      "\n",
      "We will be using the existing Interactions Dataset with workshop_interactions_dataset_arn = arn:aws:personalize:us-east-1:381491864570:dataset/personalize-immersion-day-media/INTERACTIONS\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Try to create the interactions dataset, this block with exectute fully \n",
    "    # if the interactions dataset does not exist yet\n",
    "    \n",
    "    dataset_type = 'INTERACTIONS'\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = interactions_dataset_name,\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = workshop_dataset_group_arn,\n",
    "        schemaArn = workshop_interactions_schema_arn\n",
    "    )\n",
    "\n",
    "    workshop_interactions_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "    print ('\\nCreating the Interactions Dataset with workshop_interactions_dataset_arn = {}'.format(workshop_interactions_dataset_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    # if the interactions dataset already exists, get the unique identifier workshop_interactions_dataset_arn \n",
    "    # from the existing resource \n",
    "    workshop_interactions_dataset_arn =  'arn:aws:personalize:'+region+':'+account_id+':dataset/'+workshop_dataset_group_name+'/INTERACTIONS'\n",
    "    print('The Interactions Dataset {} already exists.'.format(workshop_interactions_dataset_arn))\n",
    "    print ('\\nWe will be using the existing Interactions Dataset with workshop_interactions_dataset_arn = {}'.format(workshop_interactions_dataset_arn))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Items (Movies) Schema<a class=\"anchor\" id=\"items_schema\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "First, we define a schema to tell Amazon Personalize what type of dataset we are uploading. There are several reserved and mandatory keywords required in the schema, based on the type of dataset. More detailed information can be found in the [documentation](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html).\n",
    "\n",
    "Our item metadata data has the following columns: `ITEM_ID`, `TITLE`, `YEAR`, `IMDB_RATING`,`IMDB_NUMBEROFVOTES`,  `PLOT`, `US_MATURITY_RATING_STRING`, `US_MATURITY_RATING`,`GENRES`, `CREATION_TIMESTAMP`, and `PROMOTION` fields. These must be defined in the same order in the schema as they appear in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The schema arn:aws:personalize:us-east-1:381491864570:schema/workshop_movielens_items_schema already exists.\n",
      "\n",
      "We will be using the existing Items Schema with workshop_items_schema_arn = arn:aws:personalize:us-east-1:381491864570:schema/workshop_movielens_items_schema\n"
     ]
    }
   ],
   "source": [
    "items_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Items\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TITLE\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"YEAR\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"IMDB_RATING\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"IMDB_NUMBEROFVOTES\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"PLOT\",\n",
    "            \"type\": \"string\",\n",
    "            \"textual\": True\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"US_MATURITY_RATING_STRING\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"US_MATURITY_RATING\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"GENRES\",\n",
    "            \"type\": \"string\",\n",
    "            \"categorical\": True\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"CREATION_TIMESTAMP\",\n",
    "            \"type\": \"long\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"PROMOTION\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Try to create the items dataset schema, this block with exectute fully \n",
    "    # if the items dataset schema does not exist yet\n",
    "    \n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = items_schema_name,\n",
    "        schema = json.dumps(items_schema),\n",
    "        domain='VIDEO_ON_DEMAND'\n",
    "    )\n",
    "    workshop_items_schema_arn = create_schema_response['schemaArn']\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "\n",
    "    print ('\\nCreating the Items Schema with workshop_items_schema_arn = {}'.format(workshop_items_schema_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    # if the items dataset schema already exists, get the unique identifier workshop_items_schema_arn \n",
    "    # from the existing resource \n",
    "    \n",
    "    workshop_items_schema_arn = 'arn:aws:personalize:'+region+':'+account_id+':schema/'+items_schema_name \n",
    "    print('The schema {} already exists.'.format(workshop_items_schema_arn))\n",
    "    print ('\\nWe will be using the existing Items Schema with workshop_items_schema_arn = {}'.format(workshop_items_schema_arn))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Items Dataset\n",
    "\n",
    "With a schema created, you can create a dataset within the dataset group. Note that this does not load the data yet, but creates a schema of what the data looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Items Dataset arn:aws:personalize:us-east-1:381491864570:dataset/personalize-immersion-day-media/ITEMS already exists.\n",
      "\n",
      "We will be using the existing Items Dataset with workshop_items_dataset_arn = arn:aws:personalize:us-east-1:381491864570:dataset/personalize-immersion-day-media/ITEMS\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Try to create the items dataset, this block with exectute fully if the items dataset does not exist yet\n",
    "    \n",
    "    dataset_type = \"ITEMS\"\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = items_dataset_name,\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = workshop_dataset_group_arn,\n",
    "        schemaArn = workshop_items_schema_arn\n",
    "    )\n",
    "\n",
    "    workshop_items_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "\n",
    "    print ('\\nCreating the Items Dataset with workshop_items_dataset_arn = {}'.format(workshop_items_dataset_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    # if the items dataset already exists, get the unique identifier workshop_items_dataset_arn \n",
    "    # from the existing resource \n",
    "    \n",
    "    workshop_items_dataset_arn =  'arn:aws:personalize:'+region+':'+account_id+':dataset/'+workshop_dataset_group_name+'/ITEMS'\n",
    "    print('The Items Dataset {} already exists.'.format(workshop_items_dataset_arn))\n",
    "    print ('\\nWe will be using the existing Items Dataset with workshop_items_dataset_arn = {}'.format(workshop_items_dataset_arn))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Users Schema<a class=\"anchor\" id=\"users_schema\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "First, define a schema to tell Amazon Personalize what type of dataset you are uploading. There are several reserved and mandatory keywords required in the schema, based on the type of dataset. More detailed information can be found in the [documentation](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html).\n",
    "\n",
    "Here, you will create a schema for user data, which requires the `USER_ID`, and an additonal metadata field, in this case `GENDER`. These must be defined in the same order in the schema as they appear in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The schema arn:aws:personalize:us-east-1:381491864570:schema/workshop_movielens_users_schema already exists.\n",
      "\n",
      "We will be using the existing Users Schema with workshop_users_schema_arn = arn:aws:personalize:us-east-1:381491864570:schema/workshop_movielens_users_schema\n"
     ]
    }
   ],
   "source": [
    "users_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Users\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"MEMBERLEVEL\",\n",
    "            \"type\": \"string\",\n",
    "            \"categorical\": True\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "    \n",
    "try:\n",
    "    # Try to create the users dataset schema, this block with exectute fully \n",
    "    # if the users dataset schema does not exist yet\n",
    "    \n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = users_schema_name,\n",
    "        schema = json.dumps(users_schema),\n",
    "        domain='VIDEO_ON_DEMAND'\n",
    "    )\n",
    "    \n",
    "    workshop_users_schema_arn = create_schema_response['schemaArn']\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "\n",
    "    print ('\\nCreating the Users Schema with workshop_users_schema_arn = {}'.format(workshop_users_schema_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    # if the users dataset schema already exists, get the unique identifier workshop_users_schema_arn \n",
    "    # from the existing resource \n",
    "\n",
    "    workshop_users_schema_arn = 'arn:aws:personalize:'+region+':'+account_id+':schema/'+users_schema_name \n",
    "    print('The schema {} already exists.'.format(workshop_users_schema_arn))\n",
    "    print ('\\nWe will be using the existing Users Schema with workshop_users_schema_arn = {}'.format(workshop_users_schema_arn))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Users dataset\n",
    "\n",
    "With a schema created, you can create a dataset within the dataset group. Note that this does not load the data yet, but creates a schema of what the data looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Users Dataset arn:aws:personalize:us-east-1:381491864570:dataset/personalize-immersion-day-media/USERS already exists.\n",
      "\n",
      "We will be using the existing Users Dataset with workshop_users_dataset_arn = arn:aws:personalize:us-east-1:381491864570:dataset/personalize-immersion-day-media/USERS\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Try to create the users dataset, this block with exectute fully if the users dataset does not exist yet\n",
    "    \n",
    "    dataset_type = \"USERS\"\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = users_dataset_name,\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = workshop_dataset_group_arn,\n",
    "        schemaArn = workshop_users_schema_arn\n",
    "    )\n",
    "\n",
    "    workshop_users_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "\n",
    "    print ('\\nCreating the Users Dataset with workshop_users_dataset_arn = {}'.format(workshop_users_dataset_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    # if the users dataset already exists, get the unique identifier workshop_users_dataset_arn\n",
    "    # from the existing resource \n",
    "    \n",
    "    workshop_users_dataset_arn =  'arn:aws:personalize:'+region+':'+account_id+':dataset/'+workshop_dataset_group_name+'/USERS'\n",
    "    print('The Users Dataset {} already exists.'.format(workshop_users_dataset_arn))\n",
    "    print ('\\nWe will be using the existing Users Dataset with workshop_users_dataset_arn = {}'.format(workshop_users_dataset_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wait until all the datasets have been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactions Dataset: ACTIVE\n",
      "Build succeeded for arn:aws:personalize:us-east-1:381491864570:dataset/personalize-immersion-day-media/INTERACTIONS\n",
      "The interaction dataset  is ACTIVE\n",
      "Items Dataset: ACTIVE\n",
      "Build succeeded for arn:aws:personalize:us-east-1:381491864570:dataset/personalize-immersion-day-media/ITEMS\n",
      "The item dataset  is ACTIVE\n",
      "Users Dataset: ACTIVE\n",
      "Build succeeded for arn:aws:personalize:us-east-1:381491864570:dataset/personalize-immersion-day-media/USERS\n",
      "The user dataset  is ACTIVE\n",
      "CPU times: user 8.15 ms, sys: 432 µs, total: 8.58 ms\n",
      "Wall time: 48.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "max_time = time.time() + 6*60*60 # 6 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_response = personalize.describe_dataset(\n",
    "        datasetArn = workshop_interactions_dataset_arn\n",
    "    )\n",
    "    status_interaction_dataset =  describe_dataset_response[\"dataset\"]['status']\n",
    "    print(\"Interactions Dataset: {}\".format(status_interaction_dataset))\n",
    "    \n",
    "    if status_interaction_dataset == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(workshop_interactions_dataset_arn))\n",
    "        \n",
    "    elif status_interaction_dataset == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(workshop_interactions_dataset_arn))\n",
    "        break\n",
    "        \n",
    "    if not status_interaction_dataset == \"ACTIVE\":\n",
    "        print(\"The interaction dataset creation is still in progress\")\n",
    "    else:\n",
    "        print(\"The interaction dataset  is ACTIVE\")\n",
    "        \n",
    "\n",
    "    describe_dataset_response = personalize.describe_dataset(\n",
    "        datasetArn = workshop_items_dataset_arn\n",
    "    )\n",
    "    status_item_dataset =  describe_dataset_response[\"dataset\"]['status']\n",
    "    print(\"Items Dataset: {}\".format(status_item_dataset))\n",
    "    \n",
    "    if status_item_dataset == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(workshop_items_dataset_arn))\n",
    "        \n",
    "    elif status_item_dataset == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(workshop_items_dataset_arn))\n",
    "        break\n",
    "        \n",
    "    if not status_item_dataset == \"ACTIVE\":\n",
    "        print(\"The item dataset creation is still in progress\")\n",
    "    else:\n",
    "        print(\"The item dataset  is ACTIVE\")\n",
    "    \n",
    "    describe_dataset_response = personalize.describe_dataset(\n",
    "        datasetArn = workshop_users_dataset_arn\n",
    "    )\n",
    "    status_user_dataset =  describe_dataset_response[\"dataset\"]['status']\n",
    "    print(\"Users Dataset: {}\".format(status_user_dataset))\n",
    "    \n",
    "    if status_user_dataset == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(workshop_users_dataset_arn))\n",
    "        \n",
    "    elif status_user_dataset == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(workshop_users_dataset_arn))\n",
    "        break\n",
    "        \n",
    "    if not status_user_dataset == \"ACTIVE\":\n",
    "        print(\"The user dataset creation is still in progress\")\n",
    "    else:\n",
    "        print(\"The user dataset  is ACTIVE\")\n",
    "    \n",
    "    if status_interaction_dataset == \"ACTIVE\" and status_item_dataset == \"ACTIVE\" and status_user_dataset == 'ACTIVE':\n",
    "        break\n",
    "        \n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Item Metadata <a class=\"anchor\" id=\"import_items\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Earlier you created the dataset group and dataset to house your information, now you will execute an import job that will load the item data from the S3 bucket into the Amazon Personalize dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:us-east-1:381491864570:dataset-import-job/media_dataset_import_item\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"7692bf55-506b-441c-8597-fe7247206d1e\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Sat, 27 Apr 2024 21:58:50 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"113\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"7692bf55-506b-441c-8597-fe7247206d1e\",\n",
      "      \"strict-transport-security\": \"max-age=47304000; includeSubDomains\",\n",
      "      \"x-frame-options\": \"DENY\",\n",
      "      \"cache-control\": \"no-cache\",\n",
      "      \"x-content-type-options\": \"nosniff\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "Importing the Items Data with workshop_items_dataset_import_job_arn = arn:aws:personalize:us-east-1:381491864570:dataset-import-job/media_dataset_import_item\n"
     ]
    }
   ],
   "source": [
    "# Checking if the import job already exists\n",
    "\n",
    "# List the import jobs\n",
    "items_dataset_import_jobs = personalize.list_dataset_import_jobs(\n",
    "    datasetArn=workshop_items_dataset_arn,\n",
    "    maxResults=100\n",
    ")['datasetImportJobs']\n",
    "\n",
    "job_exists = False\n",
    "job_arn = None\n",
    "\n",
    "#check if there is an existing job with the prefix\n",
    "for job in items_dataset_import_jobs:\n",
    "    if (items_import_job_name in job['jobName']):\n",
    "        job_exists = True\n",
    "        job_arn = job['datasetImportJobArn']\n",
    "    \n",
    "if (job_exists):\n",
    "    workshop_items_dataset_import_job_arn =  job_arn\n",
    "    print('The Items Import Job {} already exists.'.format(workshop_items_dataset_import_job_arn))\n",
    "    print ('\\nWe will be using the existing Items Import Job with workshop_items_dataset_import_job_arn = {}'.format(workshop_items_dataset_import_job_arn))\n",
    "        \n",
    "else:\n",
    "    # If there is no import job with the prefix, create it:    \n",
    "    create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "        jobName = items_import_job_name,\n",
    "        datasetArn = workshop_items_dataset_arn,\n",
    "        dataSource = {\n",
    "            \"dataLocation\": \"s3://{}/{}\".format(bucket_name, \"media/items.csv\")\n",
    "        },\n",
    "        roleArn = role_arn\n",
    "    )\n",
    "\n",
    "    workshop_items_dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "    print(json.dumps(create_dataset_import_job_response, indent=2))\n",
    "    print ('\\nImporting the Items Data with workshop_items_dataset_import_job_arn = {}'.format(workshop_items_dataset_import_job_arn))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the interactions data <a class=\"anchor\" id=\"import_interactions\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Earlier you created the dataset group and dataset to house your information, so now you will execute an import job that will load the interactions data from the S3 bucket into the Amazon Personalize dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:us-east-1:381491864570:dataset-import-job/media_dataset_import_interaction\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"7cbf4291-8a17-4a3c-acca-644777be8c5c\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Sat, 27 Apr 2024 21:58:51 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"120\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"7cbf4291-8a17-4a3c-acca-644777be8c5c\",\n",
      "      \"strict-transport-security\": \"max-age=47304000; includeSubDomains\",\n",
      "      \"x-frame-options\": \"DENY\",\n",
      "      \"cache-control\": \"no-cache\",\n",
      "      \"x-content-type-options\": \"nosniff\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "Importing the Interactions Data with workshop_interactions_dataset_import_job_arn = arn:aws:personalize:us-east-1:381491864570:dataset-import-job/media_dataset_import_interaction\n"
     ]
    }
   ],
   "source": [
    "# Check if the import job already exists\n",
    "\n",
    "# List the import jobs\n",
    "interactions_dataset_import_jobs = personalize.list_dataset_import_jobs(\n",
    "    datasetArn=workshop_interactions_dataset_arn,\n",
    "    maxResults=100\n",
    ")['datasetImportJobs']\n",
    "\n",
    "#check if there is an existing job with the prefix\n",
    "job_exists = False  \n",
    "job_arn = None\n",
    "\n",
    "for job in interactions_dataset_import_jobs:\n",
    "    if (interactions_import_job_name in job['jobName']):\n",
    "        job_exists = True\n",
    "        job_arn = job['datasetImportJobArn']\n",
    "    \n",
    "if (job_exists):\n",
    "    workshop_interactions_dataset_import_job_arn = job_arn\n",
    "    print('The Interactions Import Job {} already exists.'.format(workshop_interactions_dataset_import_job_arn))\n",
    "    print ('\\nWe will be using the existing Interactions Import Job with workshop_interactions_dataset_import_job_arn = {}'.format(workshop_interactions_dataset_import_job_arn))\n",
    "        \n",
    "else:\n",
    "    # If there is no import job with the prefix, create it:   \n",
    "    create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "        jobName = interactions_import_job_name,\n",
    "        datasetArn = workshop_interactions_dataset_arn,\n",
    "        dataSource = {\n",
    "            \"dataLocation\": \"s3://{}/{}\".format(bucket_name, \"media/interactions.csv\")\n",
    "        },\n",
    "        roleArn = role_arn\n",
    "    )\n",
    "    workshop_interactions_dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "    print(json.dumps(create_dataset_import_job_response, indent=2))\n",
    "    \n",
    "    print ('\\nImporting the Interactions Data with workshop_interactions_dataset_import_job_arn = {}'.format(workshop_interactions_dataset_import_job_arn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the User Metadata <a class=\"anchor\" id=\"import_users\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Earlier you created the dataset group and dataset to house your information, now you will execute an import job that will load the user data from the S3 bucket into the Amazon Personalize dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:us-east-1:381491864570:dataset-import-job/media_dataset_import_user\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"5f3d2e9e-aedb-4271-b1e2-86453ec9dc36\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Sat, 27 Apr 2024 21:58:51 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"113\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"5f3d2e9e-aedb-4271-b1e2-86453ec9dc36\",\n",
      "      \"strict-transport-security\": \"max-age=47304000; includeSubDomains\",\n",
      "      \"x-frame-options\": \"DENY\",\n",
      "      \"cache-control\": \"no-cache\",\n",
      "      \"x-content-type-options\": \"nosniff\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "Importing the Users Data with workshop_users_dataset_import_job_arn = arn:aws:personalize:us-east-1:381491864570:dataset-import-job/media_dataset_import_user\n"
     ]
    }
   ],
   "source": [
    "# Checking if the import job already exists\n",
    "\n",
    "# List the import jobs\n",
    "users_dataset_import_jobs = personalize.list_dataset_import_jobs(\n",
    "    datasetArn=workshop_users_dataset_arn,\n",
    "    maxResults=100\n",
    ")['datasetImportJobs']\n",
    "\n",
    "#check if there is an existing job with the prefix\n",
    "job_exists = False \n",
    "job_arn = None      \n",
    "for job in users_dataset_import_jobs:\n",
    "    if (users_import_job_name in job['jobName']):\n",
    "        job_exists = True\n",
    "        job_arn = job['datasetImportJobArn']\n",
    "\n",
    "if (job_exists):\n",
    "    workshop_users_dataset_import_job_arn =  job_arn\n",
    "    print('The Users Import Job {} already exists.'.format(workshop_users_dataset_import_job_arn))\n",
    "    print ('\\nWe will be using the existing Users Import Job with workshop_users_dataset_import_job_arn = {}'.format(workshop_users_dataset_import_job_arn))\n",
    "        \n",
    "else:\n",
    "    # If there is no import job with the prefix, create it:  \n",
    "    create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "        jobName = users_import_job_name,\n",
    "        datasetArn = workshop_users_dataset_arn,\n",
    "        dataSource = {\n",
    "            \"dataLocation\": \"s3://{}/{}\".format(bucket_name, \"media/users.csv\")\n",
    "        },\n",
    "        roleArn = role_arn\n",
    "    )\n",
    "\n",
    "    workshop_users_dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "    print(json.dumps(create_dataset_import_job_response, indent=2))\n",
    "    \n",
    "    print ('\\nImporting the Users Data with workshop_users_dataset_import_job_arn = {}'.format(workshop_users_dataset_import_job_arn))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use the dataset, the import job must be active. Execute the cell below and wait for it to show the ACTIVE status. It checks the status of the import job every minute, up to a maximum of 6 hours.\n",
    "\n",
    "Importing the data can take some time, depending on the size of the dataset. In this workshop, the data import job has already been done for you. If you are not using the workshop environment, this should take around 15 minutes. While you're waiting you can learn more about Datasets and Schemas in [the documentation](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html). \n",
    "\n",
    "We need to wait for the data imports to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The interactions dataset import is still in progress\n",
      "The items dataset import is still in progress\n",
      "The user dataset import is still in progress\n",
      "\n",
      "The interactions dataset import is still in progress\n",
      "The items dataset import is still in progress\n",
      "The user dataset import is still in progress\n",
      "\n",
      "The interactions dataset import is still in progress\n",
      "The items dataset import is still in progress\n",
      "The user dataset import is still in progress\n",
      "\n",
      "The interactions dataset import is still in progress\n",
      "The items dataset import is still in progress\n",
      "The user dataset import is still in progress\n",
      "\n",
      "The interactions dataset import is still in progress\n",
      "The items dataset import is still in progress\n",
      "The user dataset import is still in progress\n",
      "\n",
      "The interactions dataset import is still in progress\n",
      "The items dataset import is still in progress\n",
      "The user dataset import is still in progress\n",
      "\n",
      "The interactions dataset import is still in progress\n",
      "The items dataset import is still in progress\n",
      "The user dataset import is still in progress\n",
      "\n",
      "The interactions dataset import is still in progress\n",
      "The items dataset import is still in progress\n",
      "The user dataset import is still in progress\n",
      "\n",
      "Build succeeded for arn:aws:personalize:us-east-1:381491864570:dataset-import-job/media_dataset_import_interaction\n",
      "The interactions dataset import is ACTIVE\n",
      "The items dataset import is still in progress\n",
      "The user dataset import is still in progress\n",
      "\n",
      "Build succeeded for arn:aws:personalize:us-east-1:381491864570:dataset-import-job/media_dataset_import_interaction\n",
      "The interactions dataset import is ACTIVE\n",
      "The items dataset import is still in progress\n",
      "The user dataset import is still in progress\n",
      "\n",
      "Build succeeded for arn:aws:personalize:us-east-1:381491864570:dataset-import-job/media_dataset_import_interaction\n",
      "The interactions dataset import is ACTIVE\n",
      "The items dataset import is still in progress\n",
      "The user dataset import is still in progress\n",
      "\n",
      "Build succeeded for arn:aws:personalize:us-east-1:381491864570:dataset-import-job/media_dataset_import_interaction\n",
      "The interactions dataset import is ACTIVE\n",
      "The items dataset import is still in progress\n",
      "Build succeeded for arn:aws:personalize:us-east-1:381491864570:dataset-import-job/media_dataset_import_user\n",
      "The user dataset import is ACTIVE\n",
      "\n",
      "Build succeeded for arn:aws:personalize:us-east-1:381491864570:dataset-import-job/media_dataset_import_interaction\n",
      "The interactions dataset import is ACTIVE\n",
      "Build succeeded for arn:aws:personalize:us-east-1:381491864570:dataset-import-job/media_dataset_import_item\n",
      "The items dataset import is ACTIVE\n",
      "Build succeeded for arn:aws:personalize:us-east-1:381491864570:dataset-import-job/media_dataset_import_user\n",
      "The user dataset import is ACTIVE\n"
     ]
    }
   ],
   "source": [
    "max_time = time.time() + 6*60*60 # 10 hours\n",
    "while time.time() < max_time:\n",
    "\n",
    "    # Interactions dataset import\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = workshop_interactions_dataset_import_job_arn\n",
    "    )\n",
    "    status_interactions_import = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    \n",
    "    if status_interactions_import == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(workshop_interactions_dataset_import_job_arn))\n",
    "        \n",
    "    elif status_interactions_import == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(workshop_interactions_dataset_import_job_arn))\n",
    "        break\n",
    "        \n",
    "    if not status_interactions_import == \"ACTIVE\":\n",
    "        print(\"The interactions dataset import is still in progress\")\n",
    "    else:\n",
    "        print(\"The interactions dataset import is ACTIVE\")\n",
    "\n",
    "    # Items dataset import\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = workshop_items_dataset_import_job_arn\n",
    "    )\n",
    "    status_items_import = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    \n",
    "    if status_items_import == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(workshop_items_dataset_import_job_arn))\n",
    "        \n",
    "    elif status_items_import == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(workshop_items_dataset_import_job_arn))\n",
    "        break\n",
    "        \n",
    "    if not status_items_import == \"ACTIVE\":\n",
    "        print(\"The items dataset import is still in progress\")\n",
    "    else:\n",
    "        print(\"The items dataset import is ACTIVE\")\n",
    "        \n",
    "        \n",
    "   # Users dataset import  \n",
    "    describe_users_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = workshop_users_dataset_import_job_arn\n",
    "    )\n",
    "    status_users_import = describe_users_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    \n",
    "    if status_users_import == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(workshop_users_dataset_import_job_arn))\n",
    "        \n",
    "    elif status_users_import == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(workshop_users_dataset_import_job_arn))\n",
    "        break\n",
    "        \n",
    "    if not status_users_import == \"ACTIVE\":\n",
    "        print(\"The user dataset import is still in progress\")\n",
    "    else:\n",
    "        print(\"The user dataset import is ACTIVE\")\n",
    "        \n",
    "\n",
    "    if status_interactions_import == \"ACTIVE\" and status_items_import == 'ACTIVE' and status_users_import  == 'ACTIVE':\n",
    "        break\n",
    "\n",
    "    print()\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You now have imported data from your 3 fictional environments (Content Management System, Analytics/Customer Data Platform and Customer Resource Management/Subscriber Management System)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this data in the next labs. In order to use this data we will store these variables so subsequent notebooks can use this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'dataset_dir' (str)\n",
      "Stored 'data_dir' (str)\n",
      "Stored 'interactions_filename' (str)\n",
      "Stored 'items_filename' (str)\n",
      "Stored 'users_filename' (str)\n",
      "Stored 'workshop_dataset_group_arn' (str)\n",
      "Stored 'workshop_interactions_dataset_arn' (str)\n",
      "Stored 'workshop_items_dataset_arn' (str)\n",
      "Stored 'workshop_users_dataset_arn' (str)\n",
      "Stored 'workshop_interactions_schema_arn' (str)\n",
      "Stored 'workshop_items_schema_arn' (str)\n",
      "Stored 'workshop_users_schema_arn' (str)\n",
      "Stored 'workshop_rerank_solution_name' (str)\n",
      "Stored 'workshop_rerank_campaign_name' (str)\n",
      "Stored 'recommender_more_like_x_name' (str)\n",
      "Stored 'recommender_top_picks_for_you_name' (str)\n",
      "Stored 'region' (str)\n",
      "Stored 'account_id' (str)\n",
      "Stored 'role_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store dataset_dir\n",
    "%store data_dir\n",
    "%store interactions_filename\n",
    "%store items_filename\n",
    "%store users_filename\n",
    "%store workshop_dataset_group_arn\n",
    "%store workshop_interactions_dataset_arn\n",
    "%store workshop_items_dataset_arn\n",
    "%store workshop_users_dataset_arn\n",
    "%store workshop_interactions_schema_arn\n",
    "%store workshop_items_schema_arn\n",
    "%store workshop_users_schema_arn\n",
    "%store workshop_rerank_solution_name\n",
    "%store workshop_rerank_campaign_name\n",
    "%store recommender_more_like_x_name\n",
    "%store recommender_top_picks_for_you_name\n",
    "%store region\n",
    "%store account_id\n",
    "%store role_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the next notebook [`Media_03_Training.ipynb`](Media_03_Training.ipynb) to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
